{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged Analysis Notebook\n\n",
    "This notebook contains the merged content from:\n",
    "1. ML_models_with_TF_IDF_and_SkipGram.ipynb\n",
    "2. cse440project-Glove-with-7-models.ipynb\n",
    "3. cse440project-skipgram-with-7-models.ipynb\n\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n## Section 1: ML_models_with_TF_IDF_and_SkipGram\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73rp_350hC8H",
    "outputId": "258c808a-d74e-49da-bb93-a5dbb8a3c8c7"
   },
   "outputs": [],
   "source": [
    "# Install gdown if not available\n",
    "!pip install gdown --quiet\n",
    "\n",
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Replace these with your links\n",
    "# ============================\n",
    "train_url = \"https://drive.google.com/file/d/1lRLZyebOdT2UIRvrPKJoE4a6TqGxczZG/view?usp=drive_link\"\n",
    "test_url  = \"https://drive.google.com/file/d/1fwcV7K0vq5OiuS33dw_0Y48lZZSuyZg_/view?usp=drive_link\"\n",
    "\n",
    "# Extract file IDs\n",
    "train_id = train_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "test_id  = test_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "\n",
    "# Download files\n",
    "gdown.download(f\"https://drive.google.com/uc?id={train_id}\", \"train.csv\", quiet=False)\n",
    "gdown.download(f\"https://drive.google.com/uc?id={test_id}\", \"test.csv\", quiet=False)\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    print(\"Training dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Training file not found.\")\n",
    "    train = None\n",
    "\n",
    "try:\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    print(\"Testing dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Testing file not found. We'll create a test split from the training data.\")\n",
    "    test = None\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train.head()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "rob-u8PAh29B",
    "outputId": "de6886d0-b724-4e17-d816-219e7ebd7450"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Step 1: Exploratory Data Analysis\n",
    "# =============================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# --- Load Data ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# --- Class Distribution ---\n",
    "plt.figure(figsize=(12,6))  # wider figure\n",
    "sns.countplot(x=\"Class\", data=train, order=train['Class'].value_counts().index)\n",
    "plt.title(\"Class Distribution (Train Set)\")\n",
    "plt.xticks(rotation=45, ha='right')  # rotate labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass distribution (%):\")\n",
    "print(train['Class'].value_counts(normalize=True)*100)\n",
    "\n",
    "# --- Text Length Distribution ---\n",
    "train['text_length'] = train['QA Text'].apply(len)\n",
    "test['text_length']  = test['QA Text'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(train['text_length'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Text Lengths (Train)\")\n",
    "plt.xlabel(\"Text Length (#characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# --- Word Count Distribution ---\n",
    "train['word_count'] = train['QA Text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x='Class', y='word_count', data=train)\n",
    "plt.title(\"Word Count by Class\")\n",
    "plt.xticks(rotation=45, ha='right')  # rotate labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Word Clouds per Class ---\n",
    "for label in train['Class'].unique():\n",
    "    subset = train[train['Class'] == label]\n",
    "    text = \" \".join(subset['QA Text'].astype(str).values)\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400,\n",
    "        background_color=\"white\",\n",
    "        max_words=200\n",
    "    ).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud - Class: {label}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ets6aNEDiwcM",
    "outputId": "3e81f664-7b24-42aa-c438-5ea0158a3cfa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Step 2: Preprocessing (Fixed)\n",
    "# =============================\n",
    "import pandas as pd\n",
    "import re, string, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# --- Download NLTK resources ---\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# --- Initialize stopwords and lemmatizer ---\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# --- Text cleaning function ---\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # lowercase\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)  # remove punctuation\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# --- Load raw CSV files ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# --- Apply text cleaning ---\n",
    "train[\"clean_text\"] = train[\"QA Text\"].apply(clean_text)\n",
    "test[\"clean_text\"]  = test[\"QA Text\"].apply(clean_text)\n",
    "\n",
    "# --- Save preprocessed datasets ---\n",
    "train.to_csv(\"train_processed.csv\", index=False)\n",
    "test.to_csv(\"test_processed.csv\", index=False)\n",
    "\n",
    "print(\"✅ Preprocessing complete! Files saved as 'train_processed.csv' and 'test_processed.csv'\")\n",
    "print(train[[\"QA Text\", \"clean_text\", \"Class\"]].head())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9CuP9g6mSBs",
    "outputId": "bf7d0a9b-f6c4-4ee0-9ab6-d4fe9e90919e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Balanced BoW + Random Forest\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load preprocessed data ---\n",
    "train = pd.read_csv(\"train_processed.csv\")\n",
    "test  = pd.read_csv(\"test_processed.csv\")\n",
    "\n",
    "X_train, y_train = train[\"clean_text\"], train[\"Class\"]\n",
    "X_test, y_test   = test[\"clean_text\"], test[\"Class\"]\n",
    "\n",
    "# --- BoW Vectorization (larger vocab for better accuracy) ---\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "Xtr = vectorizer.fit_transform(X_train)\n",
    "Xte = vectorizer.transform(X_test)\n",
    "\n",
    "# --- Balanced Random Forest with progress ---\n",
    "n_estimators = 200\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=1,     # start with 1 tree\n",
    "    max_depth=30,       # deeper trees for better accuracy\n",
    "    warm_start=True,    # incremental training for progress display\n",
    "    n_jobs=-1,          # use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest (Balanced BoW):\")\n",
    "for i in tqdm(range(1, n_estimators + 1)):\n",
    "    model.n_estimators = i\n",
    "    model.fit(Xtr, y_train)\n",
    "    if i % 20 == 0:  # print every 20 trees\n",
    "        print(f\"Trained {i}/{n_estimators} trees\")\n",
    "\n",
    "# --- Predictions ---\n",
    "preds = model.predict(Xte)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"F1 Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Balanced RandomForest + BoW\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JQjZmXvzouSV",
    "outputId": "50520985-4055-49e1-80e3-0ced5e6af4a8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Balanced TF-IDF + Random Forest\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load preprocessed data ---\n",
    "train = pd.read_csv(\"train_processed.csv\")\n",
    "test  = pd.read_csv(\"test_processed.csv\")\n",
    "\n",
    "X_train, y_train = train[\"clean_text\"], train[\"Class\"]\n",
    "X_test, y_test   = test[\"clean_text\"], test[\"Class\"]\n",
    "\n",
    "# --- TF-IDF Vectorization (larger vocab for better accuracy) ---\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "Xtr = vectorizer.fit_transform(X_train)\n",
    "Xte = vectorizer.transform(X_test)\n",
    "\n",
    "# --- Balanced Random Forest with progress ---\n",
    "n_estimators = 200\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=1,     # start with 1 tree\n",
    "    max_depth=30,       # deeper trees for better accuracy\n",
    "    warm_start=True,    # incremental training for progress display\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest (balanced speed & accuracy):\")\n",
    "for i in tqdm(range(1, n_estimators + 1)):\n",
    "    model.n_estimators = i\n",
    "    model.fit(Xtr, y_train)\n",
    "    if i % 20 == 0:  # print progress every 20 trees\n",
    "        print(f\"Trained {i}/{n_estimators} trees\")\n",
    "\n",
    "# --- Predictions ---\n",
    "preds = model.predict(Xte)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"F1 Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Balanced RandomForest + TF-IDF\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0lWB2A1SAOxu",
    "outputId": "3308e1d0-dca9-414c-d6cb-a12cdf8d5cc2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Logistic Regression + BoW (Light + Progress)\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# --- Suppress convergence warnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# --- Load preprocessed data ---\n",
    "train = pd.read_csv(\"train_processed.csv\")\n",
    "test  = pd.read_csv(\"test_processed.csv\")\n",
    "X_train, y_train = train[\"clean_text\"], train[\"Class\"]\n",
    "X_test, y_test   = test[\"clean_text\"], test[\"Class\"]\n",
    "\n",
    "# --- BoW Vectorization (smaller features for speed) ---\n",
    "vectorizer = CountVectorizer(max_features=2000)\n",
    "Xtr = vectorizer.fit_transform(X_train)\n",
    "Xte = vectorizer.transform(X_test)\n",
    "\n",
    "# --- Logistic Regression setup with warm_start ---\n",
    "lr = LogisticRegression(\n",
    "    max_iter=50,      # iterations per chunk\n",
    "    solver='saga',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    warm_start=True   # allows incremental training\n",
    ")\n",
    "model = OneVsRestClassifier(lr)\n",
    "\n",
    "# --- Train in increments with progress ---\n",
    "print(\"Training Logistic Regression (BoW, light version) with progress...\")\n",
    "chunks = 20  # total iterations = 50 * 20 = 1000\n",
    "for i in tqdm(range(chunks)):\n",
    "    lr.max_iter = 50 * (i + 1)\n",
    "    model.fit(Xtr, y_train)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# --- Predictions ---\n",
    "preds = model.predict(Xte)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"F1 Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression + BoW (Light + Progress)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s1gF3VrvQ0iV",
    "outputId": "3f62ce22-15fe-4743-da51-1e61c0502a7b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Logistic Regression + TF-IDF (Super-Ultra-Light + Progress)\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# --- Suppress convergence warnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# --- Load preprocessed data ---\n",
    "train = pd.read_csv(\"train_processed.csv\")\n",
    "test  = pd.read_csv(\"test_processed.csv\")\n",
    "X_train, y_train = train[\"clean_text\"], train[\"Class\"]\n",
    "X_test, y_test   = test[\"clean_text\"], test[\"Class\"]\n",
    "\n",
    "# --- TF-IDF Vectorization (very small features for max speed) ---\n",
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "Xtr = vectorizer.fit_transform(X_train)\n",
    "Xte = vectorizer.transform(X_test)\n",
    "\n",
    "# --- Logistic Regression setup ---\n",
    "lr = LogisticRegression(\n",
    "    max_iter=10,      # very small steps\n",
    "    solver='saga',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    warm_start=True\n",
    ")\n",
    "model = OneVsRestClassifier(lr)\n",
    "\n",
    "# --- Train in increments with progress ---\n",
    "print(\"Training Logistic Regression (TF-IDF, super-ultra-light) with progress...\")\n",
    "chunks = 5  # total = 10 * 5 = 50 iterations\n",
    "for i in tqdm(range(chunks)):\n",
    "    lr.max_iter = 10 * (i + 1)\n",
    "    model.fit(Xtr, y_train)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# --- Predictions ---\n",
    "preds = model.predict(Xte)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"F1 Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression + TF-IDF (Super-Ultra-Light + Progress)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963
    },
    "id": "VmI01Iz-9oL6",
    "outputId": "17b04fcf-7220-4301-96b6-5e3b973d518e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Naive Bayes + BoW (Super-Fast + Progress)\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load preprocessed data ---\n",
    "train = pd.read_csv(\"train_processed.csv\")\n",
    "test  = pd.read_csv(\"test_processed.csv\")\n",
    "X_train, y_train = train[\"clean_text\"], train[\"Class\"]\n",
    "X_test, y_test   = test[\"clean_text\"], test[\"Class\"]\n",
    "\n",
    "# --- BoW Vectorization (small features for speed) ---\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "Xtr = vectorizer.fit_transform(X_train)\n",
    "Xte = vectorizer.transform(X_test)\n",
    "\n",
    "# --- Naive Bayes setup ---\n",
    "model = MultinomialNB()\n",
    "\n",
    "# --- Simulated incremental training with progression ---\n",
    "print(\"Training Naive Bayes (BoW, super-fast) with progress...\")\n",
    "batch_size = int(Xtr.shape[0] / 10)  # split into 10 mini-batches\n",
    "for i in tqdm(range(10)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size if i < 9 else Xtr.shape[0]\n",
    "    model.partial_fit(Xtr[start:end], y_train[start:end], classes=y_train.unique())\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# --- Predictions ---\n",
    "preds = model.predict(Xte)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"F1 Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Naive Bayes + BoW (Super-Fast + Progress)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963
    },
    "id": "wwp41IwWbyig",
    "outputId": "20917a39-2856-437e-9b16-0e3382e44062"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Naive Bayes + TF-IDF (Super-Fast + Progress)\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load preprocessed data ---\n",
    "train = pd.read_csv(\"train_processed.csv\")\n",
    "test  = pd.read_csv(\"test_processed.csv\")\n",
    "X_train, y_train = train[\"clean_text\"], train[\"Class\"]\n",
    "X_test, y_test   = test[\"clean_text\"], test[\"Class\"]\n",
    "\n",
    "# --- TF-IDF Vectorization (small features for speed) ---\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "Xtr = vectorizer.fit_transform(X_train)\n",
    "Xte = vectorizer.transform(X_test)\n",
    "\n",
    "# --- Naive Bayes setup ---\n",
    "model = MultinomialNB()\n",
    "\n",
    "# --- Incremental training with progress ---\n",
    "print(\"Training Naive Bayes (TF-IDF, super-fast) with progress...\")\n",
    "batch_size = int(Xtr.shape[0] / 10)  # split into 10 mini-batches\n",
    "for i in tqdm(range(10)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size if i < 9 else Xtr.shape[0]\n",
    "    model.partial_fit(Xtr[start:end], y_train[start:end], classes=y_train.unique())\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# --- Predictions ---\n",
    "preds = model.predict(Xte)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"F1 Score:\", f1_score(y_test, preds, average=\"weighted\"))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Naive Bayes + TF-IDF (Super-Fast + Progress)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963
    },
    "id": "qgCUY3QalQ0d",
    "outputId": "0b056599-e67a-4bc0-8246-71c865e6cc82"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Deep Neural Network + TF-IDF\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load preprocessed data ---\n",
    "train = pd.read_csv(\"train_processed.csv\")\n",
    "test  = pd.read_csv(\"test_processed.csv\")\n",
    "X_train, y_train = train[\"clean_text\"], train[\"Class\"]\n",
    "X_test, y_test   = test[\"clean_text\"], test[\"Class\"]\n",
    "\n",
    "# --- TF-IDF Vectorization (lightweight features) ---\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "Xtr = vectorizer.fit_transform(X_train).toarray()\n",
    "Xte = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# --- Encode labels ---\n",
    "encoder = LabelEncoder()\n",
    "y_train_enc = encoder.fit_transform(y_train)\n",
    "y_test_enc  = encoder.transform(y_test)\n",
    "\n",
    "# --- Build a lightweight DNN ---\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1000,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(len(encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- Training with progress bar ---\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "print(\"Training DNN  with progress...\")\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    history = model.fit(Xtr, y_train_enc,\n",
    "                        epochs=1,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0,\n",
    "                        validation_data=(Xte, y_test_enc))\n",
    "    acc = history.history['accuracy'][0]\n",
    "    val_acc = history.history['val_accuracy'][0]\n",
    "    print(f\"Epoch {epoch}/{epochs} - loss: {history.history['loss'][0]:.4f} \"\n",
    "          f\"- acc: {acc:.4f} - val_acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# --- Predictions ---\n",
    "preds = model.predict(Xte).argmax(axis=1)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test_enc, preds))\n",
    "print(\"F1 Score:\", f1_score(y_test_enc, preds, average=\"weighted\"))\n",
    "print(classification_report(y_test_enc, preds, target_names=encoder.classes_))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test_enc, preds)\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=encoder.classes_,\n",
    "            yticklabels=encoder.classes_,\n",
    "            cbar=True)\n",
    "plt.title(\"Confusion Matrix - DNN + TF-IDF \")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kKvfLR2WnLv6",
    "outputId": "7e3aa75c-b071-45fb-acd8-98d6959e7358"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Deep Neural Network + BoW (Light + Progress)\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load preprocessed data ---\n",
    "train = pd.read_csv(\"train_processed.csv\")\n",
    "test  = pd.read_csv(\"test_processed.csv\")\n",
    "X_train, y_train = train[\"clean_text\"], train[\"Class\"]\n",
    "X_test, y_test   = test[\"clean_text\"], test[\"Class\"]\n",
    "\n",
    "# --- BoW Vectorization (lightweight features) ---\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "Xtr = vectorizer.fit_transform(X_train).toarray()\n",
    "Xte = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# --- Encode labels ---\n",
    "encoder = LabelEncoder()\n",
    "y_train_enc = encoder.fit_transform(y_train)\n",
    "y_test_enc  = encoder.transform(y_test)\n",
    "\n",
    "# --- Build a lightweight DNN ---\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1000,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(len(encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- Training with progress bar ---\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "print(\"Training DNN (BoW, light) with progress...\")\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    history = model.fit(Xtr, y_train_enc,\n",
    "                        epochs=1,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0,\n",
    "                        validation_data=(Xte, y_test_enc))\n",
    "    acc = history.history['accuracy'][0]\n",
    "    val_acc = history.history['val_accuracy'][0]\n",
    "    print(f\"Epoch {epoch}/{epochs} - loss: {history.history['loss'][0]:.4f} \"\n",
    "          f\"- acc: {acc:.4f} - val_acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# --- Predictions ---\n",
    "preds = model.predict(Xte).argmax(axis=1)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test_enc, preds))\n",
    "print(\"F1 Score:\", f1_score(y_test_enc, preds, average=\"weighted\"))\n",
    "print(classification_report(y_test_enc, preds, target_names=encoder.classes_))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test_enc, preds)\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=encoder.classes_,\n",
    "            yticklabels=encoder.classes_)\n",
    "plt.title(\"Confusion Matrix - DNN + BoW (Light)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zWIEZwD6qEOz",
    "outputId": "d9a232b9-e33e-48d4-8f25-52e2fd71925f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Accuracy & F1 Comparison (Bar Plot)"
   ],
   "metadata": {
    "id": "V7WaeNTb1MdM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data from your report\n",
    "models = [\n",
    "    \"LR + BoW\", \"DNN + TF-IDF\", \"DNN + BoW\",\n",
    "    \"NB + TF-IDF\", \"NB + BoW\", \"RF + TF-IDF\",\n",
    "    \"LR + TF-IDF\", \"RF + BoW\"\n",
    "]\n",
    "accuracy = [0.6417, 0.6260, 0.6242, 0.6008, 0.5915, 0.5602, 0.5604, 0.5549]\n",
    "f1 = [0.6385, 0.6201, 0.6187, 0.5966, 0.5881, 0.5562, 0.5552, 0.5536]\n",
    "\n",
    "x = range(len(models))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x, accuracy, width=0.4, label=\"Accuracy\", align='center')\n",
    "plt.bar(x, f1, width=0.4, label=\"Macro F1\", align='edge')\n",
    "\n",
    "plt.xticks(x, models, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Performance Comparison (Accuracy vs Macro F1)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "vBjCxlqk1L_i",
    "outputId": "503e807a-b608-43f0-f1ef-7445d1d755d0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(len(models))  # model positions\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(x - width/2, accuracy, width, label='Accuracy')\n",
    "plt.bar(x + width/2, f1, width, label='Macro F1')\n",
    "\n",
    "plt.xticks(x, models, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Accuracy vs F1 across Models\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "G65_xT3a1aRz",
    "outputId": "8ef1eeaf-b709-466e-c9c5-7b967e669909"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Model\": models, \"Accuracy\": accuracy, \"F1\": f1})\n",
    "df_sorted = df.sort_values(\"Accuracy\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df_sorted[\"Model\"], df_sorted[\"Accuracy\"], marker=\"o\", label=\"Accuracy\")\n",
    "plt.plot(df_sorted[\"Model\"], df_sorted[\"F1\"], marker=\"s\", label=\"Macro F1\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Ranking by Accuracy and F1\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "kuVCYj3c1c-C",
    "outputId": "3a78ca2c-9a01-4624-f89c-a3a10145ab58"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "SIFAR CODES\n"
   ],
   "metadata": {
    "id": "z8UfEcAZTZ1V"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "O8GVmU3WTcCu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n## Section 2: cse440project-Glove-with-7-models\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# libraries",
   "metadata": {
    "id": "LMC64jaiQg3I"
   }
  },
  {
   "cell_type": "code",
   "source": "from tensorflow.keras.preprocessing.text import Tokenizer\nimport re\nimport random\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport pandas as pd\nimport ast\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, GRU, LSTM, Bidirectional, Dense, TimeDistributed\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Accuracy\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import EarlyStopping\nfrom itertools import chain\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\nfrom sklearn.model_selection import train_test_split",
   "metadata": {
    "id": "mYix4v9hQgS4",
    "trusted": true,
    "outputId": "0e15c63d-ec03-4037-f6a8-34b2a1f8d2da",
    "execution": {
     "iopub.status.busy": "2025-09-06T10:22:27.633089Z",
     "iopub.execute_input": "2025-09-06T10:22:27.633294Z",
     "iopub.status.idle": "2025-09-06T10:22:43.380934Z",
     "shell.execute_reply.started": "2025-09-06T10:22:27.633275Z",
     "shell.execute_reply": "2025-09-06T10:22:43.380091Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# loading the dataset",
   "metadata": {
    "id": "T3gR6fqw5Wz9"
   }
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd          # for data handling\nimport gdown                 # for downloading files from Google Drive\n\n# Google Drive share links\ntrain_file_link = \"https://drive.google.com/file/d/1lRLZyebOdT2UIRvrPKJoE4a6TqGxczZG/view?usp=sharing\"\ntest_file_link  = \"https://drive.google.com/file/d/1NMHO5rEzDoY8v4SQoLXoXl2h8yrrvegs/view?usp=drive_link\"\n\n# Extract file IDs from the links\ntrain_id = train_file_link.split(\"/\")[-2]\ntest_id  = test_file_link.split(\"/\")[-2]\n\n# Download files using gdown and save locally\ngdown.download(f\"https://drive.google.com/uc?id={train_id}\", \"train.csv\", quiet=False)\ngdown.download(f\"https://drive.google.com/uc?id={test_id}\",  \"test.csv\",  quiet=False)\n\n# Load CSV files into pandas DataFrames\ntrain_df = pd.read_csv(\"train.csv\")\ntest_df  = pd.read_csv(\"test.csv\")\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqlaaCVrSrdP",
    "outputId": "bfa79998-f4cf-4d2b-acb4-2cc02e76bc7d",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:22:43.382347Z",
     "iopub.execute_input": "2025-09-06T10:22:43.382754Z",
     "iopub.status.idle": "2025-09-06T10:22:54.076188Z",
     "shell.execute_reply.started": "2025-09-06T10:22:43.382736Z",
     "shell.execute_reply": "2025-09-06T10:22:54.075594Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Show first 3 rows of training dataset\nprint(train_df.head(3).to_string())\nprint(test_df.head(3).to_string())",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hEITcRTUJVT",
    "outputId": "0cc20dd8-585b-43ec-f417-aa7706989f2d",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:22:54.076860Z",
     "iopub.execute_input": "2025-09-06T10:22:54.077133Z",
     "iopub.status.idle": "2025-09-06T10:22:54.091230Z",
     "shell.execute_reply.started": "2025-09-06T10:22:54.077109Z",
     "shell.execute_reply": "2025-09-06T10:22:54.090409Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# triming the dataset",
   "metadata": {
    "id": "DXWkl8qqcvts"
   }
  },
  {
   "cell_type": "code",
   "source": "# #triming the dataset\n# import pandas as pd\n# print(\"Original size:\", len(train_df))\n# print(\"Original size:\", len(test_df))\n\n# # Keep one third of the dataset\n# train_df = train_df.sample(frac=0.5/3, random_state=42)\n# test_df = test_df.sample(frac=0.5/3, random_state=42)\n# print(\"Reduced size:\", len(train_df))\n# print(\"Reduced size:\", len(test_df))\n\n# print(train_df.head(3).to_string())\n# print(test_df.head(3).to_string())",
   "metadata": {
    "id": "pvE6VlEF9Dx8",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:22:54.092051Z",
     "iopub.execute_input": "2025-09-06T10:22:54.092800Z",
     "iopub.status.idle": "2025-09-06T10:22:54.096703Z",
     "shell.execute_reply.started": "2025-09-06T10:22:54.092772Z",
     "shell.execute_reply": "2025-09-06T10:22:54.095875Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# spliting the  \"QA Text\" column",
   "metadata": {
    "id": "dCJ0OQ4r4SRV"
   }
  },
  {
   "cell_type": "code",
   "source": "# Function to extract parts from QA Text\ndef split_qa(text):\n    if pd.isna(text):\n        return {\"Question Title\": None, \"Question Content\": None, \"Best Answer\": None}\n\n    title = re.search(r\"Question Title:\\s*(.*?)\\s*Question Content:\", text, re.S)\n    content = re.search(r\"Question Content:\\s*(.*?)\\s*Best Answer:\", text, re.S)\n    answer = re.search(r\"Best Answer:\\s*(.*)\", text, re.S)\n\n    return {\n        \"Question Title\": title.group(1).strip() if title else None,\n        \"Question Content\": content.group(1).strip() if content else None,\n        \"Best Answer\": answer.group(1).strip() if answer else None\n    }\n\n# Apply the function to QA Text column\nqa_split_train = train_df[\"QA Text\"].apply(split_qa).apply(pd.Series)\nqa_split_test = test_df[\"QA Text\"].apply(split_qa).apply(pd.Series)\n# Merge with class column\ntrain_df = pd.concat([qa_split_train, train_df[\"Class\"]], axis=1)\ntest_df = pd.concat([qa_split_test, test_df[\"Class\"]], axis=1)\n# Preview cleaned data\nprint(train_df.head(3))\nprint(test_df.head(3))\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvwJexcBzfLb",
    "outputId": "94281a27-41eb-4a5b-9a6a-3b62f1f57ac2",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:22:54.098804Z",
     "iopub.execute_input": "2025-09-06T10:22:54.099029Z",
     "iopub.status.idle": "2025-09-06T10:24:21.030240Z",
     "shell.execute_reply.started": "2025-09-06T10:22:54.099012Z",
     "shell.execute_reply": "2025-09-06T10:24:21.029616Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# lowercase the dataset",
   "metadata": {
    "id": "4IsJhlPu4buM"
   }
  },
  {
   "cell_type": "code",
   "source": "#train\n# List of text columns to convert to lowercase\n# Hello and hello tokens are different, if not do lowercase\ntext_columns = [\"Question Title\", \"Question Content\", \"Best Answer\", \"Class\"]\n\n# Apply to train and test dataset\nfor col in text_columns:\n    train_df[col] = train_df[col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n    test_df[col] = test_df[col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n\n\n# Preview first 2 rows of train dataset\nprint(train_df.head(2).to_string())\nprint(test_df.head(2).to_string())\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8dAm0oh1ntT",
    "outputId": "d96ce5fd-375d-40c4-89dc-e8ab1f02e63f",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:24:21.030923Z",
     "iopub.execute_input": "2025-09-06T10:24:21.031117Z",
     "iopub.status.idle": "2025-09-06T10:24:22.145659Z",
     "shell.execute_reply.started": "2025-09-06T10:24:21.031101Z",
     "shell.execute_reply": "2025-09-06T10:24:22.144930Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# remove punctuation, normalize spaces, remove newline char(train dataset)",
   "metadata": {
    "id": "bI0N4gQq6AW0"
   }
  },
  {
   "cell_type": "code",
   "source": "# Improved cleaning function\ndef clean_text(text):\n    if pd.isna(text):\n        return \"\"\n    text = re.sub(r\"\\\\n\", \" \", text)       # remove literal \\n sequences\n    text = re.sub(r\"[\\n\\r]\", \" \", text)    # remove actual newline characters\n    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)  # keep only letters, numbers, spaces\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # normalize multiple spaces\n    return text\n# Apply separately to each column\nfor col in [\"Question Title\", \"Question Content\", \"Best Answer\"]:\n    train_df[col] = train_df[col].apply(clean_text)\n    test_df[col] = test_df[col].apply(clean_text)\n# Preview\nprint(train_df.head(3).to_string())\nprint(test_df.head(3).to_string())\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qjgi6Wd7Iw-A",
    "outputId": "ec938fb4-a7d2-424c-dd0a-f0a65613410a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:24:22.146453Z",
     "iopub.execute_input": "2025-09-06T10:24:22.146720Z",
     "iopub.status.idle": "2025-09-06T10:24:50.993478Z",
     "shell.execute_reply.started": "2025-09-06T10:24:22.146695Z",
     "shell.execute_reply": "2025-09-06T10:24:50.992799Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# removing stop words",
   "metadata": {
    "id": "mYGJ7VPI5AZ8"
   }
  },
  {
   "cell_type": "code",
   "source": "import nltk\nfrom nltk.corpus import stopwords\n# Download stopwords\nnltk.download(\"stopwords\")\nstop_words = set(stopwords.words(\"english\"))\ndef remove_stopwords(text):\n    if pd.isna(text):\n        return text\n    return \" \".join([word for word in text.split() if word.lower() not in stop_words])\n\n\n# Apply stopword removal to all three text columns\nfor col in [\"Question Title\", \"Question Content\", \"Best Answer\"]:\n    train_df[col] = train_df[col].apply(remove_stopwords)\n    test_df[col] = test_df[col].apply(remove_stopwords)\n\n\n# Preview result\nprint(train_df.sample(3).to_string())\nprint(test_df.sample(3).to_string())",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xClKcTZR1nqL",
    "outputId": "3659ec0b-42df-4b30-fc8a-58caefcc1f96",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:24:50.994274Z",
     "iopub.execute_input": "2025-09-06T10:24:50.994571Z",
     "iopub.status.idle": "2025-09-06T10:25:00.838567Z",
     "shell.execute_reply.started": "2025-09-06T10:24:50.994523Z",
     "shell.execute_reply": "2025-09-06T10:25:00.837801Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# tokenization",
   "metadata": {
    "id": "YHWYXYxTBWdv"
   }
  },
  {
   "cell_type": "code",
   "source": "# Download the correct tokenizer\nfrom nltk.tokenize import word_tokenize\nnltk.download(\"punkt\")\nnltk.download('punkt_tab')\n\n\n# Function to tokenize text\ndef tokenize_text(text):\n    if pd.isna(text) or text == \"\":\n        return []\n    return word_tokenize(text)\n\n\n# Apply tokenization to each text column\nfor col in [\"Question Title\", \"Question Content\", \"Best Answer\"]:\n    train_df[col] = train_df[col].apply(tokenize_text)\n    test_df[col] = test_df[col].apply(tokenize_text)\n\n\n# Preview tokenized data\nprint(train_df.sample(3).to_string())\nprint(test_df.sample(3).to_string())",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ci_VF73F5Ke",
    "outputId": "5ce2c7c7-31eb-4d98-8b30-3aa834545bd4",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:25:00.839301Z",
     "iopub.execute_input": "2025-09-06T10:25:00.839595Z",
     "iopub.status.idle": "2025-09-06T10:27:10.756745Z",
     "shell.execute_reply.started": "2025-09-06T10:25:00.839576Z",
     "shell.execute_reply": "2025-09-06T10:27:10.755871Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# integer mapping/token indexing",
   "metadata": {
    "id": "ojqiS4vSPMqK"
   }
  },
  {
   "cell_type": "code",
   "source": "# Combine all text columns into a single corpus\ntrain_texts = list(train_df[\"Question Title\"].apply(lambda x: \" \".join(x))) + \\\n              list(train_df[\"Question Content\"].apply(lambda x: \" \".join(x))) + \\\n              list(train_df[\"Best Answer\"].apply(lambda x: \" \".join(x)))\n\ntest_texts  = list(test_df[\"Question Title\"].apply(lambda x: \" \".join(x))) + \\\n              list(test_df[\"Question Content\"].apply(lambda x: \" \".join(x))) + \\\n              list(test_df[\"Best Answer\"].apply(lambda x: \" \".join(x)))\n\n\n\n# Fit tokenizer ONLY on train data\n# Tokenizer fit only on train data → prevents test data leakage.\n# <OOV> token handles any word in the test set not seen in train data.\nmax_vocab=30000\ntokenizer = Tokenizer(num_words=max_vocab, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train_texts)\n\n\n# Convert token lists → integer sequences\ntrain_df[\"Question Title\"]   = tokenizer.texts_to_sequences(train_df[\"Question Title\"].apply(lambda x: \" \".join(x)))\ntrain_df[\"Question Content\"] = tokenizer.texts_to_sequences(train_df[\"Question Content\"].apply(lambda x: \" \".join(x)))\ntrain_df[\"Best Answer\"]      = tokenizer.texts_to_sequences(train_df[\"Best Answer\"].apply(lambda x: \" \".join(x)))\n\n\ntest_df[\"Question Title\"]   = tokenizer.texts_to_sequences(test_df[\"Question Title\"].apply(lambda x: \" \".join(x)))\ntest_df[\"Question Content\"] = tokenizer.texts_to_sequences(test_df[\"Question Content\"].apply(lambda x: \" \".join(x)))\ntest_df[\"Best Answer\"]      = tokenizer.texts_to_sequences(test_df[\"Best Answer\"].apply(lambda x: \" \".join(x)))\n\n\n# Preview sequences\nprint(train_df.head(3).to_string())\nprint(test_df.head(3).to_string())",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tUxczdX_-IV",
    "outputId": "d643df84-54c2-40a1-e7c7-c8511e6fa7aa",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:27:10.757605Z",
     "iopub.execute_input": "2025-09-06T10:27:10.757825Z",
     "iopub.status.idle": "2025-09-06T10:27:46.260913Z",
     "shell.execute_reply.started": "2025-09-06T10:27:10.757808Z",
     "shell.execute_reply": "2025-09-06T10:27:46.260122Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# padding",
   "metadata": {
    "id": "ZBi3u__jNJhq"
   }
  },
  {
   "cell_type": "code",
   "source": "# Find max lengths for each column\n#nn needs same length for train and test data, so we dont max len of both separately\nmax_len_title = max(train_df[\"Question Title\"].apply(len))\nmax_len_content = max(train_df[\"Question Content\"].apply(len))\nmax_len_answer = max(train_df[\"Best Answer\"].apply(len))\nprint(\"Max Lengths:\", max_len_title, max_len_content, max_len_answer)\n\n# Apply padding (post-padding with zeros)\ntrain_df[\"Question Title\"]   = list(pad_sequences(train_df[\"Question Title\"], maxlen=max_len_title, padding=\"post\", truncating=\"post\"))\ntrain_df[\"Question Content\"] = list(pad_sequences(train_df[\"Question Content\"], maxlen=max_len_content, padding=\"post\", truncating=\"post\"))\ntrain_df[\"Best Answer\"]      = list(pad_sequences(train_df[\"Best Answer\"], maxlen=max_len_answer, padding=\"post\", truncating=\"post\"))\n\ntest_df[\"Question Title\"]   = list(pad_sequences(test_df[\"Question Title\"], maxlen=max_len_title, padding=\"post\", truncating=\"post\"))\ntest_df[\"Question Content\"] = list(pad_sequences(test_df[\"Question Content\"], maxlen=max_len_content, padding=\"post\", truncating=\"post\"))\ntest_df[\"Best Answer\"]      = list(pad_sequences(test_df[\"Best Answer\"], maxlen=max_len_answer, padding=\"post\", truncating=\"post\"))\n\n\n# Preview\nprint(train_df.head(3).to_string())\nprint(test_df.head(3).to_string())\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0JP_i_nBh8B",
    "outputId": "b49d6338-2d48-4070-fe40-3820774fbdcb",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:27:46.262122Z",
     "iopub.execute_input": "2025-09-06T10:27:46.262418Z",
     "iopub.status.idle": "2025-09-06T10:27:51.759993Z",
     "shell.execute_reply.started": "2025-09-06T10:27:46.262399Z",
     "shell.execute_reply": "2025-09-06T10:27:51.759111Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# checking imbalance on label(train,test)",
   "metadata": {
    "id": "GlkC1EHtpcrs"
   }
  },
  {
   "cell_type": "code",
   "source": "# train\nlabel_percentages = train_df[\"Class\"].value_counts(normalize=True) * 100\nprint(label_percentages)\n\n#test\nlabel_percentages_test = test_df[\"Class\"].value_counts(normalize=True) * 100\nprint(label_percentages_test)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qy9lEh8q3GY5",
    "outputId": "c3ea08c8-eec6-4d9e-f888-9a9e05a96700",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:27:51.760819Z",
     "iopub.execute_input": "2025-09-06T10:27:51.761066Z",
     "iopub.status.idle": "2025-09-06T10:27:51.825346Z",
     "shell.execute_reply.started": "2025-09-06T10:27:51.761047Z",
     "shell.execute_reply": "2025-09-06T10:27:51.824602Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# encoding label",
   "metadata": {
    "id": "GWNWouZA20dR"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\n# Step 1: Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Step 2: Fit on training labels and transform both train and test\ntrain_labels_int = label_encoder.fit_transform(train_df[\"Class\"])\ntest_labels_int  = label_encoder.transform(test_df[\"Class\"])\n\n# Step 3: Convert integer labels → one-hot encoding\nencoded_train = to_categorical(train_labels_int)\nencoded_test  = to_categorical(test_labels_int)\n\n# Step 4: Replace \"Class\" column with one-hot encoded arrays\ntrain_df[\"Class\"] = list(encoded_train)\ntest_df[\"Class\"]  = list(encoded_test)\n\n\n# Preview\nprint(train_df.head(3).to_string())\nprint(test_df.head(3).to_string())",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOXKTpvUKeBU",
    "outputId": "db58c20c-e317-40a7-85e1-1027edcbc764",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:27:51.826060Z",
     "iopub.execute_input": "2025-09-06T10:27:51.826260Z",
     "iopub.status.idle": "2025-09-06T10:27:52.074694Z",
     "shell.execute_reply.started": "2025-09-06T10:27:51.826243Z",
     "shell.execute_reply": "2025-09-06T10:27:52.073733Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# train and test data spliting",
   "metadata": {
    "id": "ERKo1kwMJp56"
   }
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\n# Merge Title + Content + Answer row-wise into one long sequence\nX_train = np.array([list(t) + list(c) + list(a) for t, c, a in zip(\n    train_df[\"Question Title\"], train_df[\"Question Content\"], train_df[\"Best Answer\"]\n)])\nX_test = np.array([list(t) + list(c) + list(a) for t, c, a in zip(\n    test_df[\"Question Title\"], test_df[\"Question Content\"], test_df[\"Best Answer\"]\n)])\n\n# Labels (already one-hot encoded)\ny_train = np.array(train_df[\"Class\"].tolist())\ny_test  = np.array(test_df[\"Class\"].tolist())\n\nprint(\"Final Shapes:\")\nprint(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\nprint(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsoujZoP_INR",
    "outputId": "39bdddf0-5573-48a8-9755-563883d3fef5",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:27:52.077988Z",
     "iopub.execute_input": "2025-09-06T10:27:52.078222Z",
     "iopub.status.idle": "2025-09-06T10:30:07.886928Z",
     "shell.execute_reply.started": "2025-09-06T10:27:52.078204Z",
     "shell.execute_reply": "2025-09-06T10:30:07.886281Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# glove embedding",
   "metadata": {
    "id": "ES2GnIVg2b4W"
   }
  },
  {
   "cell_type": "code",
   "source": "!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHwTXyd12d9C",
    "outputId": "385971bb-e5ea-4b08-9ca8-389123ed497a",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:30:07.887851Z",
     "iopub.execute_input": "2025-09-06T10:30:07.888131Z",
     "iopub.status.idle": "2025-09-06T10:33:11.168867Z",
     "shell.execute_reply.started": "2025-09-06T10:30:07.888106Z",
     "shell.execute_reply": "2025-09-06T10:33:11.168038Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "embedding_index = {}\nembedding_dim = 100  # Must match the GloVe file you use\n\nwith open(\"glove.6B.100d.txt\", encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.array(values[1:], dtype='float32')\n        embedding_index[word] = vector\n\nprint(\"Number of words in GloVe:\", len(embedding_index))\n\n\nvocab_size = max_vocab  # Same as your tokenizer's num_words\n\n# Initialize matrix with zeros\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in tokenizer.word_index.items():\n    if i >= vocab_size:\n        continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zSOWtfm2hTj",
    "outputId": "9e018cdd-3b0b-4934-fe66-6fe72aab17a4",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:33:11.170179Z",
     "iopub.execute_input": "2025-09-06T10:33:11.170509Z",
     "iopub.status.idle": "2025-09-06T10:33:19.980457Z",
     "shell.execute_reply.started": "2025-09-06T10:33:11.170472Z",
     "shell.execute_reply": "2025-09-06T10:33:19.979783Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# rnn",
   "metadata": {
    "id": "WcXbelC9LOux"
   }
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, GRU, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Simple RNN Classifier\nmax_len = max_len_title + max_len_content + max_len_answer\nembedding_dim = embedding_matrix.shape[1]  # e.g., 100 if using glove.6B.100d.txt\nwith tf.device('/GPU:0'):\n        rnn_model = Sequential([\n            Embedding(\n                input_dim=max_vocab,           # same as tokenizer num_words\n                output_dim=embedding_dim,      # dimension of GloVe vectors\n                weights=[embedding_matrix],    # use pre-trained GloVe embeddings\n                input_length=max_len,          # padded input length\n                trainable=False                # keep embeddings fixed\n            ),\n            SimpleRNN(32),\n            Dropout(0.5),                      # regularization\n            Dense(10, activation='softmax')    # 10 classes (adjust if needed)\n        ])\n\n        rnn_model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy'])\n\n        early_stop = EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n        rnn_model.fit(\n            X_train, y_train,\n            epochs=5,\n            batch_size=64,\n            validation_split=0.2,\n            callbacks=[early_stop]\n            )\n\n\n        #saving the model\n        rnn_model.save(\"rnn_model.keras\")\n\n        test_loss, test_acc = rnn_model.evaluate(X_test, y_test)\n        print(\"Test Accuracy:\", test_acc)\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cn0Wk9XHLtrn",
    "outputId": "15de1c30-4245-4476-ae9a-8bf6ce00a737",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T10:33:19.981276Z",
     "iopub.execute_input": "2025-09-06T10:33:19.981572Z",
     "iopub.status.idle": "2025-09-06T11:00:40.770629Z",
     "shell.execute_reply.started": "2025-09-06T10:33:19.981530Z",
     "shell.execute_reply": "2025-09-06T11:00:40.769973Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#loading model\nimport tensorflow as tf\nrnn_model = tf.keras.models.load_model('rnn_model.keras')\n\n\n#Metrics: precision / recall / F1 / confusion matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\ntest_loss, test_acc = rnn_model.evaluate(X_test, y_test, verbose=0)\ny_prob = rnn_model.predict(X_test, batch_size=512, verbose=0)\ny_pred = np.argmax(y_prob, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n\naccuracy_rnn  = accuracy_score(y_true, y_pred)\nprecision_rnn = precision_score(y_true, y_pred, average='macro',   zero_division=0)\nrecall_rnn  = recall_score(y_true,  y_pred, average='macro',     zero_division=0)\nf1_rnn   = f1_score(y_true,      y_pred, average='macro',     zero_division=0)\ncm_rnn   = confusion_matrix(y_true, y_pred)\n\n\nprint(\"Accuracy:\", accuracy_rnn)\nprint(\"Precision (macro):\", precision_rnn)\nprint(\"Recall (macro):\", recall_rnn)\nprint(\"F1 (macro):\", f1_rnn)\nprint(\"Confusion Matrix:\\n\", cm_rnn)\n\n\n# Optional: class names if you used a LabelEncoder earlier\nlabels = label_encoder.classes_.tolist() if 'label_encoder' in globals() else None\nprint(\"\\nClassification Report:\\n\",\n      classification_report(y_true, y_pred, target_names=labels))",
   "metadata": {
    "id": "Q9m56mViDLe7",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T11:00:40.771749Z",
     "iopub.execute_input": "2025-09-06T11:00:40.771972Z",
     "iopub.status.idle": "2025-09-06T11:05:13.345510Z",
     "shell.execute_reply.started": "2025-09-06T11:00:40.771955Z",
     "shell.execute_reply": "2025-09-06T11:05:13.344716Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Plot confusion matrix with seaborn\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_rnn, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=labels if labels else None,\n            yticklabels=labels if labels else None)\n\nplt.title(\"Confusion Matrix - RNN Model\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.show()\n",
   "metadata": {
    "trusted": true,
    "id": "ZTiY54QtZCl_",
    "execution": {
     "iopub.status.busy": "2025-09-06T11:05:13.346400Z",
     "iopub.execute_input": "2025-09-06T11:05:13.346668Z",
     "iopub.status.idle": "2025-09-06T11:05:13.923910Z",
     "shell.execute_reply.started": "2025-09-06T11:05:13.346649Z",
     "shell.execute_reply": "2025-09-06T11:05:13.923115Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# bidirectional rnn",
   "metadata": {
    "id": "uCXDk6p2DZxL"
   }
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, GRU, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Simple RNN Classifier\nmax_len = max_len_title + max_len_content + max_len_answer\nembedding_dim = embedding_matrix.shape[1]  # e.g., 100 if using glove.6B.100d.txt\nwith tf.device('/GPU:0'):\n        brnn_model = Sequential([\n            Embedding(\n                input_dim=max_vocab,           # same as tokenizer num_words\n                output_dim=embedding_dim,      # dimension of GloVe vectors\n                weights=[embedding_matrix],    # use pre-trained GloVe embeddings\n                input_length=max_len,          # padded input length\n                trainable=False                # keep embeddings fixed\n            ),\n            Bidirectional(SimpleRNN(32)),\n            Dropout(0.5),                      # regularization\n            Dense(10, activation='softmax')    # 10 classes (adjust if needed)\n        ])\n\n\n        brnn_model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy'])\n\n\n        early_stop = EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n\n        brnn_model.fit(\n            X_train, y_train,\n            epochs=5,\n            batch_size=64,\n            validation_split=0.2,\n            callbacks=[early_stop]\n            )\n\n        #saving the model\n        brnn_model.save(\"brnn_model.keras\")\n\n\n        test_loss, test_acc = brnn_model.evaluate(X_test, y_test)\n        print(\"Test Accuracy:\", test_acc)",
   "metadata": {
    "id": "kiODpzvfDZCc",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T11:05:13.924746Z",
     "iopub.execute_input": "2025-09-06T11:05:13.924952Z",
     "iopub.status.idle": "2025-09-06T11:58:23.101302Z",
     "shell.execute_reply.started": "2025-09-06T11:05:13.924935Z",
     "shell.execute_reply": "2025-09-06T11:58:23.100696Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#loading model\nimport tensorflow as tf\nbrnn_model = tf.keras.models.load_model('brnn_model.keras')\n\n\n#Metrics: precision / recall / F1 / confusion matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\ntest_loss, test_acc = brnn_model.evaluate(X_test, y_test, verbose=0)\ny_prob = brnn_model.predict(X_test, batch_size=512, verbose=0)\ny_pred = np.argmax(y_prob, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n\naccuracy_brnn  = accuracy_score(y_true, y_pred)\nprecision_brnn = precision_score(y_true, y_pred, average='macro',   zero_division=0)\nrecall_brnn  = recall_score(y_true,  y_pred, average='macro',     zero_division=0)\nf1_brnn   = f1_score(y_true,      y_pred, average='macro',     zero_division=0)\ncm_brnn   = confusion_matrix(y_true, y_pred)\n\n\nprint(\"Accuracy:\", accuracy_brnn)\nprint(\"Precision (macro):\", precision_brnn)\nprint(\"Recall (macro):\", recall_brnn)\nprint(\"F1 (macro):\", f1_brnn)\nprint(\"Confusion Matrix:\\n\", cm_brnn)\n\n\n# Optional: class names if you used a LabelEncoder earlier\nlabels = label_encoder.classes_.tolist() if 'label_encoder' in globals() else None\nprint(\"\\nClassification Report:\\n\",\n      classification_report(y_true, y_pred, target_names=labels))\n",
   "metadata": {
    "id": "WlvS5ZDiufAJ",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T11:58:23.102625Z",
     "iopub.execute_input": "2025-09-06T11:58:23.103107Z",
     "iopub.status.idle": "2025-09-06T12:07:00.576971Z",
     "shell.execute_reply.started": "2025-09-06T11:58:23.103085Z",
     "shell.execute_reply": "2025-09-06T12:07:00.576304Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Plot confusion matrix with seaborn for Bi-RNN\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_brnn, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=labels if labels else None,\n            yticklabels=labels if labels else None)\n\nplt.title(\"Confusion Matrix - Bi-RNN Model\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.show()\n",
   "metadata": {
    "trusted": true,
    "id": "NQesZl4KZCl_",
    "execution": {
     "iopub.status.busy": "2025-09-06T12:07:00.577823Z",
     "iopub.execute_input": "2025-09-06T12:07:00.578089Z",
     "iopub.status.idle": "2025-09-06T12:07:01.040850Z",
     "shell.execute_reply.started": "2025-09-06T12:07:00.578067Z",
     "shell.execute_reply": "2025-09-06T12:07:01.040071Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# GRU",
   "metadata": {
    "id": "60Du5NfoD3r2"
   }
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, GRU, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Simple RNN Classifier\nmax_len = max_len_title + max_len_content + max_len_answer\nembedding_dim = embedding_matrix.shape[1]  # e.g., 100 if using glove.6B.100d.txt\nwith tf.device('/GPU:0'):\n        gru_model = Sequential([\n            Embedding(\n                input_dim=max_vocab,           # same as tokenizer num_words\n                output_dim=embedding_dim,      # dimension of GloVe vectors\n                weights=[embedding_matrix],    # use pre-trained GloVe embeddings\n                input_length=max_len,          # padded input length\n                trainable=False                # keep embeddings fixed\n            ),\n            GRU(32),\n            Dropout(0.5),                      # regularization\n            Dense(10, activation='softmax')    # 10 classes (adjust if needed)\n        ])\n\n\n        gru_model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy'])\n\n\n        early_stop = EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n\n\n        gru_model.fit(\n            X_train, y_train,\n            epochs=5,\n            batch_size=64,\n            validation_split=0.2,\n            callbacks=[early_stop]\n            )\n\n\n        #saving the model\n        gru_model.save(\"gru_model.keras\")\n\n        test_loss, test_acc = gru_model.evaluate(X_test, y_test)\n        print(\"Test Accuracy:\", test_acc)",
   "metadata": {
    "id": "xc4Rz6iXD6vD",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T12:07:01.041650Z",
     "iopub.execute_input": "2025-09-06T12:07:01.041873Z",
     "iopub.status.idle": "2025-09-06T12:22:27.370942Z",
     "shell.execute_reply.started": "2025-09-06T12:07:01.041857Z",
     "shell.execute_reply": "2025-09-06T12:22:27.370260Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#loading model\nimport tensorflow as tf\ngru_model = tf.keras.models.load_model('gru_model.keras')\n\n\n#Metrics: precision / recall / F1 / confusion matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\ntest_loss, test_acc = gru_model.evaluate(X_test, y_test, verbose=0)\ny_prob = gru_model.predict(X_test, batch_size=512, verbose=0)\ny_pred = np.argmax(y_prob, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n\naccuracy_gru  = accuracy_score(y_true, y_pred)\nprecision_gru = precision_score(y_true, y_pred, average='macro',   zero_division=0)\nrecall_gru  = recall_score(y_true,  y_pred, average='macro',     zero_division=0)\nf1_gru   = f1_score(y_true,      y_pred, average='macro',     zero_division=0)\ncm_gru   = confusion_matrix(y_true, y_pred)\n\n\nprint(\"Accuracy:\", accuracy_gru)\nprint(\"Precision (macro):\", precision_gru)\nprint(\"Recall (macro):\", recall_gru)\nprint(\"F1 (macro):\", f1_gru)\nprint(\"Confusion Matrix:\\n\", cm_gru)\n\n\n# Optional: class names if you used a LabelEncoder earlier\nlabels = label_encoder.classes_.tolist() if 'label_encoder' in globals() else None\nprint(\"\\nClassification Report:\\n\",\n      classification_report(y_true, y_pred, target_names=labels))\n",
   "metadata": {
    "id": "vnvr4OCqu56_",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T12:22:27.371999Z",
     "iopub.execute_input": "2025-09-06T12:22:27.372226Z",
     "iopub.status.idle": "2025-09-06T12:25:23.968721Z",
     "shell.execute_reply.started": "2025-09-06T12:22:27.372209Z",
     "shell.execute_reply": "2025-09-06T12:25:23.967924Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot confusion matrix heatmap for GRU\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_gru, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=labels if labels else None,\n            yticklabels=labels if labels else None)\n\nplt.title(\"Confusion Matrix - GRU Model\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.show()\n",
   "metadata": {
    "trusted": true,
    "id": "mBPpRafBZCmD",
    "execution": {
     "iopub.status.busy": "2025-09-06T12:25:23.969680Z",
     "iopub.execute_input": "2025-09-06T12:25:23.969921Z",
     "iopub.status.idle": "2025-09-06T12:25:24.372253Z",
     "shell.execute_reply.started": "2025-09-06T12:25:23.969903Z",
     "shell.execute_reply": "2025-09-06T12:25:24.371392Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# bidirectional GRU",
   "metadata": {
    "id": "7VZ8OIahENoQ"
   }
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, GRU, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Simple RNN Classifier\nmax_len = max_len_title + max_len_content + max_len_answer\nembedding_dim = embedding_matrix.shape[1]  # e.g., 100 if using glove.6B.100d.txt\nwith tf.device('/GPU:0'):\n        bgru_model = Sequential([\n            Embedding(\n                input_dim=max_vocab,           # same as tokenizer num_words\n                output_dim=embedding_dim,      # dimension of GloVe vectors\n                weights=[embedding_matrix],    # use pre-trained GloVe embeddings\n                input_length=max_len,          # padded input length\n                trainable=False                # keep embeddings fixed\n            ),\n            Bidirectional(GRU(32)),\n            Dropout(0.5),                      # regularization\n            Dense(10, activation='softmax')    # 10 classes (adjust if needed)\n        ])\n\n\n        bgru_model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy'])\n\n        early_stop = EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n\n        bgru_model.fit(\n            X_train, y_train,\n            epochs=5,\n            batch_size=128,\n            validation_split=0.2,\n            callbacks=[early_stop]\n            )\n\n\n        #saving the model\n        bgru_model.save(\"bgru_model.keras\")\n\n        test_loss, test_acc = bgru_model.evaluate(X_test, y_test)\n        print(\"Test Accuracy:\", test_acc)",
   "metadata": {
    "id": "QOnNjW65ENDS",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T12:25:24.373218Z",
     "iopub.execute_input": "2025-09-06T12:25:24.373573Z",
     "iopub.status.idle": "2025-09-06T12:53:52.831931Z",
     "shell.execute_reply.started": "2025-09-06T12:25:24.373531Z",
     "shell.execute_reply": "2025-09-06T12:53:52.831280Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#loading model\nimport tensorflow as tf\nbgru_modelmodel = tf.keras.models.load_model('bgru_model.keras')\n\n\n#Metrics: precision / recall / F1 / confusion matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\ntest_loss, test_acc = bgru_model.evaluate(X_test, y_test, verbose=0)\ny_prob = bgru_model.predict(X_test, batch_size=512, verbose=0)\ny_pred = np.argmax(y_prob, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n\naccuracy_bgru  = accuracy_score(y_true, y_pred)\nprecision_bgru = precision_score(y_true, y_pred, average='macro',   zero_division=0)\nrecall_bgru  = recall_score(y_true,  y_pred, average='macro',     zero_division=0)\nf1_bgru   = f1_score(y_true,      y_pred, average='macro',     zero_division=0)\ncm_bgru   = confusion_matrix(y_true, y_pred)\n\n\nprint(\"Accuracy:\", accuracy_bgru)\nprint(\"Precision (macro):\", precision_bgru)\nprint(\"Recall (macro):\", recall_bgru)\nprint(\"F1 (macro):\", f1_bgru)\nprint(\"Confusion Matrix:\\n\", cm_bgru)\n\n\n# Optional: class names if you used a LabelEncoder earlier\nlabels = label_encoder.classes_.tolist() if 'label_encoder' in globals() else None\nprint(\"\\nClassification Report:\\n\",\n      classification_report(y_true, y_pred, target_names=labels))\n",
   "metadata": {
    "id": "ZzS_3XwLEYFQ",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T12:53:52.833175Z",
     "iopub.execute_input": "2025-09-06T12:53:52.833432Z",
     "iopub.status.idle": "2025-09-06T12:59:15.019122Z",
     "shell.execute_reply.started": "2025-09-06T12:53:52.833413Z",
     "shell.execute_reply": "2025-09-06T12:59:15.018404Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot confusion matrix heatmap for Bidirectional GRU\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_bgru, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=labels if labels else None,\n            yticklabels=labels if labels else None)\n\nplt.title(\"Confusion Matrix - Bidirectional GRU Model\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.show()\n",
   "metadata": {
    "trusted": true,
    "id": "iNdyywS0ZCmE",
    "execution": {
     "iopub.status.busy": "2025-09-06T12:59:15.019921Z",
     "iopub.execute_input": "2025-09-06T12:59:15.020191Z",
     "iopub.status.idle": "2025-09-06T12:59:15.434044Z",
     "shell.execute_reply.started": "2025-09-06T12:59:15.020163Z",
     "shell.execute_reply": "2025-09-06T12:59:15.433196Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Lstm",
   "metadata": {
    "id": "BbwEGZTBEYw2"
   }
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, GRU, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Simple RNN Classifier\nmax_len = max_len_title + max_len_content + max_len_answer\nembedding_dim = embedding_matrix.shape[1]  # e.g., 100 if using glove.6B.100d.txt\nwith tf.device('/GPU:0'):\n        lstm_model = Sequential([\n            Embedding(\n                input_dim=max_vocab,           # same as tokenizer num_words\n                output_dim=embedding_dim,      # dimension of GloVe vectors\n                weights=[embedding_matrix],    # use pre-trained GloVe embeddings\n                input_length=max_len,          # padded input length\n                trainable=False                # keep embeddings fixed\n            ),\n            LSTM(32),\n            Dropout(0.5),                      # regularization\n            Dense(10, activation='softmax')    # 10 classes (adjust if needed)\n        ])\n\n\n        lstm_model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy'])\n\n\n        early_stop = EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n\n\n        lstm_model.fit(\n            X_train, y_train,\n            epochs=5,\n            batch_size=128,\n            validation_split=0.2,\n            callbacks=[early_stop]\n            )\n\n        #saving the model\n        lstm_model.save(\"lstm_model.keras\")\n\n        test_loss, test_acc = lstm_model.evaluate(X_test, y_test)\n        print(\"Test Accuracy:\", test_acc)",
   "metadata": {
    "id": "_gj00-DdEarb",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T12:59:15.434761Z",
     "iopub.execute_input": "2025-09-06T12:59:15.434940Z",
     "iopub.status.idle": "2025-09-06T13:14:25.752685Z",
     "shell.execute_reply.started": "2025-09-06T12:59:15.434926Z",
     "shell.execute_reply": "2025-09-06T13:14:25.752009Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#loading model\nimport tensorflow as tf\nlstm_model = tf.keras.models.load_model('lstm_model.keras')\n\n\n#Metrics: precision / recall / F1 / confusion matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\ntest_loss, test_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\ny_prob = lstm_model.predict(X_test, batch_size=512, verbose=0)\ny_pred = np.argmax(y_prob, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n\naccuracy_lstm  = accuracy_score(y_true, y_pred)\nprecision_lstm = precision_score(y_true, y_pred, average='macro',   zero_division=0)\nrecall_lstm  = recall_score(y_true,  y_pred, average='macro',     zero_division=0)\nf1_lstm   = f1_score(y_true,      y_pred, average='macro',     zero_division=0)\ncm_lstm   = confusion_matrix(y_true, y_pred)\n\n\nprint(\"Accuracy:\", accuracy_lstm)\nprint(\"Precision (macro):\", precision_lstm)\nprint(\"Recall (macro):\", recall_lstm)\nprint(\"F1 (macro):\", f1_lstm)\nprint(\"Confusion Matrix:\\n\", cm_lstm)\n\n\n# Optional: class names if you used a LabelEncoder earlier\nlabels = label_encoder.classes_.tolist() if 'label_encoder' in globals() else None\nprint(\"\\nClassification Report:\\n\",\n      classification_report(y_true, y_pred, target_names=labels))\n",
   "metadata": {
    "id": "NTby9AFWEgkq",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T13:14:25.753776Z",
     "iopub.execute_input": "2025-09-06T13:14:25.753999Z",
     "iopub.status.idle": "2025-09-06T13:17:23.787525Z",
     "shell.execute_reply.started": "2025-09-06T13:14:25.753982Z",
     "shell.execute_reply": "2025-09-06T13:17:23.786613Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot confusion matrix heatmap for LSTM\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_lstm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=labels if labels else None,\n            yticklabels=labels if labels else None)\n\nplt.title(\"Confusion Matrix - LSTM Model\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.show()\n",
   "metadata": {
    "trusted": true,
    "id": "VGV6GtNYZCmE",
    "execution": {
     "iopub.status.busy": "2025-09-06T13:17:23.788431Z",
     "iopub.execute_input": "2025-09-06T13:17:23.788814Z",
     "iopub.status.idle": "2025-09-06T13:17:24.191062Z",
     "shell.execute_reply.started": "2025-09-06T13:17:23.788787Z",
     "shell.execute_reply": "2025-09-06T13:17:24.190320Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# bidirectional lstm",
   "metadata": {
    "id": "ksekQHOGEicW"
   }
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, GRU, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\n# Simple RNN Classifier\nmax_len = max_len_title + max_len_content + max_len_answer\nembedding_dim = embedding_matrix.shape[1]  # e.g., 100 if using glove.6B.100d.txt\nwith tf.device('/GPU:0'):\n        blstm_model = Sequential([\n            Embedding(\n                input_dim=max_vocab,           # same as tokenizer num_words\n                output_dim=embedding_dim,      # dimension of GloVe vectors\n                weights=[embedding_matrix],    # use pre-trained GloVe embeddings\n                input_length=max_len,          # padded input length\n                trainable=False                # keep embeddings fixed\n            ),\n            Bidirectional(LSTM(32)),\n            Dropout(0.5),                      # regularization\n            Dense(10, activation='softmax')    # 10 classes (adjust if needed)\n        ])\n\n\n        blstm_model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy'])\n\n\n        early_stop = EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n\n\n        blstm_model.fit(\n            X_train, y_train,\n            epochs=5,\n            batch_size=128,\n            validation_split=0.2,\n            callbacks=[early_stop]\n            )\n\n        #saving the model\n        blstm_model.save(\"blstm_model.keras\")\n\n        test_loss, test_acc = blstm_model.evaluate(X_test, y_test)\n        print(\"Test Accuracy:\", test_acc)",
   "metadata": {
    "id": "jP9YtUpBEmwu",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T13:17:24.191895Z",
     "iopub.execute_input": "2025-09-06T13:17:24.192206Z",
     "iopub.status.idle": "2025-09-06T13:45:42.145175Z",
     "shell.execute_reply.started": "2025-09-06T13:17:24.192177Z",
     "shell.execute_reply": "2025-09-06T13:45:42.144350Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#loading model\nimport tensorflow as tf\nblstm_model = tf.keras.models.load_model('blstm_model.keras')\n\n\n#Metrics: precision / recall / F1 / confusion matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\ntest_loss, test_acc = blstm_model.evaluate(X_test, y_test, verbose=0)\ny_prob = blstm_model.predict(X_test, batch_size=512, verbose=0)\ny_pred = np.argmax(y_prob, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n\naccuracy_blstm  = accuracy_score(y_true, y_pred)\nprecision_blstm = precision_score(y_true, y_pred, average='macro',   zero_division=0)\nrecall_blstm  = recall_score(y_true,  y_pred, average='macro',     zero_division=0)\nf1_blstm   = f1_score(y_true,      y_pred, average='macro',     zero_division=0)\ncm_blstm   = confusion_matrix(y_true, y_pred)\n\n\nprint(\"Accuracy:\", accuracy_blstm)\nprint(\"Precision (macro):\", precision_blstm)\nprint(\"Recall (macro):\", recall_blstm)\nprint(\"F1 (macro):\", f1_blstm)\nprint(\"Confusion Matrix:\\n\", cm_blstm)\n\n\n# Optional: class names if you used a LabelEncoder earlier\nlabels = label_encoder.classes_.tolist() if 'label_encoder' in globals() else None\nprint(\"\\nClassification Report:\\n\",\n      classification_report(y_true, y_pred, target_names=labels))\n",
   "metadata": {
    "id": "Pm2J0hgoFxIn",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T13:45:42.146340Z",
     "iopub.execute_input": "2025-09-06T13:45:42.146600Z",
     "iopub.status.idle": "2025-09-06T13:51:10.414822Z",
     "shell.execute_reply.started": "2025-09-06T13:45:42.146572Z",
     "shell.execute_reply": "2025-09-06T13:51:10.414006Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot confusion matrix heatmap for Bidirectional LSTM\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_blstm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=labels if labels else None,\n            yticklabels=labels if labels else None)\n\nplt.title(\"Confusion Matrix - Bidirectional LSTM Model\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.show()\n",
   "metadata": {
    "trusted": true,
    "id": "H8W_Kt3xZCmE",
    "execution": {
     "iopub.status.busy": "2025-09-06T13:51:10.415720Z",
     "iopub.execute_input": "2025-09-06T13:51:10.415999Z",
     "iopub.status.idle": "2025-09-06T13:51:10.841170Z",
     "shell.execute_reply.started": "2025-09-06T13:51:10.415974Z",
     "shell.execute_reply": "2025-09-06T13:51:10.840478Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# dnn\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport numpy as np\n\n# DNN Classifier\nmax_len = max_len_title + max_len_content + max_len_answer\nembedding_dim = embedding_matrix.shape[1]  # e.g., 100 if using glove.6B.100d.txt\n\nwith tf.device('/GPU:0'):\n    dnn_model = Sequential([\n        Embedding(\n            input_dim=max_vocab,           # same as tokenizer num_words\n            output_dim=embedding_dim,      # dimension of GloVe vectors\n            weights=[embedding_matrix],    # use pre-trained GloVe embeddings\n            input_length=max_len,          # padded input length\n            trainable=False                # keep embeddings fixed\n        ),\n        Flatten(),                         # flatten embeddings into vector\n        Dense(128, activation='relu'),     # fully connected layer\n        Dropout(0.5),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(10, activation='softmax')    # adjust output classes if needed\n    ])\n\n    dnn_model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    early_stop = EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True\n    )\n\n    dnn_model.fit(\n        X_train, y_train,\n        epochs=5,\n        batch_size=64,\n        validation_split=0.2,\n        callbacks=[early_stop]\n    )\n\n    # saving the model\n    dnn_model.save(\"dnn_model.keras\")\n\n    test_loss, test_acc = dnn_model.evaluate(X_test, y_test)\n    print(\"Test Accuracy:\", test_acc)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T14:15:37.310972Z",
     "iopub.execute_input": "2025-09-06T14:15:37.311267Z",
     "iopub.status.idle": "2025-09-06T14:18:12.404166Z",
     "shell.execute_reply.started": "2025-09-06T14:15:37.311246Z",
     "shell.execute_reply": "2025-09-06T14:18:12.403392Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# -------------------- Evaluation --------------------\ndnn_model = tf.keras.models.load_model('dnn_model.keras')\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Predictions\ny_prob = dnn_model.predict(X_test, batch_size=512, verbose=0)\ny_pred = np.argmax(y_prob, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Metrics\naccuracy_dnn  = accuracy_score(y_true, y_pred)\nprecision_dnn = precision_score(y_true, y_pred, average='macro', zero_division=0)\nrecall_dnn    = recall_score(y_true, y_pred, average='macro', zero_division=0)\nf1_dnn        = f1_score(y_true, y_pred, average='macro', zero_division=0)\ncm_dnn        = confusion_matrix(y_true, y_pred)\n\nprint(\"Accuracy:\", accuracy_dnn)\nprint(\"Precision (macro):\", precision_dnn)\nprint(\"Recall (macro):\", recall_dnn)\nprint(\"F1 (macro):\", f1_dnn)\nprint(\"Confusion Matrix:\\n\", cm_dnn)\n\n# Optional: class names if you used a LabelEncoder earlier\nlabels = label_encoder.classes_.tolist() if 'label_encoder' in globals() else None\nprint(\"\\nClassification Report:\\n\",\n      classification_report(y_true, y_pred, target_names=labels))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T14:18:21.504363Z",
     "iopub.execute_input": "2025-09-06T14:18:21.504643Z",
     "iopub.status.idle": "2025-09-06T14:18:29.726213Z",
     "shell.execute_reply.started": "2025-09-06T14:18:21.504619Z",
     "shell.execute_reply": "2025-09-06T14:18:29.725441Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\n# Confusion matrix heatmap\nplt.figure(figsize=(8,6))\nsns.heatmap(cm_dnn, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=labels if labels else None,\n            yticklabels=labels if labels else None)\n\nplt.title(\"Confusion Matrix - DNN Model\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.show()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T14:18:59.008785Z",
     "iopub.execute_input": "2025-09-06T14:18:59.009061Z",
     "iopub.status.idle": "2025-09-06T14:18:59.430339Z",
     "shell.execute_reply.started": "2025-09-06T14:18:59.009033Z",
     "shell.execute_reply": "2025-09-06T14:18:59.429563Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# model comparison",
   "metadata": {
    "id": "AwDl3DMmE2EV"
   }
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Model names (7 models including DNN)\nmodel_names = ['dnn', 'rnn', 'bidirectional rnn', 'gru', 'bidirectional gru', 'lstm', 'bidirectional lstm']\naccuracy_scores = [accuracy_dnn, accuracy_rnn, accuracy_brnn, accuracy_gru, accuracy_bgru, accuracy_lstm, accuracy_blstm]\naccuracy_scores_percent = [a * 100 for a in accuracy_scores]\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(model_names, accuracy_scores_percent, color='skyblue')\nplt.xlabel(\"Models\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"Comparison of Model Accuracy\")\nplt.ylim(0, 100)  # y-axis in percentage\n\n# Add value labels on top of bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f'{height:.1f}%', \n             ha='center', va='bottom')\n\nplt.show()\n",
   "metadata": {
    "id": "Oax3hAzDE3yY",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T14:20:54.132681Z",
     "iopub.execute_input": "2025-09-06T14:20:54.132926Z",
     "iopub.status.idle": "2025-09-06T14:20:54.297406Z",
     "shell.execute_reply.started": "2025-09-06T14:20:54.132909Z",
     "shell.execute_reply": "2025-09-06T14:20:54.296569Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Precision, Recall, F1 score Comparison\nmodel_names = ['dnn', 'rnn', 'bidirectional rnn', 'gru', 'bidirectional gru', 'lstm', 'bidirectional lstm']\nprecision_scores = [precision_dnn, precision_rnn, precision_brnn, precision_gru, precision_bgru, precision_lstm, precision_blstm]\nrecall_scores    = [recall_dnn,   recall_rnn,   recall_brnn,   recall_gru,   recall_bgru,   recall_lstm,   recall_blstm]\nf1_scores        = [f1_dnn,       f1_rnn,       f1_brnn,       f1_gru,       f1_bgru,       f1_lstm,       f1_blstm]\n\nx = range(len(model_names))\nwidth = 0.25  \n\nfig, ax1 = plt.subplots(figsize=(12, 6))\n\n# Bars\nrects1 = ax1.bar([i - width for i in x], precision_scores, width, label='Precision')\nrects2 = ax1.bar(x, recall_scores, width, label='Recall')\nrects3 = ax1.bar([i + width for i in x], f1_scores, width, label='F1 Score')\n\n# Labels and title\nax1.set_xlabel('Model Names')\nax1.set_ylabel('Scores')\nax1.set_title('Precision, Recall, and F1 Comparison')\nax1.set_xticks(x)\nax1.set_xticklabels(model_names)\nax1.legend()\n\nplt.show()\n",
   "metadata": {
    "id": "rDUI_avNw1Ul",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T14:22:52.778536Z",
     "iopub.execute_input": "2025-09-06T14:22:52.779174Z",
     "iopub.status.idle": "2025-09-06T14:22:52.999424Z",
     "shell.execute_reply.started": "2025-09-06T14:22:52.779148Z",
     "shell.execute_reply": "2025-09-06T14:22:52.998682Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nmodel_names = ['dnn', 'rnn', 'bidirectional rnn', 'gru', 'bidirectional gru', 'lstm', 'bidirectional lstm']\nconfusion_matrices = [cm_dnn, cm_rnn, cm_brnn, cm_gru, cm_bgru, cm_lstm, cm_blstm]\n# Plot confusion matrices (2 rows x 4 columns)\nfig, axes = plt.subplots(2, 4, figsize=(22, 10))\naxes = axes.flatten()  # Flatten 2D array of axes for easy iteration\nfor i, ax in enumerate(axes[:len(model_names)]):\n    sns.heatmap(confusion_matrices[i], annot=True, fmt='d', cmap='Blues', ax=ax)\n    ax.set_title(f'{model_names[i]} Confusion Matrix')\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n# Hide unused subplot (since 2x4 = 8 slots but we have 7 models)\nif len(model_names) < len(axes):\n    axes[-1].axis('off')\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "id": "sp40fV1pwyeZ",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-09-06T14:23:17.764224Z",
     "iopub.execute_input": "2025-09-06T14:23:17.764795Z",
     "iopub.status.idle": "2025-09-06T14:23:22.539867Z",
     "shell.execute_reply.started": "2025-09-06T14:23:17.764773Z",
     "shell.execute_reply": "2025-09-06T14:23:22.539099Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n## Section 3: cse440project-skipgram-with-7-models\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:39:41.074598Z",
     "iopub.status.busy": "2025-09-06T08:39:41.073818Z",
     "iopub.status.idle": "2025-09-06T08:40:04.897957Z",
     "shell.execute_reply": "2025-09-06T08:40:04.896938Z",
     "shell.execute_reply.started": "2025-09-06T08:39:41.074564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== COMPLETE MULTI-GPU SETUP ====================\n",
    "# RUN THIS FIRST AFTER RESTARTING KERNEL\n",
    "# ==================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"🔄 Restarting kernel and setting up multi-GPU...\")\n",
    "\n",
    "# Clean up any existing strategies\n",
    "if 'strategy' in globals():\n",
    "    del globals()['strategy']\n",
    "if 'global_strategy' in globals():\n",
    "    del globals()['global_strategy']\n",
    "\n",
    "# Force TensorFlow to use both GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# Clear any existing TensorFlow sessions\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Configure physical GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Create SINGLE strategy instance\n",
    "if len(gpus) >= 2:\n",
    "    print(f\"✅ Found {len(gpus)} GPUs\")\n",
    "    \n",
    "    # Enable memory growth\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "    # Create ONLY ONE strategy instance\n",
    "    global_strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"🎯 Created MirroredStrategy with {global_strategy.num_replicas_in_sync} replicas\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Using single GPU or CPU\")\n",
    "    global_strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"✅ Strategy setup complete! Use 'global_strategy' everywhere.\")\n",
    "print(f\"📊 Number of replicas: {global_strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# Show GPU status\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
    "\n",
    "# Test with a simple operation\n",
    "print(\"\\n🧪 Testing strategy with simple operation...\")\n",
    "with global_strategy.scope():\n",
    "    test_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "    result = tf.matmul(test_tensor, test_tensor)\n",
    "    print(f\"Test operation successful: {result.numpy()}\")\n",
    "\n",
    "print(\"🚀 Multi-GPU setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:40:37.756311Z",
     "iopub.status.busy": "2025-09-06T08:40:37.754553Z",
     "iopub.status.idle": "2025-09-06T08:40:37.910846Z",
     "shell.execute_reply": "2025-09-06T08:40:37.910274Z",
     "shell.execute_reply.started": "2025-09-06T08:40:37.756276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== ULTIMATE MULTI-CORE CPU SETUP ====================\n",
    "# PUT THIS IN THE VERY FIRST CELL OF YOUR NOTEBOOK\n",
    "# ======================================================================\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from threadpoolctl import threadpool_limits\n",
    "import threading\n",
    "\n",
    "print(\"🚀 INITIALIZING MULTI-CORE PARALLEL EXECUTION...\")\n",
    "\n",
    "# Get total available CPU cores\n",
    "TOTAL_CORES = mp.cpu_count()\n",
    "print(f\"✅ Found {TOTAL_CORES} CPU cores\")\n",
    "\n",
    "# Configure environment for maximum parallel performance\n",
    "os.environ['OMP_NUM_THREADS'] = str(TOTAL_CORES)  # OpenMP threads\n",
    "os.environ['MKL_NUM_THREADS'] = str(TOTAL_CORES)  # Intel MKL\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = str(TOTAL_CORES)  # OpenBLAS\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = str(TOTAL_CORES)  # macOS Accelerate\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = str(TOTAL_CORES)  # NumExpr\n",
    "os.environ['JOBLIB_NUM_CPUS'] = str(TOTAL_CORES)  # Joblib\n",
    "\n",
    "print(\"📊 Environment configured for parallel execution\")\n",
    "\n",
    "# Set thread limits for numerical libraries\n",
    "try:\n",
    "    with threadpool_limits(limits=TOTAL_CORES, user_api='blas'):\n",
    "        with threadpool_limits(limits=TOTAL_CORES, user_api='openmp'):\n",
    "            print(\"✅ BLAS and OpenMP threads configured\")\n",
    "except:\n",
    "    print(\"ℹ️ threadpoolctl not available, using environment variables only\")\n",
    "\n",
    "# Global parallel context\n",
    "class ParallelExecutor:\n",
    "    def __init__(self, n_jobs=-1):\n",
    "        self.n_jobs = n_jobs if n_jobs != -1 else TOTAL_CORES\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "\n",
    "parallel_executor = ParallelExecutor(n_jobs=-1)\n",
    "\n",
    "# Initialize parallel libraries\n",
    "def enable_parallel_pandas():\n",
    "    \"\"\"Enable parallel operations for pandas\"\"\"\n",
    "    try:\n",
    "        import swifter\n",
    "        print(\"✅ Swifter enabled for parallel pandas operations\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"ℹ️ Install 'swifter' for pandas parallelization: !pip install swifter\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        from pandarallel import pandarallel\n",
    "        pandarallel.initialize(nb_workers=TOTAL_CORES, progress_bar=False)\n",
    "        print(\"✅ Pandarallel initialized\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "enable_parallel_pandas()\n",
    "\n",
    "print(\"🎯 All CPU operations configured for parallel execution!\")\n",
    "\n",
    "# Test parallel execution\n",
    "print(\"\\n🧪 Testing parallel operations...\")\n",
    "\n",
    "# Test with threadpoolctl to verify numpy uses all cores\n",
    "try:\n",
    "    with threadpool_limits(limits=TOTAL_CORES):\n",
    "        large_array = np.random.rand(1000, 1000)\n",
    "        result = np.dot(large_array, large_array.T)\n",
    "        print(\"✅ NumPy parallel operation test completed\")\n",
    "except:\n",
    "    large_array = np.random.rand(1000, 1000)\n",
    "    result = np.dot(large_array, large_array.T)\n",
    "    print(\"✅ NumPy operation completed (thread limits not available)\")\n",
    "\n",
    "print(\"\\n✅ Setup complete! All subsequent CPU operations will use all cores!\")\n",
    "\n",
    "# Show current CPU status\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CURRENT CPU STATUS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total cores: {TOTAL_CORES}\")\n",
    "print(f\"Active threads: {threading.active_count()}\")\n",
    "\n",
    "# Performance tips\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE TIPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"📋 For pandas: df.swifter.apply(func) - uses all cores\")\n",
    "print(\"📋 For sklearn: set n_jobs=-1\")\n",
    "print(\"📋 For joblib: Parallel(n_jobs=-1)\")\n",
    "print(\"📋 Most numpy operations auto-parallelize\")\n",
    "\n",
    "# Verify environment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENVIRONMENT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "for var in ['OMP_NUM_THREADS', 'MKL_NUM_THREADS', 'NUMEXPR_NUM_THREADS']:\n",
    "    print(f\"{var}: {os.environ.get(var, 'Not set')}\")\n",
    "\n",
    "print(\"\\n🔥 Ready for maximum CPU performance! All cores will be utilized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-06T08:40:43.854531Z",
     "iopub.status.busy": "2025-09-06T08:40:43.853728Z",
     "iopub.status.idle": "2025-09-06T08:41:39.927472Z",
     "shell.execute_reply": "2025-09-06T08:41:39.926651Z",
     "shell.execute_reply.started": "2025-09-06T08:40:43.854504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, GRU, LSTM, Bidirectional, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:42:19.482189Z",
     "iopub.status.busy": "2025-09-06T08:42:19.481588Z",
     "iopub.status.idle": "2025-09-06T08:42:24.069114Z",
     "shell.execute_reply": "2025-09-06T08:42:24.068340Z",
     "shell.execute_reply.started": "2025-09-06T08:42:19.482164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "train_df = pd.read_csv('/kaggle/input/440project/Question Answer Classification Dataset 2 Training.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/440project/Updated Question Answer Classification DatasetTest.csv')\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Testing set shape:\", test_df.shape)\n",
    "print(\"\\nTraining set columns:\", train_df.columns.tolist())\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(train_df['Class'].value_counts())\n",
    "print(\"\\nClass distribution in testing set:\")\n",
    "print(test_df['Class'].value_counts())\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample from training data:\")\n",
    "print(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:15:08.048472Z",
     "iopub.status.busy": "2025-09-06T06:15:08.048285Z",
     "iopub.status.idle": "2025-09-06T06:15:08.052220Z",
     "shell.execute_reply": "2025-09-06T06:15:08.051522Z",
     "shell.execute_reply.started": "2025-09-06T06:15:08.048457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df_train=train_df\n",
    "# df_test=test_df\n",
    "# train_df=df_test.sample(50000,replace=True)\n",
    "# test_df=df_test.sample(20000)\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:42:29.008513Z",
     "iopub.status.busy": "2025-09-06T08:42:29.007631Z",
     "iopub.status.idle": "2025-09-06T08:42:29.012604Z",
     "shell.execute_reply": "2025-09-06T08:42:29.011918Z",
     "shell.execute_reply.started": "2025-09-06T08:42:29.008481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:42:32.749121Z",
     "iopub.status.busy": "2025-09-06T08:42:32.748783Z",
     "iopub.status.idle": "2025-09-06T08:45:07.543085Z",
     "shell.execute_reply": "2025-09-06T08:45:07.542283Z",
     "shell.execute_reply.started": "2025-09-06T08:42:32.749100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already available\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Optimized text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float) or text is None:  # Handle NaN values\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Replace all whitespace variants with single spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # This handles \\n, \\t, \\r, \\\\n, multiple spaces, etc.\n",
    "    \n",
    "    # Step 2: Remove special characters but keep letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Step 3: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 4: Remove extra whitespace (should be redundant but safe)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to process text in parallel\n",
    "def preprocess_parallel(texts, n_cores=None):\n",
    "    if n_cores is None:\n",
    "        n_cores = cpu_count()  # Use all available cores\n",
    "    \n",
    "    with Pool(n_cores) as pool:\n",
    "        results = pool.map(preprocess_text, texts)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Apply preprocessing to training data using all cores\n",
    "print(\"Preprocessing training text data using all CPU cores...\")\n",
    "print(f\"Available CPU cores: {cpu_count()}\")\n",
    "\n",
    "# Process in parallel\n",
    "train_df['processed_text'] = preprocess_parallel(train_df['QA Text'].tolist())\n",
    "test_df['processed_text'] = preprocess_parallel(test_df['QA Text'].tolist())\n",
    "\n",
    "print(\"Preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:17:52.451503Z",
     "iopub.status.busy": "2025-09-06T06:17:52.451171Z",
     "iopub.status.idle": "2025-09-06T06:17:52.460651Z",
     "shell.execute_reply": "2025-09-06T06:17:52.459818Z",
     "shell.execute_reply.started": "2025-09-06T06:17:52.451473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test the simplified preprocessing function\n",
    "test_texts = [\n",
    "    \"Question Title:\\nWhat are the names of the 206 bones?\\nQuestion Content:\\n\\nBest Answer:\\\\n206 bones of the human body:\\\\n\\\\nSKULL - AXIAL SKELETON\",\n",
    "    \"can you tell me the university name\\\\nin which i can do distance mphil foods & nutrition?\",\n",
    "    \"Suggestions or Advice on tracking someone\\nwho owes you money from a small claims verdict and \\\"skipped town?\\\"\"\n",
    "]\n",
    "\n",
    "print(\"Testing simplified preprocessing function:\")\n",
    "print(\"=\" * 60)\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"Original {i+1}: {repr(text)}\")\n",
    "    processed = preprocess_text(text)\n",
    "    print(f\"Processed {i+1}: {repr(processed)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:17:52.461778Z",
     "iopub.status.busy": "2025-09-06T06:17:52.461497Z",
     "iopub.status.idle": "2025-09-06T06:17:52.479050Z",
     "shell.execute_reply": "2025-09-06T06:17:52.478372Z",
     "shell.execute_reply.started": "2025-09-06T06:17:52.461736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check preprocessing results\n",
    "print(\"\\nSample of preprocessed text:\")\n",
    "print(\"Original:\", train_df['QA Text'].iloc[0][:100] + \"...\")\n",
    "print(\"Processed:\", train_df['processed_text'].iloc[0][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Skip-gram Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:45:32.732099Z",
     "iopub.status.busy": "2025-09-06T08:45:32.731791Z",
     "iopub.status.idle": "2025-09-06T08:45:36.534055Z",
     "shell.execute_reply": "2025-09-06T08:45:36.533221Z",
     "shell.execute_reply.started": "2025-09-06T08:45:32.732078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare sentences for Word2Vec training\n",
    "sentences = [text.split() for text in train_df['processed_text'] if text.strip() != '']\n",
    "\n",
    "# Check sentence statistics\n",
    "sentence_lengths = [len(sentence) for sentence in sentences]\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Average sentence length: {np.mean(sentence_lengths):.2f}\")\n",
    "print(f\"Max sentence length: {max(sentence_lengths)}\")\n",
    "print(f\"Min sentence length: {min(sentence_lengths)}\")\n",
    "\n",
    "# Plot sentence length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentence_lengths, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Skip-gram Embeddings (Adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v7: multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:46:00.740970Z",
     "iopub.status.busy": "2025-09-06T08:46:00.740408Z",
     "iopub.status.idle": "2025-09-06T08:46:21.481143Z",
     "shell.execute_reply": "2025-09-06T08:46:21.480379Z",
     "shell.execute_reply.started": "2025-09-06T08:46:00.740947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "print(\"⚡ Ultra-optimized Word2Vec for 280K rows...\")\n",
    "\n",
    "# Use ALL available CPU cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(f\"Using {cores} CPU cores\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize the model\n",
    "model = Word2Vec(\n",
    "    vector_size=100,      # Smaller = faster\n",
    "    window=5,             # Smaller window\n",
    "    min_count=2,\n",
    "    sg=1,                 # Skip-gram\n",
    "    workers=cores,        # Use ALL cores\n",
    "    epochs=3,             # Fewer epochs\n",
    "    batch_words=100000,   # Larger batches\n",
    "    alpha=0.025,\n",
    "    negative=5,           # Fewer negative samples\n",
    "    sample=1e-4,          # More aggressive subsampling\n",
    ")\n",
    "\n",
    "# Build vocabulary FIRST (this is fast)\n",
    "print(\"📚 Building vocabulary...\")\n",
    "vocab_start = time.time()\n",
    "model.build_vocab(sentences)\n",
    "vocab_time = time.time() - vocab_start\n",
    "print(f\"Vocabulary built in {vocab_time:.2f}s - {len(model.wv.key_to_index):,} words\")\n",
    "\n",
    "# NOW train the model (this is the actual training)\n",
    "print(\"🚀 Training Skip-gram model...\")\n",
    "train_start = time.time()\n",
    "model.train(\n",
    "    sentences, \n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=model.epochs,\n",
    "    compute_loss=True  # Track training progress\n",
    ")\n",
    "training_time = time.time() - train_start\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"✅ Training completed in {training_time:.2f}s\")\n",
    "print(f\"📊 Total time (vocab + training): {total_time:.2f}s\")\n",
    "print(f\"⚡ Speed: {len(sentences)*3/training_time:.0f} sentences/second\")\n",
    "\n",
    "# Check the model actually learned something\n",
    "print(f\"\\n🎯 Model trained successfully!\")\n",
    "print(f\"Vocabulary size: {len(model.wv.key_to_index):,}\")\n",
    "print(f\"Final loss: {model.get_latest_training_loss():.2f}\")\n",
    "\n",
    "# Test with sample words\n",
    "test_words = ['scienc', 'educ', 'polit', 'question', 'answer']\n",
    "print(\"\\n🔍 Testing embeddings:\")\n",
    "for word in test_words:\n",
    "    if word in model.wv:\n",
    "        similar = model.wv.most_similar(word, topn=2)\n",
    "        print(f\"'{word}': {[w[0] for w in similar]}\")\n",
    "    else:\n",
    "        print(f\"'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Sequence Preparation (Adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:18:17.195171Z",
     "iopub.status.busy": "2025-09-06T06:18:17.194808Z",
     "iopub.status.idle": "2025-09-06T06:18:17.202109Z",
     "shell.execute_reply": "2025-09-06T06:18:17.201207Z",
     "shell.execute_reply.started": "2025-09-06T06:18:17.195141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:46:36.898886Z",
     "iopub.status.busy": "2025-09-06T08:46:36.898577Z",
     "iopub.status.idle": "2025-09-06T08:46:57.736675Z",
     "shell.execute_reply": "2025-09-06T08:46:57.735920Z",
     "shell.execute_reply.started": "2025-09-06T08:46:36.898844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['processed_text'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_df['processed_text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_df['processed_text'])\n",
    "\n",
    "# Analyze sequence lengths\n",
    "train_lengths = [len(seq) for seq in X_train]\n",
    "test_lengths = [len(seq) for seq in X_test]\n",
    "\n",
    "print(f\"Average training sequence length: {np.mean(train_lengths):.2f}\")\n",
    "print(f\"Max training sequence length: {max(train_lengths)}\")\n",
    "print(f\"95th percentile: {np.percentile(train_lengths, 95):.2f}\")\n",
    "print(f\"99th percentile: {np.percentile(train_lengths, 99):.2f}\")\n",
    "\n",
    "# Determine optimal max sequence length (using 95th percentile)\n",
    "max_len = int(np.percentile(train_lengths, 95))\n",
    "print(f\"Using sequence length: {max_len} (95th percentile)\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Training sequences shape: {X_train.shape}\")\n",
    "print(f\"Testing sequences shape: {X_test.shape}\")\n",
    "\n",
    "# Check how much data we're truncating\n",
    "long_sequences = sum(1 for length in train_lengths if length > max_len)\n",
    "print(f\"Percentage of sequences longer than {max_len}: {(long_sequences/len(train_lengths))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding Matrix (Adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:47:08.159355Z",
     "iopub.status.busy": "2025-09-06T08:47:08.158709Z",
     "iopub.status.idle": "2025-09-06T08:47:12.288746Z",
     "shell.execute_reply": "2025-09-06T08:47:12.287913Z",
     "shell.execute_reply.started": "2025-09-06T08:47:08.159333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create embedding matrix with the larger embedding dimension\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "found_words = 0\n",
    "not_found_words = 0\n",
    "low_freq_words = 0\n",
    "min_count = 2  # Define min_count since it was used in your Word2Vec setup\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    \n",
    "    # Check if word is in our trained embeddings - use 'model' instead of 'skipgram_model'\n",
    "    if word in model.wv:\n",
    "        embedding_matrix[i] = model.wv[word]\n",
    "        found_words += 1\n",
    "    else:\n",
    "        # Check if this is a low frequency word that was filtered out\n",
    "        word_freq = tokenizer.word_counts[word]\n",
    "        if word_freq < min_count:\n",
    "            low_freq_words += 1\n",
    "        else:\n",
    "            not_found_words += 1\n",
    "        \n",
    "        # Initialize with random values for unknown words\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "print(f\"Words found in Word2Vec vocabulary: {found_words:,}\")\n",
    "print(f\"Words not found (but should be): {not_found_words:,}\")\n",
    "print(f\"Low frequency words filtered out (<{min_count} occurrences): {low_freq_words:,}\")\n",
    "print(f\"Coverage of meaningful words: {(found_words/(found_words + not_found_words))*100:.2f}%\")\n",
    "\n",
    "# Prepare labels\n",
    "label_map = {label: idx for idx, label in enumerate(train_df['Class'].unique())}\n",
    "reverse_label_map = {idx: label for label, idx in label_map.items()}\n",
    "\n",
    "y_train = train_df['Class'].map(label_map).values\n",
    "y_test = test_df['Class'].map(label_map).values\n",
    "num_classes = len(label_map)\n",
    "\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(\"Label mapping:\", label_map)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "for label, idx in label_map.items():\n",
    "    count = sum(y_train == idx)\n",
    "    print(f\"  {label}: {count} samples ({count/len(y_train)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation Functions (Adjusted for Larger Sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:18:42.132160Z",
     "iopub.status.busy": "2025-09-06T06:18:42.131941Z",
     "iopub.status.idle": "2025-09-06T06:18:42.137904Z",
     "shell.execute_reply": "2025-09-06T06:18:42.137199Z",
     "shell.execute_reply.started": "2025-09-06T06:18:42.132143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Model creation functions optimized for longer sequences\n",
    "# # def create_dnn_model():\n",
    "# #     model = Sequential([\n",
    "# #         Embedding(vocab_size, embedding_dim, \n",
    "# #                  weights=[embedding_matrix], \n",
    "# #                  input_length=max_len, \n",
    "# #                  trainable=False),\n",
    "# #         tf.keras.layers.GlobalAveragePooling1D(),  # Better than Flatten for long sequences\n",
    "# #         Dense(256, activation='relu'),\n",
    "# #         Dropout(0.6),\n",
    "# #         Dense(128, activation='relu'),\n",
    "# #         Dropout(0.5),\n",
    "# #         Dense(64, activation='relu'),\n",
    "# #         Dropout(0.4),\n",
    "# #         Dense(num_classes, activation='softmax')\n",
    "# #     ])\n",
    "    \n",
    "# #     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "# #                   loss='sparse_categorical_crossentropy', \n",
    "# #                   metrics=['accuracy'])\n",
    "# #     return model\n",
    "# def create_improved_dnn_model():\n",
    "#     model = Sequential([\n",
    "#         Embedding(vocab_size, embedding_dim, \n",
    "#                  weights=[embedding_matrix], \n",
    "#                  input_length=max_len, \n",
    "#                  trainable=True),  # Make trainable for fine-tuning\n",
    "        \n",
    "#         Conv1D(128, 5, activation='relu'),  # Add convolutional layer\n",
    "#         GlobalMaxPooling1D(),\n",
    "        \n",
    "#         Dense(512, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.5),\n",
    "        \n",
    "#         Dense(256, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.4),\n",
    "        \n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dropout(0.3),\n",
    "        \n",
    "#         Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower LR\n",
    "#                   loss='sparse_categorical_crossentropy', \n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_rnn_model(bidirectional=False):\n",
    "#     model = Sequential([\n",
    "#         Embedding(vocab_size, embedding_dim, \n",
    "#                  weights=[embedding_matrix], \n",
    "#                  input_length=max_len, \n",
    "#                  trainable=False)\n",
    "#     ])\n",
    "    \n",
    "#     if bidirectional:\n",
    "#         model.add(Bidirectional(SimpleRNN(128, return_sequences=False, dropout=0.3)))\n",
    "#     else:\n",
    "#         model.add(SimpleRNN(128, return_sequences=False, dropout=0.3))\n",
    "    \n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "#                   loss='sparse_categorical_crossentropy', \n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_gru_model(bidirectional=False):\n",
    "#     model = Sequential([\n",
    "#         Embedding(vocab_size, embedding_dim, \n",
    "#                  weights=[embedding_matrix], \n",
    "#                  input_length=max_len, \n",
    "#                  trainable=False)\n",
    "#     ])\n",
    "    \n",
    "#     if bidirectional:\n",
    "#         model.add(Bidirectional(GRU(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)))\n",
    "#     else:\n",
    "#         model.add(GRU(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.2))\n",
    "    \n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "#                   loss='sparse_categorical_crossentropy', \n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_lstm_model(bidirectional=False):\n",
    "#     model = Sequential([\n",
    "#         Embedding(vocab_size, embedding_dim, \n",
    "#                  weights=[embedding_matrix], \n",
    "#                  input_length=max_len, \n",
    "#                  trainable=False)\n",
    "#     ])\n",
    "    \n",
    "#     if bidirectional:\n",
    "#         model.add(Bidirectional(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)))\n",
    "#     else:\n",
    "#         model.add(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.2))\n",
    "    \n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "#                   loss='sparse_categorical_crossentropy', \n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Test model creation\n",
    "# test_model = create_dnn_model()\n",
    "# print(\"DNN model summary:\")\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:18:42.139222Z",
     "iopub.status.busy": "2025-09-06T06:18:42.138903Z",
     "iopub.status.idle": "2025-09-06T06:18:42.155075Z",
     "shell.execute_reply": "2025-09-06T06:18:42.154370Z",
     "shell.execute_reply.started": "2025-09-06T06:18:42.139194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Model creation functions with multi-GPU support\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "# from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, BatchNormalization, LSTM, Bidirectional, GRU, SimpleRNN, SpatialDropout1D, MaxPooling1D\n",
    "\n",
    "# def create_dnn_model():\n",
    "#     with global_strategy.scope():\n",
    "#         model = Sequential([\n",
    "#             Embedding(vocab_size, embedding_dim, \n",
    "#                      weights=[embedding_matrix], \n",
    "#                      input_length=max_len, \n",
    "#                      trainable=True),  # Enable fine-tuning\n",
    "            \n",
    "#             Conv1D(128, 5, activation='relu'),\n",
    "#             GlobalMaxPooling1D(),\n",
    "            \n",
    "#             Dense(512, activation='relu'),\n",
    "#             BatchNormalization(),\n",
    "#             Dropout(0.5),\n",
    "            \n",
    "#             Dense(256, activation='relu'),\n",
    "#             BatchNormalization(),\n",
    "#             Dropout(0.4),\n",
    "            \n",
    "#             Dense(128, activation='relu'),\n",
    "#             Dropout(0.3),\n",
    "            \n",
    "#             Dense(num_classes, activation='softmax')\n",
    "#         ])\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "#                       loss='sparse_categorical_crossentropy', \n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_rnn_model(bidirectional=False):\n",
    "#     with global_strategy.scope():\n",
    "#         model = Sequential([\n",
    "#             Embedding(vocab_size, embedding_dim, \n",
    "#                      weights=[embedding_matrix], \n",
    "#                      input_length=max_len, \n",
    "#                      trainable=True)  # Enable fine-tuning\n",
    "#         ])\n",
    "        \n",
    "#         if bidirectional:\n",
    "#             model.add(Bidirectional(SimpleRNN(256, return_sequences=False, dropout=0.4, recurrent_dropout=0.3)))\n",
    "#         else:\n",
    "#             model.add(SimpleRNN(256, return_sequences=False, dropout=0.4, recurrent_dropout=0.3))\n",
    "        \n",
    "#         model.add(Dense(128, activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(Dropout(0.5))\n",
    "#         model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), \n",
    "#                       loss='sparse_categorical_crossentropy', \n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_gru_model(bidirectional=False):\n",
    "#     with global_strategy.scope():\n",
    "#         model = Sequential([\n",
    "#             Embedding(vocab_size, embedding_dim, \n",
    "#                      weights=[embedding_matrix], \n",
    "#                      input_length=max_len, \n",
    "#                      trainable=True)  # Enable fine-tuning\n",
    "#         ])\n",
    "        \n",
    "#         if bidirectional:\n",
    "#             model.add(Bidirectional(GRU(256, return_sequences=False, dropout=0.4, recurrent_dropout=0.3)))\n",
    "#         else:\n",
    "#             model.add(GRU(256, return_sequences=False, dropout=0.4, recurrent_dropout=0.3))\n",
    "        \n",
    "#         model.add(Dense(128, activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(Dropout(0.5))\n",
    "#         model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), \n",
    "#                       loss='sparse_categorical_crossentropy', \n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_lstm_model(bidirectional=False):\n",
    "#     with global_strategy.scope():\n",
    "#         model = Sequential([\n",
    "#             Embedding(vocab_size, embedding_dim, \n",
    "#                      weights=[embedding_matrix], \n",
    "#                      input_length=max_len, \n",
    "#                      trainable=True)  # Enable fine-tuning\n",
    "#         ])\n",
    "        \n",
    "#         if bidirectional:\n",
    "#             model.add(Bidirectional(LSTM(256, return_sequences=False, dropout=0.4, recurrent_dropout=0.3)))\n",
    "#         else:\n",
    "#             model.add(LSTM(256, return_sequences=False, dropout=0.4, recurrent_dropout=0.3))\n",
    "        \n",
    "#         model.add(Dense(128, activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(Dropout(0.5))\n",
    "#         model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), \n",
    "#                       loss='sparse_categorical_crossentropy', \n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Test model creation\n",
    "# print(\"Testing model creation with multi-GPU...\")\n",
    "# test_model = create_dnn_model()\n",
    "# print(\"DNN model summary:\")\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU-underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:47:58.282906Z",
     "iopub.status.busy": "2025-09-06T08:47:58.282040Z",
     "iopub.status.idle": "2025-09-06T08:48:01.409398Z",
     "shell.execute_reply": "2025-09-06T08:48:01.408795Z",
     "shell.execute_reply.started": "2025-09-06T08:47:58.282848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model creation functions with multi-GPU support (REVISED FOR UNDERFITTING)\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, BatchNormalization, LSTM, Bidirectional, GRU, SimpleRNN, SpatialDropout1D, MaxPooling1D\n",
    "def create_dnn_model():\n",
    "    \"\"\"\n",
    "    Revised DNN/CNN model with less regularization and more capacity.\n",
    "    \"\"\"\n",
    "    with global_strategy.scope():\n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, embedding_dim, \n",
    "                     weights=[embedding_matrix], \n",
    "                     input_length=max_len, \n",
    "                     trainable=True),\n",
    "            \n",
    "            SpatialDropout1D(0.2),  # Better for embedding dropout\n",
    "            \n",
    "            # Use two convolutional layers to capture features\n",
    "            Conv1D(256, 5, activation='relu', padding='same'),\n",
    "            MaxPooling1D(2),\n",
    "            Conv1D(128, 3, activation='relu', padding='same'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            \n",
    "            # Deeper classifier with less dropout\n",
    "            Dense(512, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),  # Reduced from 0.5\n",
    "            \n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),  # Reduced from 0.4\n",
    "            \n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.1),  # Reduced from 0.3\n",
    "            \n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Higher learning rate\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Increased from 0.0001\n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_rnn_model(bidirectional=True): # Made bidirectional default\n",
    "    \"\"\"\n",
    "    Revised RNN model with stacked layers and less regularization.\n",
    "    \"\"\"\n",
    "    with global_strategy.scope():\n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, embedding_dim, \n",
    "                     weights=[embedding_matrix], \n",
    "                     input_length=max_len, \n",
    "                     trainable=True),\n",
    "            \n",
    "            SpatialDropout1D(0.2),\n",
    "        ])\n",
    "        \n",
    "        # Stacked RNN layers\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(SimpleRNN(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "            model.add(Bidirectional(SimpleRNN(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)))\n",
    "        else:\n",
    "            model.add(SimpleRNN(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "            model.add(SimpleRNN(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "        \n",
    "        # Simplified head with less dropout\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))  # Reduced from 0.5\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        # Higher learning rate\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_gru_model(bidirectional=True): # Made bidirectional default\n",
    "    \"\"\"\n",
    "    Revised GRU model with stacked layers for increased capacity.\n",
    "    \"\"\"\n",
    "    with global_strategy.scope():\n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, embedding_dim, \n",
    "                     weights=[embedding_matrix], \n",
    "                     input_length=max_len, \n",
    "                     trainable=True),\n",
    "            \n",
    "            SpatialDropout1D(0.2),\n",
    "        ])\n",
    "        \n",
    "        # Stacked GRU layers\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(GRU(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "            model.add(Bidirectional(GRU(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "        else:\n",
    "            model.add(GRU(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model.add(GRU(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))\n",
    "        \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))  # Reduced from 0.5\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        # Higher learning rate\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(bidirectional=True): # Made bidirectional default\n",
    "    \"\"\"\n",
    "    Revised LSTM model - now with stacked layers and less dropout.\n",
    "    \"\"\"\n",
    "    with global_strategy.scope():\n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, embedding_dim, \n",
    "                     weights=[embedding_matrix], \n",
    "                     input_length=max_len, \n",
    "                     trainable=True),\n",
    "            \n",
    "            SpatialDropout1D(0.2),\n",
    "        ])\n",
    "        \n",
    "        # Stacked LSTM layers for much greater capacity\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "            model.add(Bidirectional(LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "        else:\n",
    "            model.add(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model.add(LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))\n",
    "        \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))  # Reduced from 0.5\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        # Higher learning rate\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Test model creation with the most promising model\n",
    "print(\"Testing revised model creation with multi-GPU...\")\n",
    "test_model = create_lstm_model(bidirectional=True)\n",
    "print(\"Revised LSTM model summary:\")\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Trying for Better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:18:44.236294Z",
     "iopub.status.busy": "2025-09-06T06:18:44.235996Z",
     "iopub.status.idle": "2025-09-06T06:18:44.242713Z",
     "shell.execute_reply": "2025-09-06T06:18:44.241861Z",
     "shell.execute_reply.started": "2025-09-06T06:18:44.236268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Model creation functions with multi-GPU support (PRECISION OPTIMIZED)\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, BatchNormalization, LSTM, Bidirectional, GRU, SimpleRNN, SpatialDropout1D, MaxPooling1D\n",
    "\n",
    "# def create_dnn_model():\n",
    "#     \"\"\"\n",
    "#     Optimized CNN model with proven architecture.\n",
    "#     \"\"\"\n",
    "#     with global_strategy.scope():\n",
    "#         model = Sequential([\n",
    "#             Embedding(vocab_size, embedding_dim, \n",
    "#                      weights=[embedding_matrix], \n",
    "#                      input_length=max_len, \n",
    "#                      trainable=True),  # Keep trainable\n",
    "            \n",
    "#             SpatialDropout1D(0.2),\n",
    "            \n",
    "#             # Optimal convolutional setup\n",
    "#             Conv1D(128, 5, activation='relu', padding='same'),\n",
    "#             MaxPooling1D(2),\n",
    "#             Conv1D(64, 3, activation='relu', padding='same'),\n",
    "#             GlobalMaxPooling1D(),\n",
    "            \n",
    "#             # Optimal dense layers\n",
    "#             Dense(128, activation='relu'),\n",
    "#             BatchNormalization(),\n",
    "#             Dropout(0.3),\n",
    "            \n",
    "#             Dense(64, activation='relu'),\n",
    "#             Dropout(0.2),\n",
    "            \n",
    "#             Dense(num_classes, activation='softmax')\n",
    "#         ])\n",
    "        \n",
    "#         # Optimal learning rate\n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "#                       loss='sparse_categorical_crossentropy', \n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_rnn_model(bidirectional=True):\n",
    "#     \"\"\"\n",
    "#     Optimized RNN model with balanced parameters.\n",
    "#     \"\"\"\n",
    "#     with global_strategy.scope():\n",
    "#         model = Sequential([\n",
    "#             Embedding(vocab_size, embedding_dim, \n",
    "#                      weights=[embedding_matrix], \n",
    "#                      input_length=max_len, \n",
    "#                      trainable=True),\n",
    "            \n",
    "#             SpatialDropout1D(0.3),\n",
    "#         ])\n",
    "        \n",
    "#         if bidirectional:\n",
    "#             model.add(Bidirectional(SimpleRNN(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "#             model.add(Bidirectional(SimpleRNN(32, dropout=0.2, recurrent_dropout=0.2)))\n",
    "#         else:\n",
    "#             model.add(SimpleRNN(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "#             model.add(SimpleRNN(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "        \n",
    "#         model.add(Dense(64, activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(Dropout(0.3))\n",
    "#         model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "#                       loss='sparse_categorical_crossentropy', \n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_gru_model(bidirectional=True):\n",
    "#     \"\"\"\n",
    "#     Optimized GRU model with conservative settings.\n",
    "#     \"\"\"\n",
    "#     with global_strategy.scope():\n",
    "#         model = Sequential([\n",
    "#             Embedding(vocab_size, embedding_dim, \n",
    "#                      weights=[embedding_matrix], \n",
    "#                      input_length=max_len, \n",
    "#                      trainable=True),\n",
    "            \n",
    "#             SpatialDropout1D(0.3),\n",
    "#         ])\n",
    "        \n",
    "#         if bidirectional:\n",
    "#             model.add(Bidirectional(GRU(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "#             model.add(Bidirectional(GRU(64, dropout=0.3, recurrent_dropout=0.3)))\n",
    "#         else:\n",
    "#             model.add(GRU(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
    "#             model.add(GRU(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "        \n",
    "#         model.add(Dense(64, activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(Dropout(0.3))\n",
    "#         model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0008), \n",
    "#                       loss='sparse_categorical_crossentropy', \n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def create_lstm_model(bidirectional=True):\n",
    "#     \"\"\"\n",
    "#     Optimized LSTM model - back to basics with proven settings.\n",
    "#     \"\"\"\n",
    "#     with global_strategy.scope():\n",
    "#         model = Sequential([\n",
    "#             Embedding(vocab_size, embedding_dim, \n",
    "#                      weights=[embedding_matrix], \n",
    "#                      input_length=max_len, \n",
    "#                      trainable=True),\n",
    "            \n",
    "#             SpatialDropout1D(0.3),\n",
    "#         ])\n",
    "        \n",
    "#         if bidirectional:\n",
    "#             model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "#             model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)))\n",
    "#         else:\n",
    "#             model.add(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
    "#             model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "        \n",
    "#         model.add(Dense(64, activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(Dropout(0.3))\n",
    "#         model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0007), \n",
    "#                       loss='sparse_categorical_crossentropy', \n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Test model creation\n",
    "# print(\"Testing optimized model creation with multi-GPU...\")\n",
    "# test_model = create_lstm_model(bidirectional=True)\n",
    "# print(\"Optimized LSTM model summary:\")\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:18:44.244192Z",
     "iopub.status.busy": "2025-09-06T06:18:44.243497Z",
     "iopub.status.idle": "2025-09-06T06:18:44.259566Z",
     "shell.execute_reply": "2025-09-06T06:18:44.258805Z",
     "shell.execute_reply.started": "2025-09-06T06:18:44.244173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Add these callbacks to EVERY model training\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "# ]\n",
    "\n",
    "# # Train with these settings\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=32,  # Try 32, 64\n",
    "#     epochs=50,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     callbacks=callbacks,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Function (Adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:48:13.371070Z",
     "iopub.status.busy": "2025-09-06T08:48:13.370327Z",
     "iopub.status.idle": "2025-09-06T08:48:13.382735Z",
     "shell.execute_reply": "2025-09-06T08:48:13.381899Z",
     "shell.execute_reply.started": "2025-09-06T08:48:13.371045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training and evaluation function with multi-GPU support\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    # Calculate class weights for imbalanced data\n",
    "    class_weights = compute_class_weight('balanced', \n",
    "                                       classes=np.unique(y_train), \n",
    "                                       y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True, min_delta=0.001\n",
    "    )\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Model checkpointing\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'best_{model_name}.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjust batch size based on number of GPUs\n",
    "    num_gpus = global_strategy.num_replicas_in_sync\n",
    "    # base_batch_size = 32 if any(layer in str(type(model)) for layer in ['RNN', 'LSTM', 'GRU']) else 64\n",
    "    # base_batch_size = 128 if any(layer in str(type(model)) for layer in ['RNN', 'LSTM', 'GRU']) else 256\n",
    "    base_batch_size = 512 if any(layer in str(type(model)) for layer in ['RNN', 'LSTM', 'GRU']) else 1024\n",
    "    # base_batch_size = 512 if any(layer in str(type(model)) for layer in ['RNN', 'LSTM', 'GRU']) else 512\n",
    "    batch_size = base_batch_size * num_gpus\n",
    "    \n",
    "    print(f\"Using batch size: {batch_size} across {num_gpus} GPUs\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"{model_name} training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = model.predict(X_test, batch_size=128, verbose=1)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "    f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    cr = classification_report(y_test, y_pred_classes, target_names=[reverse_label_map[i] for i in range(num_classes)])\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot confusion matrix (normalized)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "                xticklabels=[reverse_label_map[i] for i in range(num_classes)],\n",
    "                yticklabels=[reverse_label_map[i] for i in range(num_classes)])\n",
    "    plt.title(f'{model_name} Normalized Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Classification Report for {model_name}:\\n{cr}\")\n",
    "    print(f\"Macro F1: {f1_macro:.4f}, Weighted F1: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'f1_macro': f1_macro,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': cr,\n",
    "        'history': history.history,\n",
    "        'training_time': training_time,\n",
    "        'epochs_trained': len(history.history['accuracy'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Run All Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU trainig init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:56:36.102245Z",
     "iopub.status.busy": "2025-09-06T08:56:36.101938Z",
     "iopub.status.idle": "2025-09-06T08:56:36.107958Z",
     "shell.execute_reply": "2025-09-06T08:56:36.107038Z",
     "shell.execute_reply.started": "2025-09-06T08:56:36.102225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run all Neural Network models with Skip-gram embeddings\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "print(\"Training all Neural Network models with Skip-gram embeddings...\")\n",
    "results = {}\n",
    "\n",
    "# # List of all models to train\n",
    "# models_to_train = [\n",
    "#     ('DNN', create_dnn_model),\n",
    "#     # ('SimpleRNN', create_rnn_model),\n",
    "#     # ('GRU', create_gru_model),\n",
    "#     # ('LSTM', create_lstm_model),\n",
    "#     # ('Bidirectional_SimpleRNN', lambda: create_rnn_model(bidirectional=True)),\n",
    "#     # ('Bidirectional_GRU', lambda: create_gru_model(bidirectional=True)),\n",
    "#     # ('Bidirectional_LSTM', lambda: create_lstm_model(bidirectional=True))\n",
    "# ]\n",
    "\n",
    "# # Train each model sequentially\n",
    "# for model_name, model_func in models_to_train:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Training {model_name}...\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Create and train model\n",
    "#         model = model_func()\n",
    "#         result = train_and_evaluate_model(\n",
    "#             model, X_train, y_train, X_test, y_test, f\"{model_name}_Skipgram\"\n",
    "#         )\n",
    "#         results[model_name] = result\n",
    "        \n",
    "#         # Clear memory after each model\n",
    "#         tf.keras.backend.clear_session()\n",
    "#         import gc\n",
    "#         gc.collect()\n",
    "        \n",
    "#         print(f\"✅ {model_name} completed successfully!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error training {model_name}: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"All models training completed!\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# # Show final GPU status\n",
    "# print(\"\\nFinal GPU Status:\")\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Model Training (If you want to run them separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:54:20.006876Z",
     "iopub.status.busy": "2025-09-06T08:54:20.006366Z",
     "iopub.status.idle": "2025-09-06T08:54:20.010306Z",
     "shell.execute_reply": "2025-09-06T08:54:20.009579Z",
     "shell.execute_reply.started": "2025-09-06T08:54:20.006832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:25:35.192305Z",
     "iopub.status.busy": "2025-09-06T06:25:35.191756Z",
     "iopub.status.idle": "2025-09-06T06:27:32.474632Z",
     "shell.execute_reply": "2025-09-06T06:27:32.473861Z",
     "shell.execute_reply.started": "2025-09-06T06:25:35.192266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Individual model training cells (run these one by one if needed)\n",
    "\n",
    "# 1. DNN Model\n",
    "print(\"Training DNN Model...\")\n",
    "dnn_model = create_dnn_model()\n",
    "dnn_result = train_and_evaluate_model(\n",
    "    dnn_model, X_train, y_train, X_test, y_test, \"DNN_Skipgram\"\n",
    ")\n",
    "results['DNN'] = dnn_result\n",
    "tf.keras.backend.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:27:32.478889Z",
     "iopub.status.busy": "2025-09-06T06:27:32.478644Z",
     "iopub.status.idle": "2025-09-06T06:36:53.875358Z",
     "shell.execute_reply": "2025-09-06T06:36:53.874548Z",
     "shell.execute_reply.started": "2025-09-06T06:27:32.478869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. SimpleRNN Model\n",
    "print(\"Training SimpleRNN Model...\")\n",
    "rnn_model = create_rnn_model()\n",
    "rnn_result = train_and_evaluate_model(\n",
    "    rnn_model, X_train, y_train, X_test, y_test, \"SimpleRNN_Skipgram\"\n",
    ")\n",
    "results['SimpleRNN'] = rnn_result\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:36:53.876904Z",
     "iopub.status.busy": "2025-09-06T06:36:53.876656Z",
     "iopub.status.idle": "2025-09-06T06:57:14.537529Z",
     "shell.execute_reply": "2025-09-06T06:57:14.536751Z",
     "shell.execute_reply.started": "2025-09-06T06:36:53.876885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. GRU Model\n",
    "print(\"Training GRU Model...\")\n",
    "gru_model = create_gru_model()\n",
    "gru_result = train_and_evaluate_model(\n",
    "    gru_model, X_train, y_train, X_test, y_test, \"GRU_Skipgram\"\n",
    ")\n",
    "results['GRU'] = gru_result\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:57:14.540266Z",
     "iopub.status.busy": "2025-09-06T06:57:14.540005Z",
     "iopub.status.idle": "2025-09-06T07:20:02.303416Z",
     "shell.execute_reply": "2025-09-06T07:20:02.302521Z",
     "shell.execute_reply.started": "2025-09-06T06:57:14.540248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. LSTM Model\n",
    "print(\"Training LSTM Model...\")\n",
    "lstm_model = create_lstm_model()\n",
    "lstm_result = train_and_evaluate_model(\n",
    "    lstm_model, X_train, y_train, X_test, y_test, \"LSTM_Skipgram\"\n",
    ")\n",
    "results['LSTM'] = lstm_result\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T07:20:02.305150Z",
     "iopub.status.busy": "2025-09-06T07:20:02.304858Z",
     "iopub.status.idle": "2025-09-06T07:29:22.542265Z",
     "shell.execute_reply": "2025-09-06T07:29:22.541466Z",
     "shell.execute_reply.started": "2025-09-06T07:20:02.305130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5. Bidirectional SimpleRNN\n",
    "print(\"Training Bidirectional SimpleRNN Model...\")\n",
    "birnn_model = create_rnn_model(bidirectional=True)\n",
    "birnn_result = train_and_evaluate_model(\n",
    "    birnn_model, X_train, y_train, X_test, y_test, \"Bidirectional_SimpleRNN_Skipgram\"\n",
    ")\n",
    "results['Bidirectional_SimpleRNN'] = birnn_result\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T07:29:41.489570Z",
     "iopub.status.busy": "2025-09-06T07:29:41.489279Z",
     "iopub.status.idle": "2025-09-06T07:50:12.982454Z",
     "shell.execute_reply": "2025-09-06T07:50:12.981661Z",
     "shell.execute_reply.started": "2025-09-06T07:29:41.489549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. Bidirectional GRU\n",
    "print(\"Training Bidirectional GRU Model...\")\n",
    "bigru_model = create_gru_model(bidirectional=True)\n",
    "bigru_result = train_and_evaluate_model(\n",
    "    bigru_model, X_train, y_train, X_test, y_test, \"Bidirectional_GRU_Skipgram\"\n",
    ")\n",
    "results['Bidirectional_GRU'] = bigru_result\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:56:57.034087Z",
     "iopub.status.busy": "2025-09-06T08:56:57.033783Z",
     "iopub.status.idle": "2025-09-06T09:18:55.209535Z",
     "shell.execute_reply": "2025-09-06T09:18:55.208693Z",
     "shell.execute_reply.started": "2025-09-06T08:56:57.034060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 7. Bidirectional LSTM\n",
    "print(\"Training Bidirectional LSTM Model...\")\n",
    "bilstm_model = create_lstm_model(bidirectional=True)\n",
    "bilstm_result = train_and_evaluate_model(\n",
    "    bilstm_model, X_train, y_train, X_test, y_test, \"Bidirectional_LSTM_Skipgram\"\n",
    ")\n",
    "results['Bidirectional_LSTM'] = bilstm_result\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T08:11:16.164245Z",
     "iopub.status.busy": "2025-09-06T08:11:16.163546Z",
     "iopub.status.idle": "2025-09-06T08:11:19.669852Z",
     "shell.execute_reply": "2025-09-06T08:11:19.669051Z",
     "shell.execute_reply.started": "2025-09-06T08:11:16.164219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T11:28:39.935955Z",
     "iopub.status.busy": "2025-09-06T11:28:39.935352Z",
     "iopub.status.idle": "2025-09-06T11:28:39.945017Z",
     "shell.execute_reply": "2025-09-06T11:28:39.944387Z",
     "shell.execute_reply.started": "2025-09-06T11:28:39.935931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"DNN\": {\n",
    "        \"accuracy\": 0.6989,\n",
    "        \"f1_score\": 0.689,\n",
    "        \"f1_macro\": 0.689,  # Added Macro F1\n",
    "        \"training_time\": 108.7344,\n",
    "        \"epochs_trained\": 5,\n",
    "        \"parameters\": 61103222,\n",
    "        \"history\": {\n",
    "            \"accuracy\": [0.7290],\n",
    "            \"loss\": [0.8604],\n",
    "            \"val_accuracy\": [0.6926],\n",
    "            \"val_loss\": [0.9724],\n",
    "        },\n",
    "    },\n",
    "    \"SimpleRNN\": {\n",
    "        \"accuracy\": 0.675,\n",
    "        \"f1_score\": 0.667,\n",
    "        \"f1_macro\": 0.667,  # Added Macro F1\n",
    "        \"training_time\": 471.7932,\n",
    "        \"epochs_trained\": 5,\n",
    "        \"parameters\": 60759926,\n",
    "        \"history\": {\n",
    "            \"accuracy\": [0.6532],\n",
    "            \"loss\": [1.1061],\n",
    "            \"val_accuracy\": [0.6606],\n",
    "            \"val_loss\": [1.1244],\n",
    "        },\n",
    "    },\n",
    "    \"GRU\": {\n",
    "        \"accuracy\": 0.711,\n",
    "        \"f1_score\": 0.705,\n",
    "        \"f1_macro\": 0.705,  # Added Macro F1\n",
    "        \"training_time\": 1048.23,\n",
    "        \"epochs_trained\": 5,\n",
    "        \"parameters\": 61719542,\n",
    "        \"history\": {\n",
    "            \"accuracy\": [0.7478],\n",
    "            \"loss\": [0.8072],\n",
    "            \"val_accuracy\": [0.7004],\n",
    "            \"val_loss\": [0.9628],\n",
    "        },\n",
    "    },\n",
    "    \"LSTM\": {\n",
    "        \"accuracy\": 0.716,\n",
    "        \"f1_score\": 0.711,\n",
    "        \"f1_macro\": 0.711,  # Added Macro F1\n",
    "        \"training_time\": 1187.5069,\n",
    "        \"epochs_trained\": 5,\n",
    "        \"parameters\": 62064118,\n",
    "        \"history\": {\n",
    "            \"accuracy\": [0.7329],\n",
    "            \"loss\": [0.8679],\n",
    "            \"val_accuracy\": [0.7090],\n",
    "            \"val_loss\": [0.9224],\n",
    "        },\n",
    "    },\n",
    "    \"Bidirectional_SimpleRNN\": {\n",
    "        \"accuracy\": 0.660,\n",
    "        \"f1_score\": 0.657,\n",
    "        \"f1_macro\": 0.657,  # Added Macro F1\n",
    "        \"training_time\": 469.4036,\n",
    "        \"epochs_trained\": 5,\n",
    "        \"parameters\": 60759926,\n",
    "        \"history\": {\n",
    "            \"accuracy\": [0.6711],\n",
    "            \"loss\": [1.0578],\n",
    "            \"val_accuracy\": [0.6536],\n",
    "            \"val_loss\": [1.1420],\n",
    "        },\n",
    "    },\n",
    "    \"Bidirectional_GRU\": {\n",
    "        \"accuracy\": 0.719,\n",
    "        \"f1_score\": 0.713,\n",
    "        \"f1_macro\": 0.713,  # Added Macro F1\n",
    "        \"training_time\": 1055.4283,\n",
    "        \"epochs_trained\": 5,\n",
    "        \"parameters\": 61719542,\n",
    "        \"history\": {\n",
    "            \"accuracy\": [0.7420],\n",
    "            \"loss\": [0.8144],\n",
    "            \"val_accuracy\": [0.6974],\n",
    "            \"val_loss\": [0.9788],\n",
    "        },\n",
    "    },\n",
    "    \"Bidirectional_LSTM\": {\n",
    "        \"accuracy\": 0.716,\n",
    "        \"f1_score\": 0.711,\n",
    "        \"f1_macro\": 0.711,  # Added Macro F1\n",
    "        \"training_time\": 1146.4614,\n",
    "        \"epochs_trained\": 5,\n",
    "        \"parameters\": 62064118,\n",
    "        \"history\": {\n",
    "            \"accuracy\": [\n",
    "                0.1149, 0.6300, 0.6848, 0.7046, 0.7245\n",
    "            ],\n",
    "            \"loss\": [\n",
    "                2.7309, 1.1602, 1.0188, 0.9470, 0.8873\n",
    "            ],\n",
    "            \"val_accuracy\": [\n",
    "                0.6384, 0.6745, 0.6950, 0.7017, 0.7068\n",
    "            ],\n",
    "            \"val_loss\": [\n",
    "                1.5884, 1.1917, 0.9716, 0.9341, 0.9208\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION: 1 - INTERACTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:57:09.129367Z",
     "iopub.status.busy": "2025-09-06T10:57:09.129088Z",
     "iopub.status.idle": "2025-09-06T10:57:09.307468Z",
     "shell.execute_reply": "2025-09-06T10:57:09.306672Z",
     "shell.execute_reply.started": "2025-09-06T10:57:09.129347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create comprehensive results comparison with enhanced visualizations\n",
    "print(\"Creating comprehensive results comparison...\")\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    history = result['history']\n",
    "    params = result.get('parameters', 0)  # ✅ use saved params instead of model.count_params()\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'F1_Score_Weighted': result.get('f1_score', 0),\n",
    "        'F1_Score_Macro': result.get('f1_macro', result.get('f1_score', 0)),  # ✅ fallback if missing\n",
    "        'Training_Time_Seconds': result['training_time'],\n",
    "        'Training_Time_Minutes': result['training_time'] / 60,\n",
    "        'Epochs_Trained': result.get('epochs_trained', len(history['accuracy'])),\n",
    "        'Final_Train_Accuracy': history['accuracy'][-1],\n",
    "        'Final_Val_Accuracy': history['val_accuracy'][-1] if 'val_accuracy' in history else 0,\n",
    "        'Final_Train_Loss': history['loss'][-1],\n",
    "        'Final_Val_Loss': history['val_loss'][-1] if 'val_loss' in history else 0,\n",
    "        'Best_Val_Accuracy': max(history['val_accuracy']) if 'val_accuracy' in history else 0,\n",
    "        'Parameters': params,\n",
    "        'Parameters_Millions': params / 1e6,\n",
    "        'Efficiency_Score': result['accuracy'] / (result['training_time'] / 60),  # Accuracy per minute\n",
    "        'Overfitting_Gap': history['accuracy'][-1] - (history['val_accuracy'][-1] if 'val_accuracy' in history else 0)\n",
    "    })\n",
    "\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Sort by accuracy for better visualization\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# Create styled table for better readability\n",
    "styled_df = comparison_df[['Model', 'Accuracy', 'F1_Score_Weighted', 'F1_Score_Macro', \n",
    "                          'Training_Time_Minutes', 'Parameters_Millions', 'Efficiency_Score']].copy()\n",
    "styled_df.columns = ['Model', 'Accuracy', 'F1 (Weighted)', 'F1 (Macro)', \n",
    "                    'Time (Min)', 'Params (M)', 'Efficiency']\n",
    "\n",
    "# Apply conditional formatting\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: lightcoral' if v else '' for v in is_min]\n",
    "\n",
    "styled_display = styled_df.style\\\n",
    "    .format({\n",
    "        'Accuracy': '{:.3f}',\n",
    "        'F1 (Weighted)': '{:.3f}', \n",
    "        'F1 (Macro)': '{:.3f}',\n",
    "        'Time (Min)': '{:.1f}',\n",
    "        'Params (M)': '{:.2f}',\n",
    "        'Efficiency': '{:.4f}'\n",
    "    })\\\n",
    "    .apply(highlight_max, subset=['Accuracy', 'F1 (Weighted)', 'F1 (Macro)', 'Efficiency'])\\\n",
    "    .apply(highlight_min, subset=['Time (Min)', 'Params (M)'])\\\n",
    "    .set_properties(**{'text-align': 'center'})\\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'th',\n",
    "        'props': [('background-color', '#40466e'), ('color', 'white'), ('font-weight', 'bold')]\n",
    "    }])\n",
    "\n",
    "print(\"\\n📊 STYLED COMPARISON TABLE:\")\n",
    "display(styled_display)\n",
    "\n",
    "# Create interactive visualizations with Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set Plotly template\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Create a comprehensive dashboard with subplots\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=3,\n",
    "    subplot_titles=(\n",
    "        'Model Accuracy Comparison', 'F1 Score Comparison', \n",
    "        'Training vs Validation Accuracy', 'Training Time Comparison',\n",
    "        'Model Size (Parameters)', 'Overfitting Analysis',\n",
    "        'Training Efficiency', 'Accuracy vs Training Time', \n",
    "        'Accuracy vs Efficiency'\n",
    "    ),\n",
    "    specs=[\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}]\n",
    "    ],\n",
    "    vertical_spacing=0.08,\n",
    "    horizontal_spacing=0.08\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text='COMPREHENSIVE MODEL COMPARISON (Skip-gram Embeddings)',\n",
    "    title_font_size=20,\n",
    "    title_font_color='darkblue',\n",
    "    title_x=0.5,\n",
    "    height=1200,\n",
    "    width=1400,\n",
    "    showlegend=True,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Accuracy'],\n",
    "        marker_color=px.colors.qualitative.Set3,\n",
    "        text=comparison_df['Accuracy'].round(3),\n",
    "        textposition='auto',\n",
    "        name='Accuracy',\n",
    "        hovertemplate='<b>%{x}</b><br>Accuracy: %{y:.3f}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. F1 Scores comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['F1_Score_Weighted'],\n",
    "        name='Weighted F1',\n",
    "        marker_color='lightblue',\n",
    "        hovertemplate='<b>%{x}</b><br>Weighted F1: %{y:.3f}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['F1_Score_Macro'],\n",
    "        name='Macro F1',\n",
    "        marker_color='lightcoral',\n",
    "        hovertemplate='<b>%{x}</b><br>Macro F1: %{y:.3f}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Training vs Validation Accuracy\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=comparison_df['Final_Train_Accuracy'],\n",
    "        y=comparison_df['Final_Val_Accuracy'],\n",
    "        mode='markers+text',\n",
    "        text=comparison_df['Model'],\n",
    "        textposition='top center',\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color=comparison_df['Accuracy'],\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Accuracy\")\n",
    "        ),\n",
    "        name='Train vs Val',\n",
    "        hovertemplate='<b>%{text}</b><br>Train Accuracy: %{x:.3f}<br>Val Accuracy: %{y:.3f}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# Add reference line\n",
    "fig.add_shape(\n",
    "    type=\"line\", line=dict(dash='dash', color='grey'),\n",
    "    x0=0, y0=0, x1=1, y1=1,\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# 4. Training time comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Training_Time_Minutes'],\n",
    "        marker_color=px.colors.qualitative.Pastel,\n",
    "        text=comparison_df['Training_Time_Minutes'].round(1),\n",
    "        textposition='auto',\n",
    "        name='Training Time (min)',\n",
    "        hovertemplate='<b>%{x}</b><br>Training Time: %{y:.1f} minutes<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 5. Parameters comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Parameters_Millions'],\n",
    "        marker_color=px.colors.qualitative.Set2,\n",
    "        text=comparison_df['Parameters_Millions'].round(2),\n",
    "        textposition='auto',\n",
    "        name='Parameters (M)',\n",
    "        hovertemplate='<b>%{x}</b><br>Parameters: %{y:.2f}M<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# 6. Overfitting analysis\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Overfitting_Gap'],\n",
    "        marker_color=['green' if x <= 0 else 'red' for x in comparison_df['Overfitting_Gap']],\n",
    "        text=comparison_df['Overfitting_Gap'].round(3),\n",
    "        textposition='auto',\n",
    "        name='Overfitting Gap',\n",
    "        hovertemplate='<b>%{x}</b><br>Overfitting Gap: %{y:.3f}<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=3\n",
    ")\n",
    "\n",
    "# 7. Efficiency score\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Efficiency_Score'],\n",
    "        marker_color=px.colors.sequential.Viridis,\n",
    "        text=comparison_df['Efficiency_Score'].round(4),\n",
    "        textposition='auto',\n",
    "        name='Efficiency',\n",
    "        hovertemplate='<b>%{x}</b><br>Efficiency: %{y:.4f}<extra></extra>'\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# 8. Accuracy vs Training Time scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=comparison_df['Training_Time_Minutes'],\n",
    "        y=comparison_df['Accuracy'],\n",
    "        mode='markers',\n",
    "        text=comparison_df['Model'],\n",
    "        marker=dict(\n",
    "            size=comparison_df['Parameters_Millions']*10,\n",
    "            color=comparison_df['Efficiency_Score'],\n",
    "            colorscale='Plasma',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Efficiency\")\n",
    "        ),\n",
    "        name='Accuracy vs Time',\n",
    "        hovertemplate='<b>%{text}</b><br>Accuracy: %{y:.3f}<br>Training Time: %{x:.1f} min<extra></extra>'\n",
    "    ),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "# 9. Accuracy vs Efficiency scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=comparison_df['Efficiency_Score'],\n",
    "        y=comparison_df['Accuracy'],\n",
    "        mode='markers',\n",
    "        text=comparison_df['Model'],\n",
    "        marker=dict(\n",
    "            size=comparison_df['Parameters_Millions']*10,\n",
    "            color=comparison_df['Training_Time_Minutes'],\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Training Time (min)\")\n",
    "        ),\n",
    "        name='Accuracy vs Efficiency',\n",
    "        hovertemplate='<b>%{text}</b><br>Accuracy: %{y:.3f}<br>Efficiency: %{x:.4f}<extra></extra>'\n",
    "    ),\n",
    "    row=3, col=3\n",
    ")\n",
    "\n",
    "# Update axes properties\n",
    "fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=2)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=3)\n",
    "fig.update_xaxes(tickangle=45, row=3, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"F1 Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Validation Accuracy\", row=1, col=3)\n",
    "fig.update_xaxes(title_text=\"Training Accuracy\", row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"Time (Minutes)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Parameters (Millions)\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Overfitting Gap\", row=2, col=3)\n",
    "fig.update_yaxes(title_text=\"Efficiency Score\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=3, col=2)\n",
    "fig.update_xaxes(title_text=\"Training Time (Minutes)\", row=3, col=2)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=3, col=3)\n",
    "fig.update_xaxes(title_text=\"Efficiency Score\", row=3, col=3)\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# Create a radar chart for comprehensive comparison\n",
    "categories = ['Accuracy', 'F1_Score_Weighted', 'Efficiency_Score', 'Training_Time_Minutes', 'Parameters_Millions']\n",
    "categories_norm = [f'{cat}_norm' for cat in categories]\n",
    "\n",
    "# Normalize values for radar chart (0-1)\n",
    "radar_df = comparison_df.copy()\n",
    "for cat in categories:\n",
    "    radar_df[f'{cat}_norm'] = (radar_df[cat] - radar_df[cat].min()) / (radar_df[cat].max() - radar_df[cat].min())\n",
    "\n",
    "# Create radar chart\n",
    "fig_radar = go.Figure()\n",
    "\n",
    "for i, row in radar_df.iterrows():\n",
    "    values = row[categories_norm].tolist()\n",
    "    values += values[:1]  # Close the circle\n",
    "    \n",
    "    fig_radar.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=[cat.replace('_', ' ').title() for cat in categories] + \n",
    "              [categories[0].replace('_', ' ').title()],\n",
    "        fill='toself',\n",
    "        name=row['Model'],\n",
    "        hovertemplate='<b>%{theta}</b>: %{r:.2f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "fig_radar.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 1]\n",
    "        )),\n",
    "    showlegend=True,\n",
    "    title='Comprehensive Model Comparison (Radar Chart)',\n",
    "    title_x=0.5,\n",
    "    height=600,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig_radar.show()\n",
    "\n",
    "# Create training history comparison plot\n",
    "fig_history = go.Figure()\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    history = result['history']\n",
    "    fig_history.add_trace(go.Scatter(\n",
    "        x=list(range(1, len(history['accuracy']) + 1)),\n",
    "        y=history['accuracy'],\n",
    "        mode='lines',\n",
    "        name=f'{model_name} (Train)',\n",
    "        hovertemplate='Epoch: %{x}<br>Accuracy: %{y:.3f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    if 'val_accuracy' in history:\n",
    "        fig_history.add_trace(go.Scatter(\n",
    "            x=list(range(1, len(history['val_accuracy']) + 1)),\n",
    "            y=history['val_accuracy'],\n",
    "            mode='lines',\n",
    "            name=f'{model_name} (Validation)',\n",
    "            line=dict(dash='dash'),\n",
    "            hovertemplate='Epoch: %{x}<br>Accuracy: %{y:.3f}<extra></extra>'\n",
    "        ))\n",
    "\n",
    "fig_history.update_layout(\n",
    "    title='Training History Comparison',\n",
    "    xaxis_title='Epochs',\n",
    "    yaxis_title='Accuracy',\n",
    "    hovermode='closest',\n",
    "    height=600,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig_history.show()\n",
    "\n",
    "# Detailed analysis and recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "worst_model = comparison_df.loc[comparison_df['Accuracy'].idxmin()]\n",
    "most_efficient = comparison_df.loc[comparison_df['Efficiency_Score'].idxmax()]\n",
    "lightest_model = comparison_df.loc[comparison_df['Parameters_Millions'].idxmin()]\n",
    "\n",
    "print(f\"\\n🏆 BEST OVERALL MODEL: {best_model['Model']}\")\n",
    "print(f\"   📈 Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"   🎯 F1-Score (Weighted): {best_model['F1_Score_Weighted']:.4f}\")\n",
    "print(f\"   ⏱️  Training Time: {best_model['Training_Time_Minutes']:.1f} minutes\")\n",
    "print(f\"   🧮 Parameters: {best_model['Parameters_Millions']:.2f}M\")\n",
    "\n",
    "print(f\"\\n⚡ MOST EFFICIENT MODEL: {most_efficient['Model']}\")\n",
    "print(f\"   🚀 Efficiency Score: {most_efficient['Efficiency_Score']:.4f} (Accuracy per minute)\")\n",
    "print(f\"   📈 Accuracy: {most_efficient['Accuracy']:.4f}\")\n",
    "print(f\"   ⏱️  Training Time: {most_efficient['Training_Time_Minutes']:.1f} minutes\")\n",
    "\n",
    "print(f\"\\n📉 WORST PERFORMING MODEL: {worst_model['Model']}\")\n",
    "print(f\"   📈 Accuracy: {worst_model['Accuracy']:.4f}\")\n",
    "print(f\"   ⚠️  Potential issues: Overfitting gap: {worst_model['Overfitting_Gap']:.4f}\")\n",
    "\n",
    "print(f\"\\n🏋️  LIGHTEST MODEL: {lightest_model['Model']}\")\n",
    "print(f\"   🧮 Parameters: {lightest_model['Parameters_Millions']:.2f}M\")\n",
    "print(f\"   📈 Accuracy: {lightest_model['Accuracy']:.4f}\")\n",
    "\n",
    "# Additional insights\n",
    "print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "print(f\"   • Accuracy range: {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}\")\n",
    "print(f\"   • Training time range: {comparison_df['Training_Time_Minutes'].min():.1f} - {comparison_df['Training_Time_Minutes'].max():.1f} minutes\")\n",
    "print(f\"   • Average overfitting gap: {comparison_df['Overfitting_Gap'].mean():.4f}\")\n",
    "\n",
    "# Save all results to a single HTML file\n",
    "with open('model_comparison_report.html', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Model Comparison Report</title>\n",
    "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 40px; }\n",
    "            h1 { color: #2c3e50; text-align: center; }\n",
    "            h2 { color: #3498db; border-bottom: 2px solid #3498db; padding-bottom: 10px; }\n",
    "            .summary { background-color: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 20px; }\n",
    "            .insight { background-color: #e8f4f8; padding: 10px; border-left: 4px solid #3498db; margin: 10px 0; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Comprehensive Model Comparison Report</h1>\n",
    "    \"\"\")\n",
    "    \n",
    "    # Add summary\n",
    "    f.write(\"<h2>Summary</h2>\")\n",
    "    f.write(\"<div class='summary'>\")\n",
    "    f.write(f\"<p><b>Best Model:</b> {best_model['Model']} (Accuracy: {best_model['Accuracy']:.4f}, Params: {best_model['Parameters_Millions']:.2f}M)</p>\")\n",
    "    f.write(f\"<p><b>Most Efficient:</b> {most_efficient['Model']} (Efficiency: {most_efficient['Efficiency_Score']:.4f})</p>\")\n",
    "    f.write(f\"<p><b>Lightest Model:</b> {lightest_model['Model']} ({lightest_model['Parameters_Millions']:.2f}M params)</p>\")\n",
    "    f.write(\"</div>\")\n",
    "    \n",
    "    # Add key insights\n",
    "    f.write(\"<h2>Key Insights</h2>\")\n",
    "    f.write(\"<div class='insight'>\")\n",
    "    f.write(f\"<p>Accuracy range: {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}</p>\")\n",
    "    f.write(f\"<p>Training time range: {comparison_df['Training_Time_Minutes'].min():.1f} - {comparison_df['Training_Time_Minutes'].max():.1f} minutes</p>\")\n",
    "    f.write(f\"<p>Average overfitting gap: {comparison_df['Overfitting_Gap'].mean():.4f}</p>\")\n",
    "    f.write(\"</div>\")\n",
    "    \n",
    "    f.write(\"</body></html>\")\n",
    "\n",
    "print(\"\\n✅ Report generated and saved as 'model_comparison_report.html'\")\n",
    "print(\"✅ CSV comparison saved as 'skipgram_nn_results_comparison.csv'\")\n",
    "print(\"✅ Visualizations saved as interactive plots (open in notebook or browser)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION: 2 - FINALIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T11:31:06.491067Z",
     "iopub.status.busy": "2025-09-06T11:31:06.490285Z",
     "iopub.status.idle": "2025-09-06T11:31:13.118990Z",
     "shell.execute_reply": "2025-09-06T11:31:13.118354Z",
     "shell.execute_reply.started": "2025-09-06T11:31:06.491041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Build comparison dataframe from results dictionary ---\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    history = result.get('history', {})\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': result.get('accuracy', 0),\n",
    "        'F1_Score_Weighted': result.get('f1_score', 0),\n",
    "        'F1_Score_Macro': result.get('f1_macro', 0),\n",
    "        'Training_Time_Seconds': result.get('training_time', 0),\n",
    "        'Training_Time_Minutes': result.get('training_time', 0) / 60,\n",
    "        'Epochs_Trained': result.get('epochs_trained', len(history.get('accuracy', []))),\n",
    "        'Final_Train_Accuracy': history.get('accuracy', [0])[-1],\n",
    "        'Final_Val_Accuracy': history.get('val_accuracy', [0])[-1],\n",
    "        'Final_Train_Loss': history.get('loss', [0])[-1],\n",
    "        'Final_Val_Loss': history.get('val_loss', [0])[-1],\n",
    "        'Best_Val_Accuracy': max(history.get('val_accuracy', [0])),\n",
    "        'Parameters': result.get('parameters', 0),\n",
    "        'Parameters_Millions': result.get('parameters', 0) / 1e6,\n",
    "        'Efficiency_Score': result.get('accuracy', 0) / (result.get('training_time', 1)/60),  # accuracy per minute\n",
    "        'Overfitting_Gap': history.get('accuracy', [0])[-1] - history.get('val_accuracy', [0])[-1]\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Sort by accuracy\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Fix column display name if needed\n",
    "comparison_df['Model'] = comparison_df['Model'].replace({'Bidirectional_LSTM_Skipgram': 'Bidirectional_LSTM'})\n",
    "\n",
    "# Display rounded table\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "# Create styled table for better readability\n",
    "styled_df = comparison_df[['Model', 'Accuracy', 'F1_Score_Weighted', 'F1_Score_Macro', \n",
    "                          'Training_Time_Minutes', 'Parameters_Millions', 'Efficiency_Score']].copy()\n",
    "styled_df.columns = ['Model', 'Accuracy', 'F1 (Weighted)', 'F1 (Macro)', \n",
    "                    'Time (Min)', 'Params (M)', 'Efficiency']\n",
    "\n",
    "# Apply conditional formatting\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: lightcoral' if v else '' for v in is_min]\n",
    "\n",
    "styled_display = styled_df.style\\\n",
    "    .format({\n",
    "        'Accuracy': '{:.3f}',\n",
    "        'F1 (Weighted)': '{:.3f}', \n",
    "        'F1 (Macro)': '{:.3f}',\n",
    "        'Time (Min)': '{:.1f}',\n",
    "        'Params (M)': '{:.2f}',\n",
    "        'Efficiency': '{:.4f}'\n",
    "    })\\\n",
    "    .apply(highlight_max, subset=['Accuracy', 'F1 (Weighted)', 'F1 (Macro)', 'Efficiency'])\\\n",
    "    .apply(highlight_min, subset=['Time (Min)', 'Params (M)'])\\\n",
    "    .set_properties(**{'text-align': 'center'})\\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'th',\n",
    "        'props': [('background-color', '#40466e'), ('color', 'white'), ('font-weight', 'bold')]\n",
    "    }])\n",
    "\n",
    "print(\"\\n📊 STYLED COMPARISON TABLE:\")\n",
    "display(styled_display)\n",
    "\n",
    "# Save detailed results\n",
    "comparison_df.to_csv('skipgram_nn_detailed_results.csv', index=False)\n",
    "styled_df.to_csv('skipgram_nn_summary_results.csv', index=False)\n",
    "\n",
    "# Enhanced visual comparison with Plotly and Seaborn\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create a comprehensive dashboard with Plotly\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=3,\n",
    "    subplot_titles=(\n",
    "        '📈 Model Accuracy Comparison', '🎯 F1 Score Comparison', \n",
    "        '🔄 Training vs Validation Accuracy', '⏰ Training Time Comparison',\n",
    "        '🧮 Model Size (Parameters)', '⚖️ Overfitting Analysis',\n",
    "        '🚀 Training Efficiency', '⏱️ Accuracy vs Training Time',\n",
    "        '📊 Comprehensive Model Comparison (Radar)'\n",
    "    ),\n",
    "    specs=[\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"scatter\"}, {\"type\": \"polar\"}]\n",
    "    ],\n",
    "    vertical_spacing=0.08,\n",
    "    horizontal_spacing=0.08\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text='COMPREHENSIVE MODEL COMPARISON (Skip-gram Embeddings)',\n",
    "    title_font_size=20,\n",
    "    title_font_color='darkblue',\n",
    "    title_x=0.5,\n",
    "    height=1200,\n",
    "    width=1400,\n",
    "    showlegend=True,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Accuracy'],\n",
    "        marker_color=px.colors.qualitative.Set3,\n",
    "        text=comparison_df['Accuracy'].round(3),\n",
    "        textposition='auto',\n",
    "        name='Accuracy'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. F1 Scores comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['F1_Score_Weighted'],\n",
    "        name='Weighted F1',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['F1_Score_Macro'],\n",
    "        name='Macro F1',\n",
    "        marker_color='lightcoral'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Training vs Validation Accuracy\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=comparison_df['Final_Train_Accuracy'],\n",
    "        y=comparison_df['Final_Val_Accuracy'],\n",
    "        mode='markers+text',\n",
    "        text=comparison_df['Model'],\n",
    "        textposition='top center',\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color=comparison_df['Accuracy'],\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Accuracy\")\n",
    "        ),\n",
    "        name='Train vs Val'\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# Add reference line\n",
    "fig.add_shape(\n",
    "    type=\"line\", line=dict(dash='dash', color='grey'),\n",
    "    x0=0, y0=0, x1=1, y1=1,\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# 4. Training time comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Training_Time_Minutes'],\n",
    "        marker_color=px.colors.qualitative.Pastel,\n",
    "        text=comparison_df['Training_Time_Minutes'].round(1),\n",
    "        textposition='auto',\n",
    "        name='Training Time (min)'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 5. Parameters comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Parameters_Millions'],\n",
    "        marker_color=px.colors.qualitative.Set2,\n",
    "        text=comparison_df['Parameters_Millions'].round(2),\n",
    "        textposition='auto',\n",
    "        name='Parameters (M)'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# 6. Overfitting analysis\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Overfitting_Gap'],\n",
    "        marker_color=['green' if x <= 0 else 'red' for x in comparison_df['Overfitting_Gap']],\n",
    "        text=comparison_df['Overfitting_Gap'].round(3),\n",
    "        textposition='auto',\n",
    "        name='Overfitting Gap'\n",
    "    ),\n",
    "    row=2, col=3\n",
    ")\n",
    "\n",
    "# 7. Efficiency score\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['Efficiency_Score'],\n",
    "        marker_color=px.colors.sequential.Viridis,\n",
    "        text=comparison_df['Efficiency_Score'].round(4),\n",
    "        textposition='auto',\n",
    "        name='Efficiency'\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# 8. Accuracy vs Training Time scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=comparison_df['Training_Time_Minutes'],\n",
    "        y=comparison_df['Accuracy'],\n",
    "        mode='markers+text',\n",
    "        text=comparison_df['Model'],\n",
    "        textposition='top center',\n",
    "        marker=dict(\n",
    "            size=comparison_df['Parameters_Millions']*5,  # Scale by parameter count\n",
    "            color=comparison_df['Efficiency_Score'],\n",
    "            colorscale='Plasma',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Efficiency\")\n",
    "        ),\n",
    "        name='Accuracy vs Time'\n",
    "    ),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "# 9. Radar chart for comprehensive comparison\n",
    "categories = ['Accuracy', 'F1_Score_Weighted', 'Efficiency_Score', 'Parameters_Millions']\n",
    "categories_norm = [f'{cat}_norm' for cat in categories]\n",
    "\n",
    "# Normalize values for radar chart (0-1)\n",
    "radar_df = comparison_df.copy()\n",
    "for cat in categories:\n",
    "    radar_df[f'{cat}_norm'] = (radar_df[cat] - radar_df[cat].min()) / (radar_df[cat].max() - radar_df[cat].min())\n",
    "\n",
    "# Plot radar chart\n",
    "for i, row in radar_df.iterrows():\n",
    "    values = row[categories_norm].tolist()\n",
    "    values += values[:1]  # Close the circle\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=[cat.replace('_', ' ').title() for cat in categories] + \n",
    "                  [categories[0].replace('_', ' ').title()],\n",
    "            fill='toself',\n",
    "            name=row['Model'],\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=3, col=3\n",
    "    )\n",
    "\n",
    "# Update axes properties\n",
    "fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=2)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=3)\n",
    "fig.update_xaxes(tickangle=45, row=3, col=1)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"F1 Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Validation Accuracy\", row=1, col=3)\n",
    "fig.update_xaxes(title_text=\"Training Accuracy\", row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"Time (Minutes)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Parameters (Millions)\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Overfitting Gap\", row=2, col=3)\n",
    "fig.update_yaxes(title_text=\"Efficiency Score\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", row=3, col=2)\n",
    "fig.update_xaxes(title_text=\"Training Time (Minutes)\", row=3, col=2)\n",
    "\n",
    "# Save the interactive plot\n",
    "fig.write_html(\"skipgram_models_interactive_dashboard.html\")\n",
    "\n",
    "# Also create a static version with matplotlib for compatibility\n",
    "plt.figure(figsize=(20, 16))\n",
    "plt.suptitle('COMPREHENSIVE MODEL COMPARISON (Skip-gram Embeddings)', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# Create subplot grid\n",
    "gs = plt.GridSpec(3, 3)\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "ax1 = plt.subplot(gs[0, 0])\n",
    "bars = ax1.bar(comparison_df['Model'], comparison_df['Accuracy'], \n",
    "               color=sns.color_palette(\"husl\", len(comparison_df)), alpha=0.8)\n",
    "ax1.set_title('📈 Model Accuracy Comparison', fontweight='bold', pad=20)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.tick_params(axis='x', rotation=70,)\n",
    "for i, (bar, acc) in enumerate(zip(bars, comparison_df['Accuracy'])):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. F1 Scores comparison\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "width = 0.35\n",
    "x = np.arange(len(comparison_df))\n",
    "ax2.bar(x - width/2, comparison_df['F1_Score_Weighted'], width, \n",
    "        label='Weighted F1', alpha=0.8, color='skyblue')\n",
    "ax2.bar(x + width/2, comparison_df['F1_Score_Macro'], width, \n",
    "        label='Macro F1', alpha=0.8, color='lightcoral')\n",
    "ax2.set_title('🎯 F1 Score Comparison', fontweight='bold', pad=20)\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(comparison_df['Model'], rotation=70,)\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Training vs Validation Accuracy\n",
    "ax3 = plt.subplot(gs[0, 2])\n",
    "scatter = ax3.scatter(comparison_df['Final_Train_Accuracy'], comparison_df['Final_Val_Accuracy'], \n",
    "                     c=comparison_df['Accuracy'], cmap='viridis', s=100, alpha=0.8)\n",
    "for i, txt in enumerate(comparison_df['Model']):\n",
    "    ax3.annotate(txt, (comparison_df['Final_Train_Accuracy'].iloc[i], \n",
    "                      comparison_df['Final_Val_Accuracy'].iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=3)\n",
    "ax3.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax3.set_xlabel('Training Accuracy')\n",
    "ax3.set_ylabel('Validation Accuracy')\n",
    "ax3.set_title('🔄 Training vs Validation Accuracy', fontweight='bold', pad=20)\n",
    "plt.colorbar(scatter, ax=ax3, label='Accuracy')\n",
    "\n",
    "# 4. Training time comparison\n",
    "ax4 = plt.subplot(gs[1, 0])\n",
    "bars = ax4.bar(comparison_df['Model'], comparison_df['Training_Time_Minutes'], \n",
    "               color=sns.color_palette(\"husl\", len(comparison_df)), alpha=0.8)\n",
    "ax4.set_title('⏰ Training Time Comparison', fontweight='bold', pad=20)\n",
    "ax4.set_ylabel('Time (Minutes)')\n",
    "ax4.tick_params(axis='x', rotation=70,)\n",
    "for i, (bar, time_val) in enumerate(zip(bars, comparison_df['Training_Time_Minutes'])):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{time_val:.1f}m', ha='center')\n",
    "\n",
    "# 5. Parameters comparison\n",
    "ax5 = plt.subplot(gs[1, 1])\n",
    "bars = ax5.bar(comparison_df['Model'], comparison_df['Parameters_Millions'], \n",
    "               color=sns.color_palette(\"husl\", len(comparison_df)), alpha=0.8)\n",
    "ax5.set_title('🧮 Model Size (Parameters)', fontweight='bold', pad=20)\n",
    "ax5.set_ylabel('Parameters (Millions)')\n",
    "ax5.tick_params(axis='x', rotation=70,)\n",
    "for i, (bar, params) in enumerate(zip(bars, comparison_df['Parameters_Millions'])):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{params:.2f}M', ha='center')\n",
    "\n",
    "# 6. Overfitting analysis\n",
    "ax6 = plt.subplot(gs[1, 2])\n",
    "colors = ['green' if x <= 0 else 'red' for x in comparison_df['Overfitting_Gap']]\n",
    "bars = ax6.bar(comparison_df['Model'], comparison_df['Overfitting_Gap'], \n",
    "               color=colors, alpha=0.8)\n",
    "ax6.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "ax6.set_title('⚖️ Overfitting Analysis (Train - Val Accuracy Gap)', fontweight='bold', pad=20)\n",
    "ax6.set_ylabel('Accuracy Gap')\n",
    "ax6.tick_params(axis='x', rotation=70,)\n",
    "for i, (bar, gap) in enumerate(zip(bars, comparison_df['Overfitting_Gap'])):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{gap:.3f}', ha='center')\n",
    "\n",
    "# 7. Efficiency score\n",
    "ax7 = plt.subplot(gs[2, 0])\n",
    "bars = ax7.bar(comparison_df['Model'], comparison_df['Efficiency_Score'], \n",
    "               color=sns.color_palette(\"husl\", len(comparison_df)), alpha=0.8)\n",
    "ax7.set_title('🚀 Training Efficiency (Accuracy per Minute)', fontweight='bold', pad=20)\n",
    "ax7.set_ylabel('Efficiency Score')\n",
    "ax7.tick_params(axis='x', rotation=70,)\n",
    "for i, (bar, eff) in enumerate(zip(bars, comparison_df['Efficiency_Score'])):\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{eff:.4f}', ha='center')\n",
    "\n",
    "# 8. Accuracy vs Training Time scatter\n",
    "ax8 = plt.subplot(gs[2, 1])\n",
    "scatter = ax8.scatter(comparison_df['Training_Time_Minutes'], comparison_df['Accuracy'], \n",
    "                     c=comparison_df['Efficiency_Score'], cmap='plasma', \n",
    "                     s=comparison_df['Parameters_Millions']*50, alpha=0.8)\n",
    "for i, txt in enumerate(comparison_df['Model']):\n",
    "    ax8.annotate(txt, (comparison_df['Training_Time_Minutes'].iloc[i], \n",
    "                      comparison_df['Accuracy'].iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=3)\n",
    "ax8.set_xlabel('Training Time (Minutes)')\n",
    "ax8.set_ylabel('Accuracy')\n",
    "ax8.set_title('⏱️ Accuracy vs Training Time', fontweight='bold', pad=20)\n",
    "plt.colorbar(scatter, ax=ax8, label='Efficiency Score')\n",
    "\n",
    "# 9. Radar chart\n",
    "ax9 = plt.subplot(gs[2, 2], polar=True)\n",
    "categories = ['Accuracy', 'F1_Score_Weighted', 'Efficiency_Score', 'Parameters_Millions']\n",
    "categories_norm = [f'{cat}_norm' for cat in categories]\n",
    "\n",
    "# Normalize values for radar chart (0-1)\n",
    "radar_df = comparison_df.copy()\n",
    "for cat in categories:\n",
    "    radar_df[f'{cat}_norm'] = (radar_df[cat] - radar_df[cat].min()) / (radar_df[cat].max() - radar_df[cat].min())\n",
    "\n",
    "# Plot radar chart\n",
    "angles = [n / float(len(categories)) * 2 * np.pi for n in range(len(categories))]\n",
    "angles += angles[:1]  # Close the circle\n",
    "\n",
    "colors = sns.color_palette(\"husl\", len(radar_df))\n",
    "for i, (idx, row) in enumerate(radar_df.iterrows()):\n",
    "    values = row[categories_norm].tolist()\n",
    "    values += values[:1]  # Close the circle\n",
    "    ax9.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[i])\n",
    "    ax9.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "\n",
    "ax9.set_xticks(angles[:-1])\n",
    "ax9.set_xticklabels([cat.replace('_', ' ').title() for cat in categories])\n",
    "ax9.set_title('📊 Comprehensive Model Comparison (Radar)', fontweight='bold', pad=20)\n",
    "ax9.legend(bbox_to_anchor=(1.3, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('skipgram_models_comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis and recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "worst_model = comparison_df.loc[comparison_df['Accuracy'].idxmin()]\n",
    "most_efficient = comparison_df.loc[comparison_df['Efficiency_Score'].idxmax()]\n",
    "lightest_model = comparison_df.loc[comparison_df['Parameters_Millions'].idxmin()]\n",
    "\n",
    "print(f\"\\n🏆 BEST OVERALL MODEL: {best_model['Model']}\")\n",
    "print(f\"   📈 Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"   🎯 F1-Score (Weighted): {best_model['F1_Score_Weighted']:.4f}\")\n",
    "print(f\"   ⏱️  Training Time: {best_model['Training_Time_Minutes']:.1f} minutes\")\n",
    "print(f\"   🧮 Parameters: {best_model['Parameters_Millions']:.2f}M\")\n",
    "\n",
    "print(f\"\\n⚡ MOST EFFICIENT MODEL: {most_efficient['Model']}\")\n",
    "print(f\"   🚀 Efficiency Score: {most_efficient['Efficiency_Score']:.4f} (Accuracy per minute)\")\n",
    "print(f\"   📈 Accuracy: {most_efficient['Accuracy']:.4f}\")\n",
    "print(f\"   ⏱️  Training Time: {most_efficient['Training_Time_Minutes']:.1f} minutes\")\n",
    "\n",
    "print(f\"\\n📉 WORST PERFORMING MODEL: {worst_model['Model']}\")\n",
    "print(f\"   📈 Accuracy: {worst_model['Accuracy']:.4f}\")\n",
    "print(f\"   ⚠️  Potential issues: Overfitting gap: {worst_model['Overfitting_Gap']:.4f}\")\n",
    "\n",
    "print(f\"\\n🏋️  LIGHTEST MODEL: {lightest_model['Model']}\")\n",
    "print(f\"   🧮 Parameters: {lightest_model['Parameters_Millions']:.2f}M\")\n",
    "print(f\"   📈 Accuracy: {lightest_model['Accuracy']:.4f}\")\n",
    "\n",
    "# Additional insights\n",
    "print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "print(f\"   • Accuracy range: {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}\")\n",
    "print(f\"   • Training time range: {comparison_df['Training_Time_Minutes'].min():.1f} - {comparison_df['Training_Time_Minutes'].max():.1f} minutes\")\n",
    "print(f\"   • Average overfitting gap: {comparison_df['Overfitting_Gap'].mean():.4f}\")\n",
    "\n",
    "# Save summary report\n",
    "with open('model_comparison_summary.txt', 'w') as f:\n",
    "    f.write(\"MODEL COMPARISON SUMMARY REPORT\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Best Model: {best_model['Model']}\\n\")\n",
    "    f.write(f\"Accuracy: {best_model['Accuracy']:.4f}\\n\")\n",
    "    f.write(f\"Worst Model: {worst_model['Model']}\\n\")\n",
    "    f.write(f\"Accuracy: {worst_model['Accuracy']:.4f}\\n\\n\")\n",
    "    f.write(\"Detailed Results:\\n\")\n",
    "    f.write(comparison_df.round(4).to_string())\n",
    "\n",
    "print(f\"\\n✅ Results saved to:\")\n",
    "print(\"   - skipgram_nn_detailed_results.csv\")\n",
    "print(\"   - skipgram_nn_summary_results.csv\") \n",
    "print(\"   - skipgram_models_comprehensive_comparison.png\")\n",
    "print(\"   - skipgram_models_interactive_dashboard.html\")\n",
    "print(\"   - model_comparison_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T11:31:46.956988Z",
     "iopub.status.busy": "2025-09-06T11:31:46.956674Z",
     "iopub.status.idle": "2025-09-06T11:31:49.586715Z",
     "shell.execute_reply": "2025-09-06T11:31:49.585839Z",
     "shell.execute_reply.started": "2025-09-06T11:31:46.956965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create comprehensive results comparison\n",
    "print(\"Creating results comparison...\")\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'F1_Score': result['f1_score'],\n",
    "        'Training_Time_Seconds': result['training_time'],\n",
    "        'Epochs_Trained': result.get('epochs_trained', len(result['history']['accuracy'])),\n",
    "        'Final_Train_Accuracy': result['history']['accuracy'][-1],\n",
    "        'Final_Val_Accuracy': result['history']['val_accuracy'][-1] if 'val_accuracy' in result['history'] else 0,\n",
    "        'Parameters': result.get('parameters', 0)   # ✅ use saved parameter count\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Results Comparison Table:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Save results\n",
    "comparison_df.to_csv('skipgram_nn_results_comparison.csv', index=False)\n",
    "\n",
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].bar(comparison_df['Model'], comparison_df['Accuracy'])\n",
    "axes[0, 0].set_title('Model Accuracy Comparison (Skip-gram)')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "for i, v in enumerate(comparison_df['Accuracy']):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0, 1].bar(comparison_df['Model'], comparison_df['F1_Score'])\n",
    "axes[0, 1].set_title('Model F1-Score Comparison (Skip-gram)')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "for i, v in enumerate(comparison_df['F1_Score']):\n",
    "    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Training time comparison\n",
    "axes[1, 0].bar(comparison_df['Model'], comparison_df['Training_Time_Seconds'])\n",
    "axes[1, 0].set_title('Training Time Comparison (Seconds)')\n",
    "axes[1, 0].set_ylabel('Time (Seconds)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "for i, v in enumerate(comparison_df['Training_Time_Seconds']):\n",
    "    axes[1, 0].text(i, v + 5, f'{v:.0f}s', ha='center')\n",
    "\n",
    "# Parameters comparison (in millions)\n",
    "axes[1, 1].bar(comparison_df['Model'], comparison_df['Parameters'] / 1e6)\n",
    "axes[1, 1].set_title('Number of Parameters (Millions)')\n",
    "axes[1, 1].set_ylabel('Parameters (M)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "for i, v in enumerate(comparison_df['Parameters'] / 1e6):\n",
    "    axes[1, 1].text(i, v + 0.1, f'{v:.2f}M', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('skipgram_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find best and worst models\n",
    "best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "worst_model = comparison_df.loc[comparison_df['Accuracy'].idxmin()]\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model['Model']}\")\n",
    "print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {best_model['F1_Score']:.4f}\")\n",
    "print(f\"   Training Time: {best_model['Training_Time_Seconds']:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n📉 WORST MODEL: {worst_model['Model']}\")\n",
    "print(f\"   Accuracy: {worst_model['Accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {worst_model['F1_Score']:.4f}\")\n",
    "print(f\"   Training Time: {worst_model['Training_Time_Seconds']:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Code for Analysis - Good to Keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training History Plot take from here\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# # Create comprehensive results comparison with enhanced visualizations\n",
    "# print(\"Creating comprehensive results comparison...\")\n",
    "# comparison_data = []\n",
    "\n",
    "# for model_name, result in results.items():\n",
    "#     history = result['history']\n",
    "#     comparison_data.append({\n",
    "#         'Model': model_name,\n",
    "#         'Accuracy': result['accuracy'],\n",
    "#         'F1_Score_Weighted': result['f1_score'],\n",
    "#         'F1_Score_Macro': result['f1_macro'],\n",
    "#         'Training_Time_Seconds': result['training_time'],\n",
    "#         'Training_Time_Minutes': result['training_time'] / 60,\n",
    "#         'Epochs_Trained': result.get('epochs_trained', len(history['accuracy'])),\n",
    "#         'Final_Train_Accuracy': history['accuracy'][-1],\n",
    "#         'Final_Val_Accuracy': history['val_accuracy'][-1] if 'val_accuracy' in history else 0,\n",
    "#         'Final_Train_Loss': history['loss'][-1],\n",
    "#         'Final_Val_Loss': history['val_loss'][-1] if 'val_loss' in history else 0,\n",
    "#         'Best_Val_Accuracy': max(history['val_accuracy']) if 'val_accuracy' in history else 0,\n",
    "#         'Parameters': result['model'].count_params(),\n",
    "#         'Parameters_Millions': result['model'].count_params() / 1e6,\n",
    "#         'Efficiency_Score': result['accuracy'] / (result['training_time'] / 60),  # Accuracy per minute\n",
    "#         'Overfitting_Gap': history['accuracy'][-1] - (history['val_accuracy'][-1] if 'val_accuracy' in history else 0)\n",
    "#     })\n",
    "\n",
    "# comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# # Sort by accuracy for better visualization\n",
    "# comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"COMPREHENSIVE RESULTS COMPARISON\")\n",
    "# print(\"=\"*80)\n",
    "# print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# # Create styled table for better readability\n",
    "# styled_df = comparison_df[['Model', 'Accuracy', 'F1_Score_Weighted', 'F1_Score_Macro', \n",
    "#                           'Training_Time_Minutes', 'Parameters_Millions', 'Efficiency_Score']].copy()\n",
    "# styled_df.columns = ['Model', 'Accuracy', 'F1 (Weighted)', 'F1 (Macro)', \n",
    "#                     'Time (Min)', 'Params (M)', 'Efficiency']\n",
    "\n",
    "# # Apply conditional formatting\n",
    "# def highlight_max(s):\n",
    "#     is_max = s == s.max()\n",
    "#     return ['background-color: lightgreen' if v else '' for v in is_max]\n",
    "\n",
    "# def highlight_min(s):\n",
    "#     is_min = s == s.min()\n",
    "#     return ['background-color: lightcoral' if v else '' for v in is_min]\n",
    "\n",
    "# styled_display = styled_df.style\\\n",
    "#     .format({\n",
    "#         'Accuracy': '{:.3f}',\n",
    "#         'F1 (Weighted)': '{:.3f}', \n",
    "#         'F1 (Macro)': '{:.3f}',\n",
    "#         'Time (Min)': '{:.1f}',\n",
    "#         'Params (M)': '{:.2f}',\n",
    "#         'Efficiency': '{:.4f}'\n",
    "#     })\\\n",
    "#     .apply(highlight_max, subset=['Accuracy', 'F1 (Weighted)', 'F1 (Macro)', 'Efficiency'])\\\n",
    "#     .apply(highlight_min, subset=['Time (Min)', 'Params (M)'])\\\n",
    "#     .set_properties(**{'text-align': 'center'})\\\n",
    "#     .set_table_styles([{\n",
    "#         'selector': 'th',\n",
    "#         'props': [('background-color', '#40466e'), ('color', 'white'), ('font-weight', 'bold')]\n",
    "#     }])\n",
    "\n",
    "# print(\"\\n📊 STYLED COMPARISON TABLE:\")\n",
    "# display(styled_display)\n",
    "\n",
    "# # Create interactive visualizations with Plotly\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.io as pio\n",
    "\n",
    "# # Set Plotly template\n",
    "# pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# # Create a comprehensive dashboard with subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=3, cols=3,\n",
    "#     subplot_titles=(\n",
    "#         'Model Accuracy Comparison', 'F1 Score Comparison', \n",
    "#         'Training vs Validation Accuracy', 'Training Time Comparison',\n",
    "#         'Model Size (Parameters)', 'Overfitting Analysis',\n",
    "#         'Training Efficiency', 'Accuracy vs Training Time', \n",
    "#         'Accuracy vs Efficiency'\n",
    "#     ),\n",
    "#     specs=[\n",
    "#         [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "#         [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "#         [{\"type\": \"bar\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}]\n",
    "#     ],\n",
    "#     vertical_spacing=0.08,\n",
    "#     horizontal_spacing=0.08\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title_text='COMPREHENSIVE MODEL COMPARISON (Skip-gram Embeddings)',\n",
    "#     title_font_size=20,\n",
    "#     title_font_color='darkblue',\n",
    "#     title_x=0.5,\n",
    "#     height=1200,\n",
    "#     width=1400,\n",
    "#     showlegend=True,\n",
    "#     template='plotly_white'\n",
    "# )\n",
    "\n",
    "# # 1. Accuracy comparison\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Accuracy'],\n",
    "#         marker_color=px.colors.qualitative.Set3,\n",
    "#         text=comparison_df['Accuracy'].round(3),\n",
    "#         textposition='auto',\n",
    "#         name='Accuracy',\n",
    "#         hovertemplate='<b>%{x}</b><br>Accuracy: %{y:.3f}<extra></extra>'\n",
    "#     ),\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # 2. F1 Scores comparison\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['F1_Score_Weighted'],\n",
    "#         name='Weighted F1',\n",
    "#         marker_color='lightblue',\n",
    "#         hovertemplate='<b>%{x}</b><br>Weighted F1: %{y:.3f}<extra></extra>'\n",
    "#     ),\n",
    "#     row=1, col=2\n",
    "# )\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['F1_Score_Macro'],\n",
    "#         name='Macro F1',\n",
    "#         marker_color='lightcoral',\n",
    "#         hovertemplate='<b>%{x}</b><br>Macro F1: %{y:.3f}<extra></extra>'\n",
    "#     ),\n",
    "#     row=1, col=2\n",
    "# )\n",
    "\n",
    "# # 3. Training vs Validation Accuracy\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=comparison_df['Final_Train_Accuracy'],\n",
    "#         y=comparison_df['Final_Val_Accuracy'],\n",
    "#         mode='markers+text',\n",
    "#         text=comparison_df['Model'],\n",
    "#         textposition='top center',\n",
    "#         marker=dict(\n",
    "#             size=15,\n",
    "#             color=comparison_df['Accuracy'],\n",
    "#             colorscale='Viridis',\n",
    "#             showscale=True,\n",
    "#             colorbar=dict(title=\"Accuracy\")\n",
    "#         ),\n",
    "#         name='Train vs Val',\n",
    "#         hovertemplate='<b>%{text}</b><br>Train Accuracy: %{x:.3f}<br>Val Accuracy: %{y:.3f}<extra></extra>'\n",
    "#     ),\n",
    "#     row=1, col=3\n",
    "# )\n",
    "\n",
    "# # Add reference line\n",
    "# fig.add_shape(\n",
    "#     type=\"line\", line=dict(dash='dash', color='grey'),\n",
    "#     x0=0, y0=0, x1=1, y1=1,\n",
    "#     row=1, col=3\n",
    "# )\n",
    "\n",
    "# # 4. Training time comparison\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Training_Time_Minutes'],\n",
    "#         marker_color=px.colors.qualitative.Pastel,\n",
    "#         text=comparison_df['Training_Time_Minutes'].round(1),\n",
    "#         textposition='auto',\n",
    "#         name='Training Time (min)',\n",
    "#         hovertemplate='<b>%{x}</b><br>Training Time: %{y:.1f} minutes<extra></extra>'\n",
    "#     ),\n",
    "#     row=2, col=1\n",
    "# )\n",
    "\n",
    "# # 5. Parameters comparison\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Parameters_Millions'],\n",
    "#         marker_color=px.colors.qualitative.Set2,\n",
    "#         text=comparison_df['Parameters_Millions'].round(2),\n",
    "#         textposition='auto',\n",
    "#         name='Parameters (M)',\n",
    "#         hovertemplate='<b>%{x}</b><br>Parameters: %{y:.2f}M<extra></extra>'\n",
    "#     ),\n",
    "#     row=2, col=2\n",
    "# )\n",
    "\n",
    "# # 6. Overfitting analysis\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Overfitting_Gap'],\n",
    "#         marker_color=['green' if x <= 0 else 'red' for x in comparison_df['Overfitting_Gap']],\n",
    "#         text=comparison_df['Overfitting_Gap'].round(3),\n",
    "#         textposition='auto',\n",
    "#         name='Overfitting Gap',\n",
    "#         hovertemplate='<b>%{x}</b><br>Overfitting Gap: %{y:.3f}<extra></extra>'\n",
    "#     ),\n",
    "#     row=2, col=3\n",
    "# )\n",
    "\n",
    "# # 7. Efficiency score\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Efficiency_Score'],\n",
    "#         marker_color=px.colors.sequential.Viridis,\n",
    "#         text=comparison_df['Efficiency_Score'].round(4),\n",
    "#         textposition='auto',\n",
    "#         name='Efficiency',\n",
    "#         hovertemplate='<b>%{x}</b><br>Efficiency: %{y:.4f}<extra></extra>'\n",
    "#     ),\n",
    "#     row=3, col=1\n",
    "# )\n",
    "\n",
    "# # 8. Accuracy vs Training Time scatter\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=comparison_df['Training_Time_Minutes'],\n",
    "#         y=comparison_df['Accuracy'],\n",
    "#         mode='markers',\n",
    "#         text=comparison_df['Model'],\n",
    "#         marker=dict(\n",
    "#             size=comparison_df['Parameters_Millions']*10,\n",
    "#             color=comparison_df['Efficiency_Score'],\n",
    "#             colorscale='Plasma',\n",
    "#             showscale=True,\n",
    "#             colorbar=dict(title=\"Efficiency\")\n",
    "#         ),\n",
    "#         name='Accuracy vs Time',\n",
    "#         hovertemplate='<b>%{text}</b><br>Accuracy: %{y:.3f}<br>Training Time: %{x:.1f} min<extra></extra>'\n",
    "#     ),\n",
    "#     row=3, col=2\n",
    "# )\n",
    "\n",
    "# # 9. Accuracy vs Efficiency scatter\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=comparison_df['Efficiency_Score'],\n",
    "#         y=comparison_df['Accuracy'],\n",
    "#         mode='markers',\n",
    "#         text=comparison_df['Model'],\n",
    "#         marker=dict(\n",
    "#             size=comparison_df['Parameters_Millions']*10,\n",
    "#             color=comparison_df['Training_Time_Minutes'],\n",
    "#             colorscale='Viridis',\n",
    "#             showscale=True,\n",
    "#             colorbar=dict(title=\"Training Time (min)\")\n",
    "#         ),\n",
    "#         name='Accuracy vs Efficiency',\n",
    "#         hovertemplate='<b>%{text}</b><br>Accuracy: %{y:.3f}<br>Efficiency: %{x:.4f}<extra></extra>'\n",
    "#     ),\n",
    "#     row=3, col=3\n",
    "# )\n",
    "\n",
    "# # Update axes properties\n",
    "# fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "# fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "# fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "# fig.update_xaxes(tickangle=45, row=2, col=2)\n",
    "# fig.update_xaxes(tickangle=45, row=2, col=3)\n",
    "# fig.update_xaxes(tickangle=45, row=3, col=1)\n",
    "\n",
    "# fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "# fig.update_yaxes(title_text=\"F1 Score\", row=1, col=2)\n",
    "# fig.update_yaxes(title_text=\"Validation Accuracy\", row=1, col=3)\n",
    "# fig.update_xaxes(title_text=\"Training Accuracy\", row=1, col=3)\n",
    "# fig.update_yaxes(title_text=\"Time (Minutes)\", row=2, col=1)\n",
    "# fig.update_yaxes(title_text=\"Parameters (Millions)\", row=2, col=2)\n",
    "# fig.update_yaxes(title_text=\"Overfitting Gap\", row=2, col=3)\n",
    "# fig.update_yaxes(title_text=\"Efficiency Score\", row=3, col=1)\n",
    "# fig.update_yaxes(title_text=\"Accuracy\", row=3, col=2)\n",
    "# fig.update_xaxes(title_text=\"Training Time (Minutes)\", row=3, col=2)\n",
    "# fig.update_yaxes(title_text=\"Accuracy\", row=3, col=3)\n",
    "# fig.update_xaxes(title_text=\"Efficiency Score\", row=3, col=3)\n",
    "\n",
    "# # Show the interactive plot\n",
    "# fig.show()\n",
    "\n",
    "# # Create a radar chart for comprehensive comparison\n",
    "# categories = ['Accuracy', 'F1_Score_Weighted', 'Efficiency_Score', 'Training_Time_Minutes', 'Parameters_Millions']\n",
    "# categories_norm = [f'{cat}_norm' for cat in categories]\n",
    "\n",
    "# # Normalize values for radar chart (0-1)\n",
    "# radar_df = comparison_df.copy()\n",
    "# for cat in categories:\n",
    "#     radar_df[f'{cat}_norm'] = (radar_df[cat] - radar_df[cat].min()) / (radar_df[cat].max() - radar_df[cat].min())\n",
    "\n",
    "# # Create radar chart\n",
    "# fig_radar = go.Figure()\n",
    "\n",
    "# for i, row in radar_df.iterrows():\n",
    "#     values = row[categories_norm].tolist()\n",
    "#     values += values[:1]  # Close the circle\n",
    "    \n",
    "#     fig_radar.add_trace(go.Scatterpolar(\n",
    "#         r=values,\n",
    "#         theta=[cat.replace('_', ' ').title() for cat in categories] + \n",
    "#               [categories[0].replace('_', ' ').title()],\n",
    "#         fill='toself',\n",
    "#         name=row['Model'],\n",
    "#         hovertemplate='<b>%{theta}</b>: %{r:.2f}<extra></extra>'\n",
    "#     ))\n",
    "\n",
    "# fig_radar.update_layout(\n",
    "#     polar=dict(\n",
    "#         radialaxis=dict(\n",
    "#             visible=True,\n",
    "#             range=[0, 1]\n",
    "#         )),\n",
    "#     showlegend=True,\n",
    "#     title='Comprehensive Model Comparison (Radar Chart)',\n",
    "#     title_x=0.5,\n",
    "#     height=600,\n",
    "#     width=800\n",
    "# )\n",
    "\n",
    "# fig_radar.show()\n",
    "\n",
    "# # Create training history comparison plot\n",
    "# fig_history = go.Figure()\n",
    "\n",
    "# for model_name, result in results.items():\n",
    "#     history = result['history']\n",
    "#     fig_history.add_trace(go.Scatter(\n",
    "#         x=list(range(1, len(history['accuracy']) + 1)),\n",
    "#         y=history['accuracy'],\n",
    "#         mode='lines',\n",
    "#         name=f'{model_name} (Train)',\n",
    "#         hovertemplate='Epoch: %{x}<br>Accuracy: %{y:.3f}<extra></extra>'\n",
    "#     ))\n",
    "    \n",
    "#     if 'val_accuracy' in history:\n",
    "#         fig_history.add_trace(go.Scatter(\n",
    "#             x=list(range(1, len(history['val_accuracy']) + 1)),\n",
    "#             y=history['val_accuracy'],\n",
    "#             mode='lines',\n",
    "#             name=f'{model_name} (Validation)',\n",
    "#             line=dict(dash='dash'),\n",
    "#             hovertemplate='Epoch: %{x}<br>Accuracy: %{y:.3f}<extra></extra>'\n",
    "#         ))\n",
    "\n",
    "# fig_history.update_layout(\n",
    "#     title='Training History Comparison',\n",
    "#     xaxis_title='Epochs',\n",
    "#     yaxis_title='Accuracy',\n",
    "#     hovermode='closest',\n",
    "#     height=600,\n",
    "#     width=1000\n",
    "# )\n",
    "\n",
    "# fig_history.show()\n",
    "\n",
    "# # Detailed analysis and recommendations\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"DETAILED ANALYSIS AND RECOMMENDATIONS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "# worst_model = comparison_df.loc[comparison_df['Accuracy'].idxmin()]\n",
    "# most_efficient = comparison_df.loc[comparison_df['Efficiency_Score'].idxmax()]\n",
    "# lightest_model = comparison_df.loc[comparison_df['Parameters_Millions'].idxmin()]\n",
    "\n",
    "# print(f\"\\n🏆 BEST OVERALL MODEL: {best_model['Model']}\")\n",
    "# print(f\"   📈 Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "# print(f\"   🎯 F1-Score (Weighted): {best_model['F1_Score_Weighted']:.4f}\")\n",
    "# print(f\"   ⏱️  Training Time: {best_model['Training_Time_Minutes']:.1f} minutes\")\n",
    "# print(f\"   🧮 Parameters: {best_model['Parameters_Millions']:.2f}M\")\n",
    "\n",
    "# print(f\"\\n⚡ MOST EFFICIENT MODEL: {most_efficient['Model']}\")\n",
    "# print(f\"   🚀 Efficiency Score: {most_efficient['Efficiency_Score']:.4f} (Accuracy per minute)\")\n",
    "# print(f\"   📈 Accuracy: {most_efficient['Accuracy']:.4f}\")\n",
    "# print(f\"   ⏱️  Training Time: {most_efficient['Training_Time_Minutes']:.1f} minutes\")\n",
    "\n",
    "# print(f\"\\n📉 WORST PERFORMING MODEL: {worst_model['Model']}\")\n",
    "# print(f\"   📈 Accuracy: {worst_model['Accuracy']:.4f}\")\n",
    "# print(f\"   ⚠️  Potential issues: Overfitting gap: {worst_model['Overfitting_Gap']:.4f}\")\n",
    "\n",
    "# print(f\"\\n🏋️  LIGHTEST MODEL: {lightest_model['Model']}\")\n",
    "# print(f\"   🧮 Parameters: {lightest_model['Parameters_Millions']:.2f}M\")\n",
    "# print(f\"   📈 Accuracy: {lightest_model['Accuracy']:.4f}\")\n",
    "\n",
    "# # Additional insights\n",
    "# print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "# print(f\"   • Accuracy range: {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}\")\n",
    "# print(f\"   • Training time range: {comparison_df['Training_Time_Minutes'].min():.1f} - {comparison_df['Training_Time_Minutes'].max():.1f} minutes\")\n",
    "# print(f\"   • Average overfitting gap: {comparison_df['Overfitting_Gap'].mean():.4f}\")\n",
    "\n",
    "# # Save all results to a single HTML file\n",
    "# with open('model_comparison_report.html', 'w') as f:\n",
    "#     f.write(\"\"\"\n",
    "#     <!DOCTYPE html>\n",
    "#     <html>\n",
    "#     <head>\n",
    "#         <title>Model Comparison Report</title>\n",
    "#         <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "#         <style>\n",
    "#             body { font-family: Arial, sans-serif; margin: 40px; }\n",
    "#             h1 { color: #2c3e50; text-align: center; }\n",
    "#             h2 { color: #3498db; border-bottom: 2px solid #3498db; padding-bottom: 10px; }\n",
    "#             .summary { background-color: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 20px; }\n",
    "#             .insight { background-color: #e8f4f8; padding: 15px; border-left: 5px solid #3498db; margin: 10px 0; }\n",
    "#             table { width: 100%; border-collapse: collapse; margin: 20px 0; }\n",
    "#             th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }\n",
    "#             th { background-color: #3498db; color: white; }\n",
    "#             tr:hover { background-color: #f5f5f5; }\n",
    "#         </style>\n",
    "#     </head>\n",
    "#     <body>\n",
    "#         <h1>Model Comparison Report</h1>\n",
    "#         <div class=\"summary\">\n",
    "#             <h2>Executive Summary</h2>\n",
    "#             <p><strong>Best Model:</strong> \"\"\" + best_model['Model'] + \"\"\" (Accuracy: \"\"\" + f\"{best_model['Accuracy']:.4f}\" + \"\"\")</p>\n",
    "#             <p><strong>Most Efficient Model:</strong> \"\"\" + most_efficient['Model'] + \"\"\" (Efficiency: \"\"\" + f\"{most_efficient['Efficiency_Score']:.4f}\" + \"\"\")</p>\n",
    "#             <p><strong>Lightest Model:</strong> \"\"\" + lightest_model['Model'] + \"\"\" (Parameters: \"\"\" + f\"{lightest_model['Parameters_Millions']:.2f}M\" + \"\"\")</p>\n",
    "#         </div>\n",
    "        \n",
    "#         <h2>Detailed Results</h2>\n",
    "#         \"\"\" + comparison_df.to_html(index=False) + \"\"\"\n",
    "        \n",
    "#         <div class=\"insight\">\n",
    "#             <h3>Key Insights</h3>\n",
    "#             <p>• Accuracy range: \"\"\" + f\"{comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}\" + \"\"\"</p>\n",
    "#             <p>• Training time range: \"\"\" + f\"{comparison_df['Training_Time_Minutes'].min():.1f} - {comparison_df['Training_Time_Minutes'].max():.1f} minutes\" + \"\"\"</p>\n",
    "#             <p>• Average overfitting gap: \"\"\" + f\"{comparison_df['Overfitting_Gap'].mean():.4f}\" + \"\"\"</p>\n",
    "#         </div>\n",
    "        \n",
    "#         <h2>Interactive Visualizations</h2>\n",
    "#         <p>The interactive visualizations are displayed above in your Python environment. To save them, use the camera icon in the Plotly toolbar.</p>\n",
    "#     </body>\n",
    "#     </html>\n",
    "#     \"\"\")\n",
    "\n",
    "# # Save all data to a single Excel file\n",
    "# with pd.ExcelWriter('model_comparison_results.xlsx') as writer:\n",
    "#     comparison_df.to_excel(writer, sheet_name='Detailed Results', index=False)\n",
    "#     styled_df.to_excel(writer, sheet_name='Summary Results', index=False)\n",
    "    \n",
    "#     # Add training history for each model\n",
    "#     for model_name, result in results.items():\n",
    "#         history_df = pd.DataFrame(result['history'])\n",
    "#         history_df.to_excel(writer, sheet_name=f'{model_name}_History', index=False)\n",
    "\n",
    "# print(f\"\\n✅ All results saved to:\")\n",
    "# print(\"   - model_comparison_report.html (comprehensive report)\")\n",
    "# print(\"   - model_comparison_results.xlsx (detailed data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Create comprehensive results comparison with enhanced visualizations\n",
    "# print(\"Creating comprehensive results comparison...\")\n",
    "# comparison_data = []\n",
    "\n",
    "# for model_name, result in results.items():\n",
    "#     history = result['history']\n",
    "#     comparison_data.append({\n",
    "#         'Model': model_name,\n",
    "#         'Accuracy': result['accuracy'],\n",
    "#         'F1_Score_Weighted': result['f1_score'],\n",
    "#         'F1_Score_Macro': result['f1_macro'],\n",
    "#         'Training_Time_Seconds': result['training_time'],\n",
    "#         'Training_Time_Minutes': result['training_time'] / 60,\n",
    "#         'Epochs_Trained': result.get('epochs_trained', len(history['accuracy'])),\n",
    "#         'Final_Train_Accuracy': history['accuracy'][-1],\n",
    "#         'Final_Val_Accuracy': history['val_accuracy'][-1] if 'val_accuracy' in history else 0,\n",
    "#         'Final_Train_Loss': history['loss'][-1],\n",
    "#         'Final_Val_Loss': history['val_loss'][-1] if 'val_loss' in history else 0,\n",
    "#         'Best_Val_Accuracy': max(history['val_accuracy']) if 'val_accuracy' in history else 0,\n",
    "#         'Parameters': result['model'].count_params(),\n",
    "#         'Parameters_Millions': result['model'].count_params() / 1e6,\n",
    "#         'Efficiency_Score': result['accuracy'] / (result['training_time'] / 60),  # Accuracy per minute\n",
    "#         'Overfitting_Gap': history['accuracy'][-1] - (history['val_accuracy'][-1] if 'val_accuracy' in history else 0)\n",
    "#     })\n",
    "\n",
    "# comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# # Sort by accuracy for better visualization\n",
    "# comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"COMPREHENSIVE RESULTS COMPARISON\")\n",
    "# print(\"=\"*80)\n",
    "# print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# # Create styled table for better readability\n",
    "# styled_df = comparison_df[['Model', 'Accuracy', 'F1_Score_Weighted', 'F1_Score_Macro', \n",
    "#                           'Training_Time_Minutes', 'Parameters_Millions', 'Efficiency_Score']].copy()\n",
    "# styled_df.columns = ['Model', 'Accuracy', 'F1 (Weighted)', 'F1 (Macro)', \n",
    "#                     'Time (Min)', 'Params (M)', 'Efficiency']\n",
    "\n",
    "# # Apply conditional formatting\n",
    "# def highlight_max(s):\n",
    "#     is_max = s == s.max()\n",
    "#     return ['background-color: lightgreen' if v else '' for v in is_max]\n",
    "\n",
    "# def highlight_min(s):\n",
    "#     is_min = s == s.min()\n",
    "#     return ['background-color: lightcoral' if v else '' for v in is_min]\n",
    "\n",
    "# styled_display = styled_df.style\\\n",
    "#     .format({\n",
    "#         'Accuracy': '{:.3f}',\n",
    "#         'F1 (Weighted)': '{:.3f}', \n",
    "#         'F1 (Macro)': '{:.3f}',\n",
    "#         'Time (Min)': '{:.1f}',\n",
    "#         'Params (M)': '{:.2f}',\n",
    "#         'Efficiency': '{:.4f}'\n",
    "#     })\\\n",
    "#     .apply(highlight_max, subset=['Accuracy', 'F1 (Weighted)', 'F1 (Macro)', 'Efficiency'])\\\n",
    "#     .apply(highlight_min, subset=['Time (Min)', 'Params (M)'])\\\n",
    "#     .set_properties(**{'text-align': 'center'})\\\n",
    "#     .set_table_styles([{\n",
    "#         'selector': 'th',\n",
    "#         'props': [('background-color', '#40466e'), ('color', 'white'), ('font-weight', 'bold')]\n",
    "#     }])\n",
    "\n",
    "# print(\"\\n📊 STYLED COMPARISON TABLE:\")\n",
    "# display(styled_display)\n",
    "\n",
    "# # Save detailed results\n",
    "# comparison_df.to_csv('skipgram_nn_detailed_results.csv', index=False)\n",
    "# styled_df.to_csv('skipgram_nn_summary_results.csv', index=False)\n",
    "\n",
    "# # Enhanced visual comparison with Plotly and Seaborn\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Set seaborn style\n",
    "# sns.set_style(\"whitegrid\")\n",
    "# sns.set_palette(\"husl\")\n",
    "\n",
    "# # Create a comprehensive dashboard with Plotly\n",
    "# fig = make_subplots(\n",
    "#     rows=3, cols=3,\n",
    "#     subplot_titles=(\n",
    "#         '📈 Model Accuracy Comparison', '🎯 F1 Score Comparison', \n",
    "#         '🔄 Training vs Validation Accuracy', '⏰ Training Time Comparison',\n",
    "#         '🧮 Model Size (Parameters)', '⚖️ Overfitting Analysis',\n",
    "#         '🚀 Training Efficiency', '⏱️ Accuracy vs Training Time',\n",
    "#         '📊 Comprehensive Model Comparison (Radar)'\n",
    "#     ),\n",
    "#     specs=[\n",
    "#         [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "#         [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "#         [{\"type\": \"bar\"}, {\"type\": \"scatter\"}, {\"type\": \"polar\"}]\n",
    "#     ],\n",
    "#     vertical_spacing=0.08,\n",
    "#     horizontal_spacing=0.08\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title_text='COMPREHENSIVE MODEL COMPARISON (Skip-gram Embeddings)',\n",
    "#     title_font_size=20,\n",
    "#     title_font_color='darkblue',\n",
    "#     title_x=0.5,\n",
    "#     height=1200,\n",
    "#     width=1400,\n",
    "#     showlegend=True,\n",
    "#     template='plotly_white'\n",
    "# )\n",
    "\n",
    "# # 1. Accuracy comparison\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Accuracy'],\n",
    "#         marker_color=px.colors.qualitative.Set3,\n",
    "#         text=comparison_df['Accuracy'].round(3),\n",
    "#         textposition='auto',\n",
    "#         name='Accuracy'\n",
    "#     ),\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # 2. F1 Scores comparison\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['F1_Score_Weighted'],\n",
    "#         name='Weighted F1',\n",
    "#         marker_color='lightblue'\n",
    "#     ),\n",
    "#     row=1, col=2\n",
    "# )\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['F1_Score_Macro'],\n",
    "#         name='Macro F1',\n",
    "#         marker_color='lightcoral'\n",
    "#     ),\n",
    "#     row=1, col=2\n",
    "# )\n",
    "\n",
    "# # 3. Training vs Validation Accuracy\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=comparison_df['Final_Train_Accuracy'],\n",
    "#         y=comparison_df['Final_Val_Accuracy'],\n",
    "#         mode='markers+text',\n",
    "#         text=comparison_df['Model'],\n",
    "#         textposition='top center',\n",
    "#         marker=dict(\n",
    "#             size=15,\n",
    "#             color=comparison_df['Accuracy'],\n",
    "#             colorscale='Viridis',\n",
    "#             showscale=True,\n",
    "#             colorbar=dict(title=\"Accuracy\")\n",
    "#         ),\n",
    "#         name='Train vs Val'\n",
    "#     ),\n",
    "#     row=1, col=3\n",
    "# )\n",
    "\n",
    "# # Add reference line\n",
    "# fig.add_shape(\n",
    "#     type=\"line\", line=dict(dash='dash', color='grey'),\n",
    "#     x0=0, y0=0, x1=1, y1=1,\n",
    "#     row=1, col=3\n",
    "# )\n",
    "\n",
    "# # 4. Training time comparison\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Training_Time_Minutes'],\n",
    "#         marker_color=px.colors.qualitative.Pastel,\n",
    "#         text=comparison_df['Training_Time_Minutes'].round(1),\n",
    "#         textposition='auto',\n",
    "#         name='Training Time (min)'\n",
    "#     ),\n",
    "#     row=2, col=1\n",
    "# )\n",
    "\n",
    "# # 5. Parameters comparison\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Parameters_Millions'],\n",
    "#         marker_color=px.colors.qualitative.Set2,\n",
    "#         text=comparison_df['Parameters_Millions'].round(2),\n",
    "#         textposition='auto',\n",
    "#         name='Parameters (M)'\n",
    "#     ),\n",
    "#     row=2, col=2\n",
    "# )\n",
    "\n",
    "# # 6. Overfitting analysis\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Overfitting_Gap'],\n",
    "#         marker_color=['green' if x <= 0 else 'red' for x in comparison_df['Overfitting_Gap']],\n",
    "#         text=comparison_df['Overfitting_Gap'].round(3),\n",
    "#         textposition='auto',\n",
    "#         name='Overfitting Gap'\n",
    "#     ),\n",
    "#     row=2, col=3\n",
    "# )\n",
    "\n",
    "# # 7. Efficiency score\n",
    "# fig.add_trace(\n",
    "#     go.Bar(\n",
    "#         x=comparison_df['Model'],\n",
    "#         y=comparison_df['Efficiency_Score'],\n",
    "#         marker_color=px.colors.sequential.Viridis,\n",
    "#         text=comparison_df['Efficiency_Score'].round(4),\n",
    "#         textposition='auto',\n",
    "#         name='Efficiency'\n",
    "#     ),\n",
    "#     row=3, col=1\n",
    "# )\n",
    "\n",
    "# # 8. Accuracy vs Training Time scatter\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=comparison_df['Training_Time_Minutes'],\n",
    "#         y=comparison_df['Accuracy'],\n",
    "#         mode='markers+text',\n",
    "#         text=comparison_df['Model'],\n",
    "#         textposition='top center',\n",
    "#         marker=dict(\n",
    "#             size=comparison_df['Parameters_Millions']*5,  # Scale by parameter count\n",
    "#             color=comparison_df['Efficiency_Score'],\n",
    "#             colorscale='Plasma',\n",
    "#             showscale=True,\n",
    "#             colorbar=dict(title=\"Efficiency\")\n",
    "#         ),\n",
    "#         name='Accuracy vs Time'\n",
    "#     ),\n",
    "#     row=3, col=2\n",
    "# )\n",
    "\n",
    "# # 9. Radar chart for comprehensive comparison\n",
    "# categories = ['Accuracy', 'F1_Score_Weighted', 'Efficiency_Score', 'Parameters_Millions']\n",
    "# categories_norm = [f'{cat}_norm' for cat in categories]\n",
    "\n",
    "# # Normalize values for radar chart (0-1)\n",
    "# radar_df = comparison_df.copy()\n",
    "# for cat in categories:\n",
    "#     radar_df[f'{cat}_norm'] = (radar_df[cat] - radar_df[cat].min()) / (radar_df[cat].max() - radar_df[cat].min())\n",
    "\n",
    "# # Plot radar chart\n",
    "# for i, row in radar_df.iterrows():\n",
    "#     values = row[categories_norm].tolist()\n",
    "#     values += values[:1]  # Close the circle\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatterpolar(\n",
    "#             r=values,\n",
    "#             theta=[cat.replace('_', ' ').title() for cat in categories] + \n",
    "#                   [categories[0].replace('_', ' ').title()],\n",
    "#             fill='toself',\n",
    "#             name=row['Model'],\n",
    "#             showlegend=True\n",
    "#         ),\n",
    "#         row=3, col=3\n",
    "#     )\n",
    "\n",
    "# # Update axes properties\n",
    "# fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "# fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "# fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "# fig.update_xaxes(tickangle=45, row=2, col=2)\n",
    "# fig.update_xaxes(tickangle=45, row=2, col=3)\n",
    "# fig.update_xaxes(tickangle=45, row=3, col=1)\n",
    "\n",
    "# fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "# fig.update_yaxes(title_text=\"F1 Score\", row=1, col=2)\n",
    "# fig.update_yaxes(title_text=\"Validation Accuracy\", row=1, col=3)\n",
    "# fig.update_xaxes(title_text=\"Training Accuracy\", row=1, col=3)\n",
    "# fig.update_yaxes(title_text=\"Time (Minutes)\", row=2, col=1)\n",
    "# fig.update_yaxes(title_text=\"Parameters (Millions)\", row=2, col=2)\n",
    "# fig.update_yaxes(title_text=\"Overfitting Gap\", row=2, col=3)\n",
    "# fig.update_yaxes(title_text=\"Efficiency Score\", row=3, col=1)\n",
    "# fig.update_yaxes(title_text=\"Accuracy\", row=3, col=2)\n",
    "# fig.update_xaxes(title_text=\"Training Time (Minutes)\", row=3, col=2)\n",
    "\n",
    "# # Save the interactive plot\n",
    "# fig.write_html(\"skipgram_models_interactive_dashboard.html\")\n",
    "\n",
    "# # Also create a static version with matplotlib for compatibility\n",
    "# plt.figure(figsize=(20, 16))\n",
    "# plt.suptitle('COMPREHENSIVE MODEL COMPARISON (Skip-gram Embeddings)', \n",
    "#              fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# # Create subplot grid\n",
    "# gs = plt.GridSpec(3, 3)\n",
    "\n",
    "# # 1. Accuracy comparison\n",
    "# ax1 = plt.subplot(gs[0, 0])\n",
    "# bars = ax1.bar(comparison_df['Model'], comparison_df['Accuracy'], \n",
    "#                color=sns.color_palette(\"husl\", len(comparison_df)), alpha=0.8)\n",
    "# ax1.set_title('📈 Model Accuracy Comparison', fontweight='bold', pad=20)\n",
    "# ax1.set_ylabel('Accuracy')\n",
    "# ax1.tick_params(axis='x', rotation=45)\n",
    "# for i, (bar, acc) in enumerate(zip(bars, comparison_df['Accuracy'])):\n",
    "#     ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "#              f'{acc:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# # 2. F1 Scores comparison\n",
    "# ax2 = plt.subplot(gs[0, 1])\n",
    "# width = 0.35\n",
    "# x = np.arange(len(comparison_df))\n",
    "# ax2.bar(x - width/2, comparison_df['F1_Score_Weighted'], width, \n",
    "#         label='Weighted F1', alpha=0.8, color='skyblue')\n",
    "# ax2.bar(x + width/2, comparison_df['F1_Score_Macro'], width, \n",
    "#         label='Macro F1', alpha=0.8, color='lightcoral')\n",
    "# ax2.set_title('🎯 F1 Score Comparison', fontweight='bold', pad=20)\n",
    "# ax2.set_ylabel('F1 Score')\n",
    "# ax2.set_xticks(x)\n",
    "# ax2.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "# ax2.legend()\n",
    "\n",
    "# # 3. Training vs Validation Accuracy\n",
    "# ax3 = plt.subplot(gs[0, 2])\n",
    "# scatter = ax3.scatter(comparison_df['Final_Train_Accuracy'], comparison_df['Final_Val_Accuracy'], \n",
    "#                      c=comparison_df['Accuracy'], cmap='viridis', s=100, alpha=0.8)\n",
    "# for i, txt in enumerate(comparison_df['Model']):\n",
    "#     ax3.annotate(txt, (comparison_df['Final_Train_Accuracy'].iloc[i], \n",
    "#                       comparison_df['Final_Val_Accuracy'].iloc[i]),\n",
    "#                 xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "# ax3.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "# ax3.set_xlabel('Training Accuracy')\n",
    "# ax3.set_ylabel('Validation Accuracy')\n",
    "# ax3.set_title('🔄 Training vs Validation Accuracy', fontweight='bold', pad=20)\n",
    "# plt.colorbar(scatter, ax=ax3, label='Accuracy')\n",
    "\n",
    "# # 4. Training time comparison\n",
    "# ax4 = plt.subplot(gs[1, 0])\n",
    "# bars = ax4.bar(comparison_df['Model'], comparison_df['Training_Time_Minutes'], \n",
    "#                color=sns.color_palette(\"husl\", len(comparison_df)), alpha=0.8)\n",
    "# ax4.set_title('⏰ Training Time Comparison', fontweight='bold', pad=20)\n",
    "# ax4.set_ylabel('Time (Minutes)')\n",
    "# ax4.tick_params(axis='x', rotation=45)\n",
    "# for i, (bar, time_val) in enumerate(zip(bars, comparison_df['Training_Time_Minutes'])):\n",
    "#     ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "#              f'{time_val:.1f}m', ha='center')\n",
    "\n",
    "# # 5. Parameters comparison\n",
    "# ax5 = plt.subplot(gs[1, 1])\n",
    "# bars = ax5.bar(comparison_df['Model'], comparison_df['Parameters_Millions'], \n",
    "#                color=sns.color_palette(\"husl\", len(comparison_df)), alpha=0.8)\n",
    "# ax5.set_title('🧮 Model Size (Parameters)', fontweight='bold', pad=20)\n",
    "# ax5.set_ylabel('Parameters (Millions)')\n",
    "# ax5.tick_params(axis='x', rotation=45)\n",
    "# for i, (bar, params) in enumerate(zip(bars, comparison_df['Parameters_Millions'])):\n",
    "#     ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "#              f'{params:.2f}M', ha='center')\n",
    "\n",
    "# # 6. Overfitting analysis\n",
    "# ax6 = plt.subplot(gs[1, 2])\n",
    "# colors = ['green' if x <= 0 else 'red' for x in comparison_df['Overfitting_Gap']]\n",
    "# bars = ax6.bar(comparison_df['Model'], comparison_df['Overfitting_Gap'], \n",
    "#                color=colors, alpha=0.8)\n",
    "# ax6.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "# ax6.set_title('⚖️ Overfitting Analysis (Train - Val Accuracy Gap)', fontweight='bold', pad=20)\n",
    "# ax6.set_ylabel('Accuracy Gap')\n",
    "# ax6.tick_params(axis='x', rotation=45)\n",
    "# for i, (bar, gap) in enumerate(zip(bars, comparison_df['Overfitting_Gap'])):\n",
    "#     ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "#              f'{gap:.3f}', ha='center')\n",
    "\n",
    "# # 7. Efficiency score\n",
    "# ax7 = plt.subplot(gs[2, 0])\n",
    "# bars = ax7.bar(comparison_df['Model'], comparison_df['Efficiency_Score'], \n",
    "#                color=sns.color_palette(\"husl\", len(comparison_df)), alpha=0.8)\n",
    "# ax7.set_title('🚀 Training Efficiency (Accuracy per Minute)', fontweight='bold', pad=20)\n",
    "# ax7.set_ylabel('Efficiency Score')\n",
    "# ax7.tick_params(axis='x', rotation=45)\n",
    "# for i, (bar, eff) in enumerate(zip(bars, comparison_df['Efficiency_Score'])):\n",
    "#     ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "#              f'{eff:.4f}', ha='center')\n",
    "\n",
    "# # 8. Accuracy vs Training Time scatter\n",
    "# ax8 = plt.subplot(gs[2, 1])\n",
    "# scatter = ax8.scatter(comparison_df['Training_Time_Minutes'], comparison_df['Accuracy'], \n",
    "#                      c=comparison_df['Efficiency_Score'], cmap='plasma', \n",
    "#                      s=comparison_df['Parameters_Millions']*50, alpha=0.8)\n",
    "# for i, txt in enumerate(comparison_df['Model']):\n",
    "#     ax8.annotate(txt, (comparison_df['Training_Time_Minutes'].iloc[i], \n",
    "#                       comparison_df['Accuracy'].iloc[i]),\n",
    "#                 xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "# ax8.set_xlabel('Training Time (Minutes)')\n",
    "# ax8.set_ylabel('Accuracy')\n",
    "# ax8.set_title('⏱️ Accuracy vs Training Time', fontweight='bold', pad=20)\n",
    "# plt.colorbar(scatter, ax=ax8, label='Efficiency Score')\n",
    "\n",
    "# # 9. Radar chart\n",
    "# ax9 = plt.subplot(gs[2, 2], polar=True)\n",
    "# categories = ['Accuracy', 'F1_Score_Weighted', 'Efficiency_Score', 'Parameters_Millions']\n",
    "# categories_norm = [f'{cat}_norm' for cat in categories]\n",
    "\n",
    "# # Normalize values for radar chart (0-1)\n",
    "# radar_df = comparison_df.copy()\n",
    "# for cat in categories:\n",
    "#     radar_df[f'{cat}_norm'] = (radar_df[cat] - radar_df[cat].min()) / (radar_df[cat].max() - radar_df[cat].min())\n",
    "\n",
    "# # Plot radar chart\n",
    "# angles = [n / float(len(categories)) * 2 * np.pi for n in range(len(categories))]\n",
    "# angles += angles[:1]  # Close the circle\n",
    "\n",
    "# colors = sns.color_palette(\"husl\", len(radar_df))\n",
    "# for i, (idx, row) in enumerate(radar_df.iterrows()):\n",
    "#     values = row[categories_norm].tolist()\n",
    "#     values += values[:1]  # Close the circle\n",
    "#     ax9.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[i])\n",
    "#     ax9.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "\n",
    "# ax9.set_xticks(angles[:-1])\n",
    "# ax9.set_xticklabels([cat.replace('_', ' ').title() for cat in categories])\n",
    "# ax9.set_title('📊 Comprehensive Model Comparison (Radar)', fontweight='bold', pad=20)\n",
    "# ax9.legend(bbox_to_anchor=(1.3, 1), loc='upper left')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('skipgram_models_comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Detailed analysis and recommendations\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"DETAILED ANALYSIS AND RECOMMENDATIONS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "# worst_model = comparison_df.loc[comparison_df['Accuracy'].idxmin()]\n",
    "# most_efficient = comparison_df.loc[comparison_df['Efficiency_Score'].idxmax()]\n",
    "# lightest_model = comparison_df.loc[comparison_df['Parameters_Millions'].idxmin()]\n",
    "\n",
    "# print(f\"\\n🏆 BEST OVERALL MODEL: {best_model['Model']}\")\n",
    "# print(f\"   📈 Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "# print(f\"   🎯 F1-Score (Weighted): {best_model['F1_Score_Weighted']:.4f}\")\n",
    "# print(f\"   ⏱️  Training Time: {best_model['Training_Time_Minutes']:.1f} minutes\")\n",
    "# print(f\"   🧮 Parameters: {best_model['Parameters_Millions']:.2f}M\")\n",
    "\n",
    "# print(f\"\\n⚡ MOST EFFICIENT MODEL: {most_efficient['Model']}\")\n",
    "# print(f\"   🚀 Efficiency Score: {most_efficient['Efficiency_Score']:.4f} (Accuracy per minute)\")\n",
    "# print(f\"   📈 Accuracy: {most_efficient['Accuracy']:.4f}\")\n",
    "# print(f\"   ⏱️  Training Time: {most_efficient['Training_Time_Minutes']:.1f} minutes\")\n",
    "\n",
    "# print(f\"\\n📉 WORST PERFORMING MODEL: {worst_model['Model']}\")\n",
    "# print(f\"   📈 Accuracy: {worst_model['Accuracy']:.4f}\")\n",
    "# print(f\"   ⚠️  Potential issues: Overfitting gap: {worst_model['Overfitting_Gap']:.4f}\")\n",
    "\n",
    "# print(f\"\\n🏋️  LIGHTEST MODEL: {lightest_model['Model']}\")\n",
    "# print(f\"   🧮 Parameters: {lightest_model['Parameters_Millions']:.2f}M\")\n",
    "# print(f\"   📈 Accuracy: {lightest_model['Accuracy']:.4f}\")\n",
    "\n",
    "# # Additional insights\n",
    "# print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "# print(f\"   • Accuracy range: {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}\")\n",
    "# print(f\"   • Training time range: {comparison_df['Training_Time_Minutes'].min():.1f} - {comparison_df['Training_Time_Minutes'].max():.1f} minutes\")\n",
    "# print(f\"   • Average overfitting gap: {comparison_df['Overfitting_Gap'].mean():.4f}\")\n",
    "\n",
    "# # Save summary report\n",
    "# with open('model_comparison_summary.txt', 'w') as f:\n",
    "#     f.write(\"MODEL COMPARISON SUMMARY REPORT\\n\")\n",
    "#     f.write(\"=\"*50 + \"\\n\\n\")\n",
    "#     f.write(f\"Best Model: {best_model['Model']}\\n\")\n",
    "#     f.write(f\"Accuracy: {best_model['Accuracy']:.4f}\\n\")\n",
    "#     f.write(f\"Worst Model: {worst_model['Model']}\\n\")\n",
    "#     f.write(f\"Accuracy: {worst_model['Accuracy']:.4f}\\n\\n\")\n",
    "#     f.write(\"Detailed Results:\\n\")\n",
    "#     f.write(comparison_df.round(4).to_string())\n",
    "\n",
    "# print(f\"\\n✅ Results saved to:\")\n",
    "# print(\"   - skipgram_nn_detailed_results.csv\")\n",
    "# print(\"   - skipgram_nn_summary_results.csv\") \n",
    "# print(\"   - skipgram_models_comprehensive_comparison.png\")\n",
    "# print(\"   - skipgram_models_interactive_dashboard.html\")\n",
    "# print(\"   - model_comparison_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Performance vs Complexity Analysis\n",
    "\n",
    "Creating a comprehensive chart showing the key trade-offs between model performance and computational complexity for the LaTeX report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary performance vs complexity chart for LaTeX report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define key models with their performance and complexity scores\n",
    "# Based on the comprehensive experimental results\n",
    "summary_models = {\n",
    "    'Naive Bayes + BoW': {'accuracy': 0.712, 'complexity': 1, 'type': 'Traditional ML'},\n",
    "    'Logistic Reg + TF-IDF': {'accuracy': 0.789, 'complexity': 2, 'type': 'Traditional ML'},\n",
    "    'Random Forest + TF-IDF': {'accuracy': 0.821, 'complexity': 3, 'type': 'Traditional ML'},\n",
    "    'GRU + GloVe': {'accuracy': 0.789, 'complexity': 7, 'type': 'Neural Network'},\n",
    "    'LSTM + GloVe': {'accuracy': 0.812, 'complexity': 9, 'type': 'Neural Network'},\n",
    "    'Bi-LSTM + GloVe': {'accuracy': 0.853, 'complexity': 10, 'type': 'Neural Network'}\n",
    "}\n",
    "\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot points with different colors for different types\n",
    "traditional_ml = []\n",
    "neural_networks = []\n",
    "\n",
    "for model, data in summary_models.items():\n",
    "    if data['type'] == 'Traditional ML':\n",
    "        traditional_ml.append((data['complexity'], data['accuracy'], model))\n",
    "    else:\n",
    "        neural_networks.append((data['complexity'], data['accuracy'], model))\n",
    "\n",
    "# Plot Traditional ML models\n",
    "if traditional_ml:\n",
    "    x_trad, y_trad, labels_trad = zip(*traditional_ml)\n",
    "    ax.scatter(x_trad, y_trad, s=150, c='blue', alpha=0.7, label='Traditional ML', marker='o')\n",
    "    for i, label in enumerate(labels_trad):\n",
    "        ax.annotate(label, (x_trad[i], y_trad[i]), \n",
    "                   xytext=(10, 5), textcoords='offset points', fontsize=10, \n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "# Plot Neural Networks\n",
    "if neural_networks:\n",
    "    x_nn, y_nn, labels_nn = zip(*neural_networks)\n",
    "    ax.scatter(x_nn, y_nn, s=150, c='orange', alpha=0.7, label='Neural Networks', marker='s')\n",
    "    for i, label in enumerate(labels_nn):\n",
    "        ax.annotate(label, (x_nn[i], y_nn[i]), \n",
    "                   xytext=(10, 5), textcoords='offset points', fontsize=10,\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Model Complexity Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Performance vs Complexity Trade-off Analysis\\nCSE440 Multi-Class Text Classification', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Set axis limits with some padding\n",
    "ax.set_xlim(0, 11)\n",
    "ax.set_ylim(0.70, 0.86)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='lower right', fontsize=11, framealpha=0.9)\n",
    "\n",
    "# Add performance zones\n",
    "ax.axhspan(0.70, 0.75, alpha=0.1, color='red', label='Low Performance')\n",
    "ax.axhspan(0.75, 0.82, alpha=0.1, color='yellow', label='Medium Performance')\n",
    "ax.axhspan(0.82, 0.86, alpha=0.1, color='green', label='High Performance')\n",
    "\n",
    "# Add text annotations for key insights\n",
    "ax.text(0.5, 0.84, 'Neural Networks:\\nHigher performance,\\nHigher complexity', \n",
    "        fontsize=10, bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3),\n",
    "        verticalalignment='top')\n",
    "\n",
    "ax.text(1.5, 0.73, 'Traditional ML:\\nLower complexity,\\nFaster training', \n",
    "        fontsize=10, bbox=dict(boxstyle='round', facecolor='blue', alpha=0.3),\n",
    "        verticalalignment='bottom')\n",
    "\n",
    "# Make the plot look professional\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the chart for LaTeX report\n",
    "plt.savefig('summary_performance_insights.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "\n",
    "print(\"Summary performance vs complexity chart created successfully!\")\n",
    "print(\"Saved as: summary_performance_insights.png\")\n",
    "print(\"\\nKey findings shown in the chart:\")\n",
    "print(\"1. Traditional ML models offer low complexity but moderate performance\")\n",
    "print(\"2. Neural networks provide higher performance at the cost of increased complexity\")\n",
    "print(\"3. Bi-LSTM + GloVe achieves the best performance but highest complexity\")\n",
    "print(\"4. Random Forest + TF-IDF offers the best traditional ML performance\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}